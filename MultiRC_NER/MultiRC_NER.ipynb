{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PX4QQ3CFW1TF"
   },
   "source": [
    "# **MultiRC** - Multihop multiple-choice question answering dataset\n",
    "\n",
    "# **Model** - NER-based QA\n",
    "\n",
    "**APPROACH** -\n",
    "\n",
    "**Dataset Preparation**\n",
    "1. Concatenate paragraph + question + answers into a single context\n",
    "2. Use discriminatory tags for each of- paragraph(P), question(Q), correct answer(C), wrong answer(W) and inside tags(I)\n",
    "3. Now, the dataset is a CSV file with the following structure-\n",
    "\n",
    "\\<ID, TOKEN, TAG\\>\n",
    "\n",
    "where,\n",
    "\n",
    "ID- unique for every (paragraph,question,answers) combination\n",
    "\n",
    "TOKEN- paragraph + question + options concatenated  tokenized\n",
    "\n",
    "TAG - pre-determned tag for every portion in the context\n",
    "\n",
    "\n",
    "**Model Preparation**\n",
    "\n",
    "4. Train the model to learn this variation of BIO tagging\n",
    "\n",
    "**Evaluation Preparation**\n",
    "\n",
    "5. Evaluate model's performnance against expected results- tagging the correct answer as CI tags and wrong answer as WI tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulTtzBt1xM9G"
   },
   "source": [
    "# NOTE : Search \"TODO\" to make changes for original/sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ddbuTPIUi7h"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jwPQNGAnWbHc"
   },
   "source": [
    "# Mounting data\n",
    "1. train.csv - training set\n",
    "2. dev.csv - testing set\n",
    "\n",
    "Note- We are using validation set as our test set since the MultiRC test set is not publicly available and it's not possible to verify labels and analyse model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oVMX9PY6U-cc",
    "outputId": "92cd234f-903d-4131-fc97-7f7eb6da0f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hB8x1XAPhB2D"
   },
   "outputs": [],
   "source": [
    "PARENT_DIR = \"/content/gdrive/My Drive/MultiRC_NER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "UutbeUmMVP_P",
    "outputId": "5a1f0548-e0d3-4af4-9c9b-4e594044bbaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.csv\t\tdev_v2.csv  qa\t       train_sample.csv  train_v3.csv\n",
      "dev_sample.csv\tdev_v3.csv  train.csv  train_v2.csv\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/gdrive/My Drive/MultiRC_NER/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WaD3PslCWEVK"
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "colab_type": "code",
    "id": "WyQ9p51QWDHq",
    "outputId": "6da133ed-b926-40dd-adb0-2c6a476c590f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.3)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=1214ce8d3aaccceabfbb6d8b1a91f28e6b9386860e9271381848e134bdc38d95\n",
      "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-0.0.12\n",
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
      "\u001b[K     |████████████████████████████████| 573kB 6.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 15.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 29.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 49.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=28edfac242524cb0f252b68d29bd347df857404c1857fbf7f74ab62b62a85769\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcUnNx1WUi7l"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nBU6k3dQUi7p",
    "outputId": "f2a911ab-4536-408f-c511-d576527ff1ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm,trange\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForTokenClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "-sUo6M6fUi7t",
    "outputId": "05077854-beb9-480b-80ae-821107d89f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras                    2.3.1          \n",
      "Keras-Applications       1.0.8          \n",
      "Keras-Preprocessing      1.1.0          \n",
      "torch                    1.5.0+cu101    \n",
      "torchsummary             1.5.1          \n",
      "torchtext                0.3.1          \n",
      "torchvision              0.6.0+cu101    \n",
      "transformers             2.8.0          \n"
     ]
    }
   ],
   "source": [
    "# Check library version\n",
    "!pip list | grep -E 'transformers|torch|Keras'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HmaFwPpUi7w"
   },
   "source": [
    "This notebook works with env:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1jIVHbJUi7x"
   },
   "source": [
    "- Keras                2.3.1                 \n",
    "- torch                1.1.0                 \n",
    "- transformers         2.2.0      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4X_eNNqGUi75"
   },
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04kRhJ2mUi76"
   },
   "source": [
    "**Load CSV data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7sZnV5gGUi76"
   },
   "outputs": [],
   "source": [
    "data_path = PARENT_DIR + \"/data\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctpiyXm9Ui7-"
   },
   "outputs": [],
   "source": [
    "# TODO: \"train.csv\" - original, \"train_sample.csv\" - sampled file(1/100th data)\n",
    "train_file_address = PARENT_DIR + \"/data/train_v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1j-H3sKUi8A"
   },
   "outputs": [],
   "source": [
    "# Fillna method can make same sentence with same sentence name\n",
    "# NOTE - encoding latin1 => utf-8\n",
    "df_data = pd.read_csv(train_file_address,sep=\",\",encoding=\"utf-8\").fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HuoBFgbsUi8D",
    "outputId": "aa35ca34-d2f9-4277-b5e0-331871e15a15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'TOKEN', 'TAG'], dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "Gf7dOgcAUi8H",
    "outputId": "bfa96a8e-a92f-4a53-aca3-f14604f7ef51"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>TAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Animated</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>history</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Of</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>course</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>cartoon</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>highly</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>oversimplified</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>most</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>critics</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>consider</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>it</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>one</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID           TOKEN TAG\n",
       "0    1        Animated   P\n",
       "1    1         history   I\n",
       "2    1              of   I\n",
       "3    1             the   I\n",
       "4    1              US   I\n",
       "5    1               .   I\n",
       "6    1              Of   P\n",
       "7    1          course   I\n",
       "8    1             the   I\n",
       "9    1         cartoon   I\n",
       "10   1              is   I\n",
       "11   1          highly   I\n",
       "12   1  oversimplified   I\n",
       "13   1             and   I\n",
       "14   1            most   I\n",
       "15   1         critics   I\n",
       "16   1        consider   I\n",
       "17   1              it   I\n",
       "18   1             one   I\n",
       "19   1              of   I"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkB6hAWKUi8L"
   },
   "source": [
    "**TAG categories**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "iLeBXoekUi8U",
    "outputId": "7188f5f2-381d-4f0a-8da3-8ee250cc951d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I    1541258\n",
       "P      68462\n",
       "W      15218\n",
       "C      12025\n",
       "Q       5131\n",
       "Name: TAG, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TAG distribution\n",
    "df_data.TAG.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKbYp0YaUi8X"
   },
   "source": [
    "### TAG nomenclature\n",
    "As shown and explained above, there are 4 distinct tags, one each for- Paragraph, Question, Correct answer and Wrong answer\n",
    "- P: Paragraph sentence begin, word at the first  position\n",
    "- Q: Question sentence begin, word at the first  position\n",
    "- C: Correct answer sentence begin, word at the first  position\n",
    "- W: Wrong answer sentence begin, word at the first  position\n",
    "- I: inside, word not at the first position, for sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmtZkBnFUi8e"
   },
   "source": [
    "## Parser data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRHouNE4Ui8e"
   },
   "source": [
    "**Parser data into document structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HE87UalmUi8f"
   },
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"TOKEN\"].values.tolist(),\n",
    "                                                           s[\"TAG\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"ID\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eok4eiP9Ui8i"
   },
   "outputs": [],
   "source": [
    "# Get full document data structure\n",
    "getter = SentenceGetter(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLrGqAXxUi8k"
   },
   "outputs": [],
   "source": [
    "# Get sentence data\n",
    "sentences = [[s[0] for s in sent] for sent in getter.sentences]\n",
    "# sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQrZXSEcUi8p"
   },
   "outputs": [],
   "source": [
    "# Get TAG labels data\n",
    "labels = [[s[1] for s in sent] for sent in getter.sentences]\n",
    "# print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kMCiMsc7Ui8u"
   },
   "source": [
    "**Convert TAG name into index for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpRLGoGuUi8v"
   },
   "outputs": [],
   "source": [
    "tags_vals = list(set(df_data[\"TAG\"].values))\n",
    "# Add X  label for word piece support\n",
    "# Add [CLS] and [SEP] as BERT need\n",
    "tags_vals.append('X')\n",
    "tags_vals.append('[CLS]')\n",
    "tags_vals.append('[SEP]')\n",
    "tags_vals = set(tags_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kC5wynXUi87"
   },
   "outputs": [],
   "source": [
    "# Set a dict for mapping id to tag name\n",
    "# tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "# TODO: why manual ?\n",
    "# Manual definition\n",
    "tag2idx={'C': 2,\n",
    " 'I': 3,\n",
    " 'P': 0,\n",
    " 'Q': 1,\n",
    " 'W': 4,\n",
    " 'X':5,\n",
    " '[CLS]':6,\n",
    " '[SEP]':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_DHev6tUi9C"
   },
   "outputs": [],
   "source": [
    "# Mapping index to name (reverse)\n",
    "tag2name={tag2idx[key] : key for key in tag2idx.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MaQoRbYUi9N"
   },
   "source": [
    "## Preprocess Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORMHJaGvUi9N"
   },
   "source": [
    "Raw data => trainable data for BERT, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tfRMtXayUi9O"
   },
   "source": [
    "- GPU environment\n",
    "- Loading tokenizer and tokenize\n",
    "- Set 3 embeddings - token, mask word, segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0oUKNJBMUi9R"
   },
   "source": [
    "**Setting-up GPU environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vtq3HojUi9S"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kuu0D9gaUi9U",
    "outputId": "ecf12c41-a33c-41cd-d0b6-a543d5022d10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Il3J1IZ5Ui9X"
   },
   "source": [
    "### Loading Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NE6BEvm7Ui9Y"
   },
   "source": [
    "Downloading the tokenizer file into GDrive folder first :\n",
    "- [vocab.txt](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvvpxN3RUi9Y"
   },
   "outputs": [],
   "source": [
    "vocabulary = PARENT_DIR + \"/models/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKRuXFMiUi9a"
   },
   "outputs": [],
   "source": [
    "# Length of the sentence = 384 (dataset analysis- paragraph + question + answers = ~ 350, generally.)\n",
    "# CAUTION - should be less than 512\n",
    "# TODO : try with increased length\n",
    "max_len  = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ThgQqj6wUi9d"
   },
   "outputs": [],
   "source": [
    "# load tokenizer, with manual file address or pretrained address\n",
    "tokenizer=BertTokenizer(vocab_file=vocabulary,do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmXz0B4oUi9k"
   },
   "source": [
    "**Tokenizer text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ESveNcEUi9l"
   },
   "source": [
    "- In hunggingface for bert, when come across OOV, will word piece the word\n",
    "- We need to adjust the labels base on the tokenize result, “##abc” need to set label \"X\" \n",
    "- Need to set \"[CLS]\" at front and \"[SEP]\" at the end, as what the paper do, [BERT indexer should add [CLS] and [SEP] tokens](https://github.com/allenai/allennlp/issues/2141)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "LamYC4kWUi9m",
    "outputId": "ad0ddede-a938-4f5f-8f89-9b36aa2e30c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.0,len:378\n",
      "texts:[CLS] Animated history of the US . Of course the cartoon is highly overs ##im ##plified and most critics consider it one of the weak ##est parts of the film . But it makes a valid claim which you ignore entirely : That the strategy to promote gun rights for white people and to out ##law gun possession by black people was a way to up ##hold racism without letting an openly terrorist organization like the K ##K ##K flourish . Did the 19th century N ##RA in the southern states promote gun rights for black people ? I highly doubt it . But if they didn ' t one of their functions was to continue the racism of the K ##K ##K . This is the key message of this part of the animation which is again being ignored by its critics . B ##uel ##l shooting in Flint . You write : F ##act : The little boy was the class th ##ug already suspended from school for stabbing another kid with a pencil and had fought with Kay ##la the day before . This characterization of a six - year - old as a pencil - stabbing th ##ug is exactly the kind of h ##yster ##ia that Moore ' s film warns against . It is the typical right - wing reaction which looks for simple answers that do not con ##tra ##dict the Republican minds ##et . The kid was a little bastard and the parents were involved in drugs - - case closed . But why do people deal with drugs ? Because it ' s so much fun to do so ? It is by now well documented that the CIA tolerate ##d crack sales in US cities to fund the operation of South American con ##tras It is equally well known that the so - called war on drugs begun under the Nixon administration is a failure which has cost hundreds of billion ##s and made America the world leader in prison population ( both in relative and absolute numbers ) . Does the author claim the animated films message is that the N ##RA up ##hold ##s racism ? Yes . Up ##hold and continue . No . [SEP]\n",
      "No.0,len:378\n",
      "lables:[CLS] P I I I I I P I I I I I I X X I I I I I I I I I X I I I I I P I I I I I I I I I X I I I I I I I I I I I I I X I I I I I I I I I I X I I I I I I I I I I X X I I P I I I I X I I I I I I I I I I I P I I I I P I I I X X I I I I I I I I I I I I X X I P I I I I I I I I I I I I I I I I I I I P X X I I I I P I X I X X I I I I I I I X I I I I I I I I I I I I I I I I X I I I I P I I I I X X X X I I I X X I X I I I I I I X X I I X X I I I I P I I I I X X I I I I I I I I I I X X I I I X I P I I I I I I I I I I I I I X I I I P I I I I I I I P I X X I I I I I I I P I I I I I I I I I X I I I I I I I I I I I I I X I I I I I I I I X X I I I I I I I I I I I I I I I I I X I I I I I I I I I I X I I I I I X I Q I I I I I I I I I I I X I X X I I C I C X I I I W I [SEP]\n",
      "No.1,len:430\n",
      "texts:[CLS] Animated history of the US . Of course the cartoon is highly overs ##im ##plified and most critics consider it one of the weak ##est parts of the film . But it makes a valid claim which you ignore entirely : That the strategy to promote gun rights for white people and to out ##law gun possession by black people was a way to up ##hold racism without letting an openly terrorist organization like the K ##K ##K flourish . Did the 19th century N ##RA in the southern states promote gun rights for black people ? I highly doubt it . But if they didn ' t one of their functions was to continue the racism of the K ##K ##K . This is the key message of this part of the animation which is again being ignored by its critics . B ##uel ##l shooting in Flint . You write : F ##act : The little boy was the class th ##ug already suspended from school for stabbing another kid with a pencil and had fought with Kay ##la the day before . This characterization of a six - year - old as a pencil - stabbing th ##ug is exactly the kind of h ##yster ##ia that Moore ' s film warns against . It is the typical right - wing reaction which looks for simple answers that do not con ##tra ##dict the Republican minds ##et . The kid was a little bastard and the parents were involved in drugs - - case closed . But why do people deal with drugs ? Because it ' s so much fun to do so ? It is by now well documented that the CIA tolerate ##d crack sales in US cities to fund the operation of South American con ##tras It is equally well known that the so - called war on drugs begun under the Nixon administration is a failure which has cost hundreds of billion ##s and made America the world leader in prison population ( both in relative and absolute numbers ) . Which key message ( s ) do ( es ) this passage say the critics ignored ? The strategy to promote gun rights for white people while out ##law ##ing it for black people allowed r ##ac ##isi ##m to continue without allowing to K ##K ##K to flourish . That it ant ##agon ##ized the K ##K ##K . That the K ##K ##K was a terrorist organization . The strategy to promote the K ##K ##K . [SEP]\n",
      "No.1,len:430\n",
      "lables:[CLS] P I I I I I P I I I I I I X X I I I I I I I I I X I I I I I P I I I I I I I I I X I I I I I I I I I I I I I X I I I I I I I I I I X I I I I I I I I I I X X I I P I I I I X I I I I I I I I I I I P I I I I P I I I X X I I I I I I I I I I I I X X I P I I I I I I I I I I I I I I I I I I I P X X I I I I P I X I X X I I I I I I I X I I I I I I I I I I I I I I I I X I I I I P I I I I X X X X I I I X X I X I I I I I I X X I I X X I I I I P I I I I X X I I I I I I I I I I X X I I I X I P I I I I I I I I I I I I I X I I I P I I I I I I I P I X X I I I I I I I P I I I I I I I I I X I I I I I I I I I I I I I X I I I I I I I I X X I I I I I I I I I I I I I I I I I X I I I I I I I I I I X I I I I I X I Q I I X X X I X X X I I I I I I I C I I I I I I I I I I X X I I I I I I X X X I I I I I I X X I I I W I I X X I I X X I W I I X X I I I I I W I I I I I X X I [SEP]\n",
      "No.2,len:488\n",
      "texts:[CLS] Animated history of the US . Of course the cartoon is highly overs ##im ##plified and most critics consider it one of the weak ##est parts of the film . But it makes a valid claim which you ignore entirely : That the strategy to promote gun rights for white people and to out ##law gun possession by black people was a way to up ##hold racism without letting an openly terrorist organization like the K ##K ##K flourish . Did the 19th century N ##RA in the southern states promote gun rights for black people ? I highly doubt it . But if they didn ' t one of their functions was to continue the racism of the K ##K ##K . This is the key message of this part of the animation which is again being ignored by its critics . B ##uel ##l shooting in Flint . You write : F ##act : The little boy was the class th ##ug already suspended from school for stabbing another kid with a pencil and had fought with Kay ##la the day before . This characterization of a six - year - old as a pencil - stabbing th ##ug is exactly the kind of h ##yster ##ia that Moore ' s film warns against . It is the typical right - wing reaction which looks for simple answers that do not con ##tra ##dict the Republican minds ##et . The kid was a little bastard and the parents were involved in drugs - - case closed . But why do people deal with drugs ? Because it ' s so much fun to do so ? It is by now well documented that the CIA tolerate ##d crack sales in US cities to fund the operation of South American con ##tras It is equally well known that the so - called war on drugs begun under the Nixon administration is a failure which has cost hundreds of billion ##s and made America the world leader in prison population ( both in relative and absolute numbers ) . What type of the film is being discussed and what is on of the key messages ? Animated history of the US and one of the key messages is continuing the racist goals of the K ##K ##K . This is cartoon film depicted violence in elementary schools among six - year olds . . Animated key message : how gun rights are promoted for whites out ##law ##ed for blacks . It is an animated history of the US and one of the key messages is to continue the racism of the K ##K ##K . L ##I ##ve action key message : N ##RA in the southern states promote gun rights for black people . Documentary key message : the CIA fought cocaine smuggling into the U . S . in the ' 80s . [SEP]\n",
      "No.2,len:488\n",
      "lables:[CLS] P I I I I I P I I I I I I X X I I I I I I I I I X I I I I I P I I I I I I I I I X I I I I I I I I I I I I I X I I I I I I I I I I X I I I I I I I I I I X X I I P I I I I X I I I I I I I I I I I P I I I I P I I I X X I I I I I I I I I I I I X X I P I I I I I I I I I I I I I I I I I I I P X X I I I I P I X I X X I I I I I I I X I I I I I I I I I I I I I I I I X I I I I P I I I I X X X X I I I X X I X I I I I I I X X I I X X I I I I P I I I I X X I I I I I I I I I I X X I I I X I P I I I I I I I I I I I I I X I I I P I I I I I I I P I X X I I I I I I I P I I I I I I I I I X I I I I I I I I I I I I I X I I I I I I I I X X I I I I I I I I I I I I I I I I I X I I I I I I I I I I X I I I I I X I Q I I I I I I I I I I I I I I I I C I I I I I I I I I I I I I I I I I I X X I W I I I I I I I I I I X X I I X C I I X I I I I I I I I X X I I I C I I I I I I I I I I I I I I I I I I I I I X X I W X X I I I X I X I I I I I I I I I I I W I I X I I I I I I I I I X X I I I X I [SEP]\n",
      "No.3,len:446\n",
      "texts:[CLS] Animated history of the US . Of course the cartoon is highly overs ##im ##plified and most critics consider it one of the weak ##est parts of the film . But it makes a valid claim which you ignore entirely : That the strategy to promote gun rights for white people and to out ##law gun possession by black people was a way to up ##hold racism without letting an openly terrorist organization like the K ##K ##K flourish . Did the 19th century N ##RA in the southern states promote gun rights for black people ? I highly doubt it . But if they didn ' t one of their functions was to continue the racism of the K ##K ##K . This is the key message of this part of the animation which is again being ignored by its critics . B ##uel ##l shooting in Flint . You write : F ##act : The little boy was the class th ##ug already suspended from school for stabbing another kid with a pencil and had fought with Kay ##la the day before . This characterization of a six - year - old as a pencil - stabbing th ##ug is exactly the kind of h ##yster ##ia that Moore ' s film warns against . It is the typical right - wing reaction which looks for simple answers that do not con ##tra ##dict the Republican minds ##et . The kid was a little bastard and the parents were involved in drugs - - case closed . But why do people deal with drugs ? Because it ' s so much fun to do so ? It is by now well documented that the CIA tolerate ##d crack sales in US cities to fund the operation of South American con ##tras It is equally well known that the so - called war on drugs begun under the Nixon administration is a failure which has cost hundreds of billion ##s and made America the world leader in prison population ( both in relative and absolute numbers ) . Which type of rights are being discussed and promoted by which group ? Under ##cover drug operations by the N ##RA . C ##rack ##down on crack dealing by the K ##K ##K . T ##hr right to use drugs if one sees fit was discussed by the Legal Aid Society . Disc ##uss ##ion of a strategy to promote gun rights for white people was discussed by N ##RS and K ##K ##K . The discussion of gun rights being promoted by the N ##RA . Gun rights promoted by the N ##RA . [SEP]\n",
      "No.3,len:446\n",
      "lables:[CLS] P I I I I I P I I I I I I X X I I I I I I I I I X I I I I I P I I I I I I I I I X I I I I I I I I I I I I I X I I I I I I I I I I X I I I I I I I I I I X X I I P I I I I X I I I I I I I I I I I P I I I I P I I I X X I I I I I I I I I I I I X X I P I I I I I I I I I I I I I I I I I I I P X X I I I I P I X I X X I I I I I I I X I I I I I I I I I I I I I I I I X I I I I P I I I I X X X X I I I X X I X I I I I I I X X I I X X I I I I P I I I I X X I I I I I I I I I I X X I I I X I P I I I I I I I I I I I I I X I I I P I I I I I I I P I X X I I I I I I I P I I I I I I I I I X I I I I I I I I I I I I I X I I I I I I I I X X I I I I I I I I I I I I I I I I I X I I I I I I I I I I X I I I I I X I Q I I I I I I I I I I I I W X I I I I I X I W X X I I I I I I X X I W X I I I I I I I I I I I I I I I I C X X I I I I I I I I I I I I I I X I I X X I C I I I I I I I I I X I C I I I I I X I [SEP]\n",
      "No.4,len:386\n",
      "texts:[CLS] Animated history of the US . Of course the cartoon is highly overs ##im ##plified and most critics consider it one of the weak ##est parts of the film . But it makes a valid claim which you ignore entirely : That the strategy to promote gun rights for white people and to out ##law gun possession by black people was a way to up ##hold racism without letting an openly terrorist organization like the K ##K ##K flourish . Did the 19th century N ##RA in the southern states promote gun rights for black people ? I highly doubt it . But if they didn ' t one of their functions was to continue the racism of the K ##K ##K . This is the key message of this part of the animation which is again being ignored by its critics . B ##uel ##l shooting in Flint . You write : F ##act : The little boy was the class th ##ug already suspended from school for stabbing another kid with a pencil and had fought with Kay ##la the day before . This characterization of a six - year - old as a pencil - stabbing th ##ug is exactly the kind of h ##yster ##ia that Moore ' s film warns against . It is the typical right - wing reaction which looks for simple answers that do not con ##tra ##dict the Republican minds ##et . The kid was a little bastard and the parents were involved in drugs - - case closed . But why do people deal with drugs ? Because it ' s so much fun to do so ? It is by now well documented that the CIA tolerate ##d crack sales in US cities to fund the operation of South American con ##tras It is equally well known that the so - called war on drugs begun under the Nixon administration is a failure which has cost hundreds of billion ##s and made America the world leader in prison population ( both in relative and absolute numbers ) . In the author ' s mind which characterization of the B ##uel ##l school shooter is more appropriate ? T ##hu ##g or Ba ##star ##d ? Ba ##star ##d . T ##hu ##g . [SEP]\n",
      "No.4,len:386\n",
      "lables:[CLS] P I I I I I P I I I I I I X X I I I I I I I I I X I I I I I P I I I I I I I I I X I I I I I I I I I I I I I X I I I I I I I I I I X I I I I I I I I I I X X I I P I I I I X I I I I I I I I I I I P I I I I P I I I X X I I I I I I I I I I I I X X I P I I I I I I I I I I I I I I I I I I I P X X I I I I P I X I X X I I I I I I I X I I I I I I I I I I I I I I I I X I I I I P I I I I X X X X I I I X X I X I I I I I I X X I I X X I I I I P I I I I X X I I I I I I I I I I X X I I I X I P I I I I I I I I I I I I I X I I I P I I I I I I I P I X X I I I I I I I P I I I I I I I I I X I I I I I I I I I I I I I X I I I I I I I I X X I I I I I I I I I I I I I I I I I X I I I I I I I I I I X I I I I I X I Q I I X X I I I I I I X X I I I I I I I X X I I X X I C X X I W X X I [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = []\n",
    "word_piece_labels = []\n",
    "i_inc = 0\n",
    "for word_list,label in (zip(sentences,labels)):\n",
    "    temp_lable = []\n",
    "    temp_token = []\n",
    "    \n",
    "    # Add [CLS] at the front \n",
    "    temp_lable.append('[CLS]')\n",
    "    temp_token.append('[CLS]')\n",
    "    \n",
    "    for word,lab in zip(word_list,label):\n",
    "        token_list = tokenizer.tokenize(word)\n",
    "        for m,token in enumerate(token_list):\n",
    "            temp_token.append(token)\n",
    "            if m==0:\n",
    "                temp_lable.append(lab)\n",
    "            else:\n",
    "                temp_lable.append('X')  \n",
    "                \n",
    "    # Add [SEP] at the end\n",
    "    temp_lable.append('[SEP]')\n",
    "    temp_token.append('[SEP]')\n",
    "    \n",
    "    tokenized_texts.append(temp_token)\n",
    "    word_piece_labels.append(temp_lable)\n",
    "    \n",
    "    if 5 > i_inc:\n",
    "        print(\"No.%d,len:%d\"%(i_inc,len(temp_token)))\n",
    "        print(\"texts:%s\"%(\" \".join(temp_token)))\n",
    "        print(\"No.%d,len:%d\"%(i_inc,len(temp_lable)))\n",
    "        print(\"lables:%s\"%(\" \".join(temp_lable)))\n",
    "    i_inc +=1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36NKtzIoUi9y"
   },
   "source": [
    "### Setting-up token embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zf0AGkN1Ui9y"
   },
   "source": [
    "Pad or trim the text and label to fit the need for max len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgI58czOUi9z"
   },
   "outputs": [],
   "source": [
    "# Make text token into id\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hT4lASzUi91"
   },
   "outputs": [],
   "source": [
    "# Make label into id, pad with \"W\" meaning others/wrong\n",
    "# Note - Replaced \"O\" -> \"W\" (wrong)\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in word_piece_labels],\n",
    "                     maxlen=max_len, value=tag2idx[\"W\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "# print(tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q90_O7JVUi96"
   },
   "source": [
    "### Setting-up mask word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E03rTe7kUi96"
   },
   "outputs": [],
   "source": [
    "# For fine tune of predict, with token mask is 1,pad token is 0\n",
    "attention_masks = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "attention_masks[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mg7m0PyoUi99"
   },
   "source": [
    "### Setting-up segment embedding(Analysis- for sequance tagging task, it's not necessary to make this embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2mMlimtUi99"
   },
   "outputs": [],
   "source": [
    "# Since only one sentence, all the segment set to 0\n",
    "segment_ids = [[0] * len(input_id) for input_id in input_ids]\n",
    "segment_ids[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1fH7smpci0t"
   },
   "outputs": [],
   "source": [
    "# print(segment_ids) # ERROR - IOPub data rate exceeded. (TOO MUCH!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HXfDgtzuUi-F"
   },
   "source": [
    "## Load Training DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArF7Fq8fUi-J"
   },
   "outputs": [],
   "source": [
    "tr_inputs, tr_tags, tr_masks, tr_segs = input_ids, tags, attention_masks, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lYcZ4p6lUi-O",
    "outputId": "e1aae6d4-c0b0-4540-ba2e-2d63115c344f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5131, 5131)"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_inputs),len(tr_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "v38ZLJ0-tKDp",
    "outputId": "7a249f25-bf28-4eda-da82-2e4b763320c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101 24238  1607 ...     0     0     0]\n",
      " [  101 24238  1607 ...  1111  1602  1234]\n",
      " [  101 24238  1607 ... 18848  2513  1104]\n",
      " ...\n",
      " [  101   158   119 ...  1523  1113  9170]\n",
      " [  101   158   119 ...  1523  1113  9170]\n",
      " [  101   158   119 ...  1523  1113  9170]]\n"
     ]
    }
   ],
   "source": [
    "print(tr_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiMXgofyUi-T"
   },
   "source": [
    "**Set data into tensor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXbuWgNaUi-T"
   },
   "source": [
    "NOTE - Not recommend tensor.to(device) at this process, since it will run out of GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LEsSzo2Ui-U"
   },
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "tr_segs = torch.tensor(tr_segs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nR_mX0aUi-a"
   },
   "source": [
    "**Put data into data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-iD8Yay_Ui-b"
   },
   "outputs": [],
   "source": [
    "# Set batch num\n",
    "batch_num = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFWiVf50Ui-e"
   },
   "outputs": [],
   "source": [
    "# Only set token embedding, attention embedding, no segment embedding\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# Drop last can make batch training better for the last one\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4JdNTXXUi-i"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bIB2t48Ui-i"
   },
   "source": [
    "- Pre-requisite: Downloading model files in GDrive\n",
    "- Model used - BERT-base-cased\n",
    "- pytorch_model.bin: [pytorch_model.bin](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin)\n",
    "- config.json: [config.json](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqQuPl0yUi-j"
   },
   "source": [
    "**Loading BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpnX05xiUi-j"
   },
   "outputs": [],
   "source": [
    "# In this folder, contain model confg(json) and model weight(bin) files\n",
    "# pytorch_model.bin, download from: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin\n",
    "# config.json, downlaod from: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json\n",
    "model_file_address = PARENT_DIR + \"/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Gtov4DiFfinI",
    "outputId": "3e0b65f6-182f-4371-90af-8a492dc45b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  pytorch_model.bin\tvocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/gdrive/My Drive/MultiRC_NER/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jr7114QmUi-k"
   },
   "outputs": [],
   "source": [
    "# Will load config and weight with from_pretrained()\n",
    "model = BertForTokenClassification.from_pretrained(model_file_address,num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z55qnZFjUi-m"
   },
   "outputs": [],
   "source": [
    "model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nX7OqEG4Ui-o"
   },
   "outputs": [],
   "source": [
    "# Set model to GPU,if you are using GPU machine\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXgBBGaYUi-p"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: for multi GPU support\n",
    "#if n_gpu >1:\n",
    "#   model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ToxEzMs9Ui-r"
   },
   "outputs": [],
   "source": [
    "# Set epoch and grad max num\n",
    "epochs = 5\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDeMuG_qUi-t"
   },
   "outputs": [],
   "source": [
    "# Cacluate train optimization num\n",
    "num_train_optimization_steps = int( math.ceil(len(tr_inputs) / batch_num) / 1) * epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJ1liMnDUi-z"
   },
   "source": [
    "### Setting-up fine tuning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43hTJyHJUi-z"
   },
   "source": [
    "**Manual optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvuJtdG7Ui-z"
   },
   "outputs": [],
   "source": [
    "# True: fine tuning all the layers using AdamW\n",
    "# False: only fine tuning the classifier layers\n",
    "FULL_FINETUNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iv1pdD9YUi-2"
   },
   "outputs": [],
   "source": [
    "if FULL_FINETUNING:\n",
    "    # Fine tune model all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # Only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ZybMSrOUi-8"
   },
   "source": [
    "### Fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lz3zEEi_Ui-8"
   },
   "outputs": [],
   "source": [
    "# TRAIN loop\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mS_C97Sou9KF"
   },
   "outputs": [],
   "source": [
    "# Check logs for crash\n",
    "#!cat /var/log/colab-jupyter.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "DZ95271QUi--",
    "outputId": "fc5f8f40-8918-459a-dd76-41520fe46274",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5131\n",
      "  Batch size = 16\n",
      "  Num steps = 1605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch:  20%|██        | 1/5 [10:31<42:07, 631.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0983140283060493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 2/5 [21:03<31:35, 631.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00955794602195965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 3/5 [31:41<21:07, 633.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0076077595091192055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 4/5 [42:23<10:36, 636.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0066671763692284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [52:53<00:00, 634.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.005843549657220138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(tr_inputs)))\n",
    "print(\"  Batch size = %d\"%(batch_num))\n",
    "print(\"  Num steps = %d\"%(num_train_optimization_steps))\n",
    "for _ in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "        attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, scores = outputs[:2]\n",
    "      #  if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "       #     loss = loss.mean()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CHPIwbuGUi_C"
   },
   "source": [
    "## Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YsUIIUCSUi_C"
   },
   "outputs": [],
   "source": [
    "# TODO: output/ => original data, output/sample/ => sampled data\n",
    "bert_out_address = PARENT_DIR + \"/output/trained_v2_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdOW-B46Ui_I"
   },
   "outputs": [],
   "source": [
    "# Save a trained model, configuration and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dir if not exits\n",
    "if not os.path.exists(bert_out_address):\n",
    "        os.makedirs(bert_out_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2uAveVXUi_L"
   },
   "outputs": [],
   "source": [
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = bert_out_address + \"pytorch_model.bin\"\n",
    "output_config_file = bert_out_address + \"config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GWx_l9fkUi_N",
    "outputId": "c32a6e0a-403d-4d25-90e8-c050fc315679",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/gdrive/My Drive/MultiRC_NER/output/trained_v2_model/vocab.txt',)"
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model into file in drive\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(bert_out_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdOCouaEzzNb"
   },
   "source": [
    "# ----------- END OF TRAINING -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZpyxO9_zjCS"
   },
   "source": [
    "# Refer to MultiRC-NER_eval note book for EVALUATIONS & ANALYSIS"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3yWP5SKUUi_S",
    "J3BVkBWLUi_Z"
   ],
   "machine_shape": "hm",
   "name": "Copy of MultiRC-NER.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
