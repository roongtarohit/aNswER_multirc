// train and evaluate on a single task (multirc) without doing any new pretraining,
// you should set target_tasks and pretraining_tasks to the same task, set
// do_pretrain to 1, and do_target_task_training to 0.


include "defaults.conf"  // relative path to this file

// write to local storage by default for this demo
exp_name = mutlirc_bert_cased_exp
run_name = mutlirc_bert_cased_run

// train the model on multirc task dataset (intermediate training)
pretrain_tasks = "multirc"
// fine tune the model for multirc task and evaluate on the same
target_tasks = "multirc"
//   "finetune" will update the parameters of the encoders models as well as the downstream models. (This disables d_proj.)
transfer_paradigm = "finetune"

// main stages of operation
do_pretrain = 1  // Run training on the tasks in pretrain_tasks.
do_target_task_training = 0  // After do_pretrain, train on the target tasks in target_tasks.

// use a pre-trained BERT model
input_module = bert-base-cased

// Maximum number of validation checks. Will stop once this limit has been
// reached. This cannot be disabled, but if you plan to rely on max_epochs or
max_vals = 100
val_interval = 50

random_seed = 42

load_model = 0
reload_tasks = 0
reload_indexing = 0
reload_vocab = 0

classifier = mlp
classifier_hid_dim = 32
max_seq_len = 10
max_word_v_size = 1000
pair_attn = 0

d_word = 50

sent_enc = bow
skip_embs = 0

batch_size = 8

lr = 0.0003
target_train_val_interval = 10
target_train_max_vals = 10
