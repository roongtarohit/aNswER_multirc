// train and evaluate on a single task (multirc) without doing any new pretraining,
// you should set target_tasks and pretraining_tasks to the same task, set
// do_pretrain to 1, and do_target_task_training to 0.


include "defaults.conf"  // relative path to this file

// write to local storage by default for this demo
exp_name = mutlirc_bert_cased_exp
run_name = mutlirc_bert_cased_run8

// train the model on multirc task dataset (intermediate training)
pretrain_tasks = "multirc"
// fine tune the model for multirc task and evaluate on the same
target_tasks = "multirc"
//   "finetune" will update the parameters of the encoders models as well as the downstream models. (This disables d_proj.)
transfer_paradigm = "finetune"

// main stages of operation
do_pretrain = 1  // Run training on the tasks in pretrain_tasks.
do_target_task_training = 0  // After do_pretrain, dont need to train on the target tasks in target_tasks.
do_full_eval = 1 // Evaluate the trained model

// use a pre-trained BERT model: https://huggingface.co/transformers/pretrained_models.html
input_module = bert-large-cased

// validation of stopping criteria happens for every 10 gradient steps
val_interval = 10

//Stopping criteria
// Option 1: Maximum number of validation checks. Will stop once this limit has been reached.
// **Lets keep this to 2 digit**
max_vals = 20
// Option 2: Maximum no of epochs for training, will work only if the value is positive
max_epochs = 10
// Option 3: Minimum learning rate. Training will stop when our explicit LR decay lowers the LR below this point
//min_lr = 0.0001

//BERT Set up
classifier = log_reg
// Optimizer. All valid AllenNLP options are available. Use 'bert_adam' for reproducing BERT experiments.
// Warning: bert_adam is designed for cases where the number of epochs is known in advance, so it may not behave reasonably unless max_epochs is set to a reasonable positive value.
optimizer = bert_adam
// standard/expected to set this to 'none' when using GPT or BERT.
sent_enc = none
dropout = 0.1
sep_embs_for_skip = 1
skip_embs = 1

lr = 0.00001


write_preds = "val,test"
write_strict_glue_format = 1


random_seed = 22

load_model = 0
reload_tasks = 0
reload_indexing = 0
reload_vocab = 0

max_seq_len = 10
max_word_v_size = 1000
pair_attn = 0

d_word = 50
batch_size = 8
target_train_val_interval = 10
target_train_max_vals = 10
