03/19 06:58:17 PM: Git branch: develop
03/19 06:58:17 PM: Git SHA: dd631065bacec9ae53e4fc1832cfe49321974afb
03/19 06:58:17 PM: Parsed args: 
{
  "batch_size": 8,
  "classifier": "log_reg",
  "d_word": 50,
  "do_target_task_training": 0,
  "exp_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/",
  "exp_name": "mutlirc_bert_cased_exp",
  "input_module": "bert-base-cased",
  "load_model": 0,
  "local_log_path": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3/log.log",
  "lr": 0.003,
  "max_epochs": 5,
  "max_seq_len": 10,
  "max_vals": 10,
  "max_word_v_size": 1000,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "pretrain_tasks": "multirc",
  "random_seed": 402,
  "remote_log_name": "mutlirc_bert_cased_exp__mutlirc_bert_cased_run3",
  "run_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3",
  "run_name": "mutlirc_bert_cased_run3",
  "sent_enc": "bow",
  "skip_embs": 0,
  "target_tasks": "multirc",
  "target_train_max_vals": 10,
  "target_train_val_interval": 10,
  "transfer_paradigm": "finetune",
  "val_interval": 50
}
03/19 06:58:17 PM: Saved config to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3/params.conf
03/19 06:58:17 PM: Using random seed 402
03/19 06:58:17 PM: Loading tasks...
03/19 06:58:17 PM: Writing pre-preprocessed tasks to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/
03/19 06:58:17 PM: 	Loaded existing task multirc
03/19 06:58:17 PM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
03/19 06:58:17 PM: 	Finished loading tasks: multirc.
03/19 06:58:17 PM: Loading token dictionary from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab.
03/19 06:58:17 PM: 	Loaded vocab from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab
03/19 06:58:17 PM: 	Vocab namespace chars: size 100
03/19 06:58:17 PM: 	Vocab namespace bert_cased: size 28998
03/19 06:58:17 PM: 	Vocab namespace tokens: size 1004
03/19 06:58:17 PM: 	Finished building vocab.
03/19 06:58:17 PM: 	Task 'multirc', split 'train': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__train_data
03/19 06:58:17 PM: 	Task 'multirc', split 'val': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__val_data
03/19 06:58:17 PM: 	Task 'multirc', split 'test': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__test_data
03/19 06:58:17 PM: 	Finished indexing tasks
03/19 06:58:17 PM: 	Creating trimmed pretraining-only version of multirc train.
03/19 06:58:17 PM: 	Creating trimmed target-only version of multirc train.
03/19 06:58:17 PM: 	  Training on multirc
03/19 06:58:17 PM: 	  Evaluating on multirc
03/19 06:58:17 PM: 	Finished loading tasks in 0.077s
03/19 06:58:17 PM: 	 Tasks: ['multirc']
03/19 06:58:17 PM: Building model...
03/19 06:58:17 PM: Using BERT model (bert-base-cased).
03/19 06:58:17 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
03/19 06:58:17 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

03/19 06:58:17 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/19 06:58:19 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/19 06:58:19 PM: Initializing parameters
03/19 06:58:19 PM: Done initializing parameters; the following parameters are using their default initialization from their code
03/19 06:58:19 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
03/19 06:58:19 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
03/19 06:58:19 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
03/19 06:58:19 PM:    _text_field_embedder.model.pooler.dense.bias
03/19 06:58:19 PM:    _text_field_embedder.model.pooler.dense.weight
03/19 06:58:19 PM: 	Task 'multirc' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "max",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
03/19 06:58:19 PM: Model specification:
03/19 06:58:19 PM: MultiTaskModel(
  (sent_encoder): BoWSentEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
03/19 06:58:19 PM: Model parameters:
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:19 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:19 PM: 	multirc_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/19 06:58:19 PM: 	multirc_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
03/19 06:58:19 PM: Total number of parameters: 108311810 (1.08312e+08)
03/19 06:58:19 PM: Number of trainable parameters: 108311810 (1.08312e+08)
03/19 06:58:19 PM: Finished building model in 1.979s
03/19 06:58:19 PM: Will run the following steps for this experiment:
Training model on tasks: multirc 
Evaluating model on tasks: multirc 

03/19 06:58:19 PM: Training...
03/19 06:58:19 PM: patience = 5
03/19 06:58:19 PM: val_interval = 50
03/19 06:58:19 PM: max_vals = 10
03/19 06:58:19 PM: cuda_device = -1
03/19 06:58:19 PM: grad_norm = 5.0
03/19 06:58:19 PM: grad_clipping = None
03/19 06:58:19 PM: lr_decay = 0.99
03/19 06:58:19 PM: min_lr = 1e-06
03/19 06:58:19 PM: keep_all_checkpoints = 0
03/19 06:58:19 PM: val_data_limit = 5000
03/19 06:58:19 PM: max_epochs = 5
03/19 06:58:19 PM: dec_val_scale = 250
03/19 06:58:19 PM: training_data_fraction = 1
03/19 06:58:19 PM: accumulation_steps = 1
03/19 06:58:19 PM: type = bert_adam
03/19 06:58:19 PM: parameter_groups = None
03/19 06:58:19 PM: Number of trainable parameters: 108311810
03/19 06:58:19 PM: infer_type_and_cast = True
03/19 06:58:19 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 06:58:19 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 06:58:19 PM: lr = 0.003
03/19 06:58:19 PM: t_total = 500
03/19 06:58:19 PM: warmup = 0.1
03/19 06:58:19 PM: type = reduce_on_plateau
03/19 06:58:19 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 06:58:19 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 06:58:19 PM: mode = max
03/19 06:58:19 PM: factor = 0.5
03/19 06:58:19 PM: patience = 1
03/19 06:58:19 PM: threshold = 0.0001
03/19 06:58:19 PM: threshold_mode = abs
03/19 06:58:19 PM: verbose = True
03/19 06:58:19 PM: Starting training without restoring from a checkpoint.
03/19 06:58:19 PM: Training examples per task, before any subsampling: {'multirc': 27243}
03/19 06:58:19 PM: Beginning training with stopping criteria based on metric: multirc_avg
03/19 06:58:30 PM: Update 5: task multirc, steps since last val 5 (total steps = 5): ans_f1: 0.3125, qst_f1: 0.1316, em: 0.4211, avg: 0.3668, multirc_loss: 0.9632
03/19 06:58:42 PM: Update 11: task multirc, steps since last val 11 (total steps = 11): ans_f1: 0.4828, qst_f1: 0.2471, em: 0.4706, avg: 0.4767, multirc_loss: 1.0470
03/19 06:58:54 PM: Update 17: task multirc, steps since last val 17 (total steps = 17): ans_f1: 0.4878, qst_f1: 0.2290, em: 0.5191, avg: 0.5034, multirc_loss: 1.0087
03/19 06:59:05 PM: Update 23: task multirc, steps since last val 23 (total steps = 23): ans_f1: 0.4750, qst_f1: 0.2095, em: 0.5257, avg: 0.5004, multirc_loss: 0.9860
03/19 06:59:18 PM: Update 29: task multirc, steps since last val 29 (total steps = 29): ans_f1: 0.4246, qst_f1: 0.1682, em: 0.5324, avg: 0.4785, multirc_loss: 1.0039
03/19 06:59:30 PM: Update 35: task multirc, steps since last val 35 (total steps = 35): ans_f1: 0.4483, qst_f1: 0.1905, em: 0.5135, avg: 0.4809, multirc_loss: 1.0200
03/19 06:59:40 PM: Update 40: task multirc, steps since last val 40 (total steps = 40): ans_f1: 0.4177, qst_f1: 0.1689, em: 0.5137, avg: 0.4657, multirc_loss: 1.0363
03/19 06:59:50 PM: Update 45: task multirc, steps since last val 45 (total steps = 45): ans_f1: 0.4488, qst_f1: 0.1959, em: 0.4985, avg: 0.4737, multirc_loss: 1.0662
03/19 07:00:00 PM: Update 50: task multirc, steps since last val 50 (total steps = 50): ans_f1: 0.4713, qst_f1: 0.2124, em: 0.5069, avg: 0.4891, multirc_loss: 1.0703
03/19 07:00:00 PM: ***** Step 50 / Validation 1 *****
03/19 07:00:01 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 07:00:01 PM: Validating...
03/19 07:00:11 PM: Evaluate: task multirc, batch 71 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 1.9121
03/19 07:00:21 PM: Evaluate: task multirc, batch 133 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0048, avg: 0.0024, multirc_loss: 1.7542
03/19 07:00:31 PM: Evaluate: task multirc, batch 197 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0033, avg: 0.0017, multirc_loss: 1.7017
03/19 07:00:41 PM: Evaluate: task multirc, batch 261 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0048, avg: 0.0024, multirc_loss: 1.6473
03/19 07:00:52 PM: Evaluate: task multirc, batch 327 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0019, multirc_loss: 1.6340
03/19 07:01:02 PM: Evaluate: task multirc, batch 391 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0033, avg: 0.0016, multirc_loss: 1.6592
03/19 07:01:12 PM: Evaluate: task multirc, batch 457 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0027, avg: 0.0014, multirc_loss: 1.6943
03/19 07:01:23 PM: Evaluate: task multirc, batch 526 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 1.6752
03/19 07:01:33 PM: Evaluate: task multirc, batch 590 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 1.6581
03/19 07:01:36 PM: Best result seen so far for multirc.
03/19 07:01:36 PM: Best result seen so far for micro.
03/19 07:01:36 PM: Best result seen so far for macro.
03/19 07:01:36 PM: Updating LR scheduler:
03/19 07:01:36 PM: 	Best result seen so far for macro_avg: 0.002
03/19 07:01:36 PM: 	# validation passes without improvement: 0
03/19 07:01:36 PM: multirc_loss: training: 1.070254 validation: 1.668848
03/19 07:01:36 PM: macro_avg: validation: 0.001574
03/19 07:01:36 PM: micro_avg: validation: 0.001574
03/19 07:01:36 PM: multirc_ans_f1: training: 0.471264 validation: 0.000000
03/19 07:01:36 PM: multirc_qst_f1: training: 0.212373 validation: 0.000000
03/19 07:01:36 PM: multirc_em: training: 0.506925 validation: 0.003148
03/19 07:01:36 PM: multirc_avg: training: 0.489095 validation: 0.001574
03/19 07:01:36 PM: Global learning rate: 0.003
03/19 07:01:36 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3
03/19 07:01:44 PM: Update 53: task multirc, steps since last val 3 (total steps = 53): ans_f1: 0.2500, qst_f1: 0.0833, em: 0.5000, avg: 0.3750, multirc_loss: 0.8330
03/19 07:01:55 PM: Update 59: task multirc, steps since last val 9 (total steps = 59): ans_f1: 0.2593, qst_f1: 0.0972, em: 0.4444, avg: 0.3519, multirc_loss: 0.8251
03/19 07:02:05 PM: Update 64: task multirc, steps since last val 14 (total steps = 64): ans_f1: 0.4423, qst_f1: 0.2054, em: 0.4821, avg: 0.4622, multirc_loss: 0.8534
03/19 07:02:17 PM: Update 70: task multirc, steps since last val 20 (total steps = 70): ans_f1: 0.4275, qst_f1: 0.1818, em: 0.5195, avg: 0.4735, multirc_loss: 0.8433
03/19 07:02:28 PM: Update 75: task multirc, steps since last val 25 (total steps = 75): ans_f1: 0.3896, qst_f1: 0.1571, em: 0.5183, avg: 0.4540, multirc_loss: 0.8641
03/19 07:02:40 PM: Update 81: task multirc, steps since last val 31 (total steps = 81): ans_f1: 0.4286, qst_f1: 0.1901, em: 0.5021, avg: 0.4653, multirc_loss: 0.9028
03/19 07:02:51 PM: Update 86: task multirc, steps since last val 36 (total steps = 86): ans_f1: 0.4052, qst_f1: 0.1710, em: 0.5092, avg: 0.4572, multirc_loss: 0.8887
03/19 07:03:02 PM: Update 91: task multirc, steps since last val 41 (total steps = 91): ans_f1: 0.3893, qst_f1: 0.1629, em: 0.4984, avg: 0.4438, multirc_loss: 0.8764
03/19 07:03:13 PM: Update 96: task multirc, steps since last val 46 (total steps = 96): ans_f1: 0.4092, qst_f1: 0.1763, em: 0.5000, avg: 0.4546, multirc_loss: 0.8849
03/19 07:03:21 PM: ***** Step 100 / Validation 2 *****
03/19 07:03:21 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 07:03:21 PM: Validating...
03/19 07:03:23 PM: Evaluate: task multirc, batch 10 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 2.5354
03/19 07:03:33 PM: Evaluate: task multirc, batch 75 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 2.9227
03/19 07:03:43 PM: Evaluate: task multirc, batch 137 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 2.6452
03/19 07:03:53 PM: Evaluate: task multirc, batch 208 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0062, avg: 0.0031, multirc_loss: 2.5418
03/19 07:04:04 PM: Evaluate: task multirc, batch 276 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0046, avg: 0.0023, multirc_loss: 2.5208
03/19 07:04:14 PM: Evaluate: task multirc, batch 345 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0019, multirc_loss: 2.4879
03/19 07:04:24 PM: Evaluate: task multirc, batch 411 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0031, avg: 0.0015, multirc_loss: 2.5836
03/19 07:04:35 PM: Evaluate: task multirc, batch 479 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0019, multirc_loss: 2.5826
03/19 07:04:45 PM: Evaluate: task multirc, batch 546 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 2.5730
03/19 07:04:55 PM: Updating LR scheduler:
03/19 07:04:55 PM: 	Best result seen so far for macro_avg: 0.002
03/19 07:04:55 PM: 	# validation passes without improvement: 1
03/19 07:04:55 PM: multirc_loss: training: 0.875148 validation: 2.553314
03/19 07:04:55 PM: macro_avg: validation: 0.001574
03/19 07:04:55 PM: micro_avg: validation: 0.001574
03/19 07:04:55 PM: multirc_ans_f1: training: 0.426035 validation: 0.000000
03/19 07:04:55 PM: multirc_qst_f1: training: 0.190605 validation: 0.000000
03/19 07:04:55 PM: multirc_em: training: 0.504065 validation: 0.003148
03/19 07:04:55 PM: multirc_avg: training: 0.465050 validation: 0.001574
03/19 07:04:55 PM: Global learning rate: 0.003
03/19 07:04:55 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3
03/19 07:04:58 PM: Update 101: task multirc, steps since last val 1 (total steps = 101): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.3750, avg: 0.1875, multirc_loss: 0.8037
03/19 07:05:10 PM: Update 106: task multirc, steps since last val 6 (total steps = 106): ans_f1: 0.2222, qst_f1: 0.0833, em: 0.4167, avg: 0.3194, multirc_loss: 0.8727
03/19 07:05:21 PM: Update 111: task multirc, steps since last val 11 (total steps = 111): ans_f1: 0.2778, qst_f1: 0.1149, em: 0.4023, avg: 0.3400, multirc_loss: 0.8319
03/19 07:05:32 PM: Update 116: task multirc, steps since last val 16 (total steps = 116): ans_f1: 0.3019, qst_f1: 0.1260, em: 0.4173, avg: 0.3596, multirc_loss: 0.8217
03/19 07:05:42 PM: Update 121: task multirc, steps since last val 21 (total steps = 121): ans_f1: 0.3053, qst_f1: 0.1212, em: 0.4606, avg: 0.3830, multirc_loss: 0.8037
03/19 07:05:53 PM: Update 126: task multirc, steps since last val 26 (total steps = 126): ans_f1: 0.2933, qst_f1: 0.1089, em: 0.4950, avg: 0.3942, multirc_loss: 0.7808
03/19 07:06:03 PM: Update 131: task multirc, steps since last val 31 (total steps = 131): ans_f1: 0.2857, qst_f1: 0.1064, em: 0.4894, avg: 0.3875, multirc_loss: 0.7855
03/19 07:06:13 PM: Update 136: task multirc, steps since last val 36 (total steps = 136): ans_f1: 0.2798, qst_f1: 0.0996, em: 0.5055, avg: 0.3927, multirc_loss: 0.7760
03/19 07:06:24 PM: Update 141: task multirc, steps since last val 41 (total steps = 141): ans_f1: 0.2778, qst_f1: 0.0979, em: 0.5050, avg: 0.3914, multirc_loss: 0.7735
03/19 07:06:35 PM: Update 146: task multirc, steps since last val 46 (total steps = 146): ans_f1: 0.2720, qst_f1: 0.0989, em: 0.4807, avg: 0.3764, multirc_loss: 0.7872
03/19 07:06:43 PM: ***** Step 150 / Validation 3 *****
03/19 07:06:43 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 07:06:43 PM: Validating...
03/19 07:06:45 PM: Evaluate: task multirc, batch 12 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 1.5577
03/19 07:06:55 PM: Evaluate: task multirc, batch 84 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0072, avg: 0.0036, multirc_loss: 1.6590
03/19 07:07:05 PM: Evaluate: task multirc, batch 154 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 1.5194
03/19 07:07:15 PM: Evaluate: task multirc, batch 225 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0057, avg: 0.0029, multirc_loss: 1.4607
03/19 07:07:26 PM: Evaluate: task multirc, batch 295 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0065, avg: 0.0032, multirc_loss: 1.4667
03/19 07:07:36 PM: Evaluate: task multirc, batch 360 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 1.4773
03/19 07:07:46 PM: Evaluate: task multirc, batch 417 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0030, avg: 0.0015, multirc_loss: 1.5033
03/19 07:07:57 PM: Evaluate: task multirc, batch 477 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0052, avg: 0.0026, multirc_loss: 1.5066
03/19 07:08:08 PM: Evaluate: task multirc, batch 541 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0046, avg: 0.0023, multirc_loss: 1.4990
03/19 07:08:18 PM: Updating LR scheduler:
03/19 07:08:18 PM: 	Best result seen so far for macro_avg: 0.002
03/19 07:08:18 PM: 	# validation passes without improvement: 0
03/19 07:08:18 PM: multirc_loss: training: 0.786757 validation: 1.489905
03/19 07:08:18 PM: macro_avg: validation: 0.001574
03/19 07:08:18 PM: micro_avg: validation: 0.001574
03/19 07:08:18 PM: multirc_ans_f1: training: 0.320558 validation: 0.000000
03/19 07:08:18 PM: multirc_qst_f1: training: 0.122711 validation: 0.000000
03/19 07:08:18 PM: multirc_em: training: 0.486264 validation: 0.003148
03/19 07:08:18 PM: multirc_avg: training: 0.403411 validation: 0.001574
03/19 07:08:18 PM: Global learning rate: 0.0015
03/19 07:08:18 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3
03/19 07:08:21 PM: Update 151: task multirc, steps since last val 1 (total steps = 151): ans_f1: 0.8000, qst_f1: 0.5000, em: 0.7500, avg: 0.7750, multirc_loss: 0.7265
03/19 07:08:31 PM: Update 156: task multirc, steps since last val 6 (total steps = 156): ans_f1: 0.5938, qst_f1: 0.4043, em: 0.4681, avg: 0.5309, multirc_loss: 0.7860
03/19 07:08:41 PM: Update 161: task multirc, steps since last val 11 (total steps = 161): ans_f1: 0.6034, qst_f1: 0.4024, em: 0.4634, avg: 0.5334, multirc_loss: 0.7681
03/19 07:08:53 PM: Update 167: task multirc, steps since last val 17 (total steps = 167): ans_f1: 0.5548, qst_f1: 0.3177, em: 0.4844, avg: 0.5196, multirc_loss: 0.7456
03/19 07:09:04 PM: Update 172: task multirc, steps since last val 22 (total steps = 172): ans_f1: 0.5348, qst_f1: 0.2894, em: 0.4939, avg: 0.5143, multirc_loss: 0.7394
03/19 07:09:15 PM: Update 177: task multirc, steps since last val 27 (total steps = 177): ans_f1: 0.5023, qst_f1: 0.2548, em: 0.4950, avg: 0.4987, multirc_loss: 0.7380
03/19 07:09:26 PM: Update 182: task multirc, steps since last val 32 (total steps = 182): ans_f1: 0.4980, qst_f1: 0.2474, em: 0.5106, avg: 0.5043, multirc_loss: 0.7196
03/19 07:09:38 PM: Update 187: task multirc, steps since last val 37 (total steps = 187): ans_f1: 0.4876, qst_f1: 0.2474, em: 0.4962, avg: 0.4919, multirc_loss: 0.7241
03/19 07:09:51 PM: Update 192: task multirc, steps since last val 42 (total steps = 192): ans_f1: 0.5061, qst_f1: 0.2629, em: 0.4983, avg: 0.5022, multirc_loss: 0.7227
03/19 07:10:02 PM: Update 196: task multirc, steps since last val 46 (total steps = 196): ans_f1: 0.5189, qst_f1: 0.2787, em: 0.4954, avg: 0.5072, multirc_loss: 0.7195
03/19 07:10:12 PM: Update 200: task multirc, steps since last val 50 (total steps = 200): ans_f1: 0.5307, qst_f1: 0.2894, em: 0.4971, avg: 0.5139, multirc_loss: 0.7186
03/19 07:10:12 PM: ***** Step 200 / Validation 4 *****
03/19 07:10:12 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 07:10:12 PM: Validating...
03/19 07:10:22 PM: Evaluate: task multirc, batch 53 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 2.5293
03/19 07:10:32 PM: Evaluate: task multirc, batch 110 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 2.2592
03/19 07:10:43 PM: Evaluate: task multirc, batch 164 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 2.1346
03/19 07:10:53 PM: Evaluate: task multirc, batch 210 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0061, avg: 0.0031, multirc_loss: 2.0979
03/19 07:11:03 PM: Evaluate: task multirc, batch 258 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0049, avg: 0.0025, multirc_loss: 2.0725
03/19 07:11:14 PM: Evaluate: task multirc, batch 315 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0041, avg: 0.0020, multirc_loss: 2.0550
03/19 07:11:24 PM: Evaluate: task multirc, batch 376 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 2.0800
03/19 07:11:35 PM: Evaluate: task multirc, batch 439 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0028, avg: 0.0014, multirc_loss: 2.1317
03/19 07:11:45 PM: Evaluate: task multirc, batch 501 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0038, avg: 0.0019, multirc_loss: 2.1258
03/19 07:11:56 PM: Evaluate: task multirc, batch 564 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 2.1077
03/19 07:12:03 PM: Updating LR scheduler:
03/19 07:12:03 PM: 	Best result seen so far for macro_avg: 0.002
03/19 07:12:03 PM: 	# validation passes without improvement: 1
03/19 07:12:03 PM: multirc_loss: training: 0.718616 validation: 2.106793
03/19 07:12:03 PM: macro_avg: validation: 0.001574
03/19 07:12:03 PM: micro_avg: validation: 0.001574
03/19 07:12:03 PM: multirc_ans_f1: training: 0.530713 validation: 0.000000
03/19 07:12:03 PM: multirc_qst_f1: training: 0.289429 validation: 0.000000
03/19 07:12:03 PM: multirc_em: training: 0.497143 validation: 0.003148
03/19 07:12:03 PM: multirc_avg: training: 0.513928 validation: 0.001574
03/19 07:12:03 PM: Global learning rate: 0.0015
03/19 07:12:03 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3
03/19 07:12:07 PM: Update 201: task multirc, steps since last val 1 (total steps = 201): ans_f1: 0.7500, qst_f1: 0.3750, em: 0.7500, avg: 0.7500, multirc_loss: 0.6889
03/19 07:12:18 PM: Update 206: task multirc, steps since last val 6 (total steps = 206): ans_f1: 0.5882, qst_f1: 0.3125, em: 0.5625, avg: 0.5754, multirc_loss: 0.7305
03/19 07:12:29 PM: Update 211: task multirc, steps since last val 11 (total steps = 211): ans_f1: 0.5227, qst_f1: 0.2636, em: 0.5116, avg: 0.5172, multirc_loss: 0.7296
03/19 07:12:41 PM: Update 216: task multirc, steps since last val 16 (total steps = 216): ans_f1: 0.5124, qst_f1: 0.2453, em: 0.5280, avg: 0.5202, multirc_loss: 0.7283
03/19 07:12:53 PM: Update 221: task multirc, steps since last val 21 (total steps = 221): ans_f1: 0.4605, qst_f1: 0.2127, em: 0.4969, avg: 0.4787, multirc_loss: 0.7325
03/19 07:13:05 PM: Update 226: task multirc, steps since last val 26 (total steps = 226): ans_f1: 0.4817, qst_f1: 0.2273, em: 0.5051, avg: 0.4934, multirc_loss: 0.7144
03/19 07:13:17 PM: Update 231: task multirc, steps since last val 31 (total steps = 231): ans_f1: 0.4796, qst_f1: 0.2208, em: 0.5171, avg: 0.4984, multirc_loss: 0.7113
03/19 07:13:28 PM: Update 235: task multirc, steps since last val 35 (total steps = 235): ans_f1: 0.4762, qst_f1: 0.2235, em: 0.5057, avg: 0.4910, multirc_loss: 0.7121
03/19 07:13:39 PM: Update 240: task multirc, steps since last val 40 (total steps = 240): ans_f1: 0.4604, qst_f1: 0.2106, em: 0.5068, avg: 0.4836, multirc_loss: 0.7195
03/19 07:13:50 PM: Update 245: task multirc, steps since last val 45 (total steps = 245): ans_f1: 0.4695, qst_f1: 0.2138, em: 0.5106, avg: 0.4900, multirc_loss: 0.7135
03/19 07:14:02 PM: Update 250: task multirc, steps since last val 50 (total steps = 250): ans_f1: 0.4699, qst_f1: 0.2170, em: 0.5055, avg: 0.4877, multirc_loss: 0.7137
03/19 07:14:02 PM: ***** Step 250 / Validation 5 *****
03/19 07:14:02 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 07:14:02 PM: Validating...
03/19 07:14:13 PM: Evaluate: task multirc, batch 58 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 3.0126
03/19 07:14:23 PM: Evaluate: task multirc, batch 107 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 2.6306
03/19 07:14:33 PM: Evaluate: task multirc, batch 163 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 2.4872
03/19 07:14:43 PM: Evaluate: task multirc, batch 211 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0091, avg: 0.0046, multirc_loss: 2.4341
03/19 07:14:53 PM: Evaluate: task multirc, batch 260 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0048, avg: 0.0024, multirc_loss: 2.4217
03/19 07:15:04 PM: Evaluate: task multirc, batch 300 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0042, avg: 0.0021, multirc_loss: 2.4068
03/19 07:15:14 PM: Evaluate: task multirc, batch 349 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0018, multirc_loss: 2.3932
03/19 07:15:25 PM: Evaluate: task multirc, batch 403 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0031, avg: 0.0016, multirc_loss: 2.4564
03/19 07:15:35 PM: Evaluate: task multirc, batch 462 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0027, avg: 0.0013, multirc_loss: 2.4899
03/19 07:15:46 PM: Evaluate: task multirc, batch 524 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 2.4633
03/19 07:15:57 PM: Evaluate: task multirc, batch 583 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0033, avg: 0.0016, multirc_loss: 2.4401
03/19 07:16:02 PM: Updating LR scheduler:
03/19 07:16:02 PM: 	Best result seen so far for macro_avg: 0.002
03/19 07:16:02 PM: 	# validation passes without improvement: 0
03/19 07:16:02 PM: multirc_loss: training: 0.713661 validation: 2.452641
03/19 07:16:02 PM: macro_avg: validation: 0.001574
03/19 07:16:02 PM: micro_avg: validation: 0.001574
03/19 07:16:02 PM: multirc_ans_f1: training: 0.469914 validation: 0.000000
03/19 07:16:02 PM: multirc_qst_f1: training: 0.217033 validation: 0.000000
03/19 07:16:02 PM: multirc_em: training: 0.505495 validation: 0.003148
03/19 07:16:02 PM: multirc_avg: training: 0.487704 validation: 0.001574
03/19 07:16:02 PM: Global learning rate: 0.00075
03/19 07:16:02 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3
03/19 07:16:08 PM: Update 252: task multirc, steps since last val 2 (total steps = 252): ans_f1: 0.4615, qst_f1: 0.1875, em: 0.5625, avg: 0.5120, multirc_loss: 0.6869
03/19 07:16:20 PM: Update 257: task multirc, steps since last val 7 (total steps = 257): ans_f1: 0.3590, qst_f1: 0.1321, em: 0.5472, avg: 0.4531, multirc_loss: 0.6857
03/19 07:16:31 PM: Update 262: task multirc, steps since last val 12 (total steps = 262): ans_f1: 0.3881, qst_f1: 0.1407, em: 0.5556, avg: 0.4718, multirc_loss: 0.6936
03/19 07:16:43 PM: Update 267: task multirc, steps since last val 17 (total steps = 267): ans_f1: 0.3579, qst_f1: 0.1312, em: 0.5433, avg: 0.4506, multirc_loss: 0.7008
03/19 07:16:55 PM: Update 272: task multirc, steps since last val 22 (total steps = 272): ans_f1: 0.4000, qst_f1: 0.1554, em: 0.5521, avg: 0.4761, multirc_loss: 0.7043
03/19 07:17:07 PM: Update 277: task multirc, steps since last val 27 (total steps = 277): ans_f1: 0.4024, qst_f1: 0.1617, em: 0.5400, avg: 0.4712, multirc_loss: 0.6995
03/19 07:17:18 PM: Update 282: task multirc, steps since last val 32 (total steps = 282): ans_f1: 0.4041, qst_f1: 0.1659, em: 0.5411, avg: 0.4726, multirc_loss: 0.7073
03/19 07:17:30 PM: Update 287: task multirc, steps since last val 37 (total steps = 287): ans_f1: 0.4286, qst_f1: 0.1779, em: 0.5526, avg: 0.4906, multirc_loss: 0.6938
03/19 07:17:42 PM: Update 292: task multirc, steps since last val 42 (total steps = 292): ans_f1: 0.4644, qst_f1: 0.2022, em: 0.5567, avg: 0.5105, multirc_loss: 0.6925
03/19 07:17:54 PM: Update 297: task multirc, steps since last val 47 (total steps = 297): ans_f1: 0.4856, qst_f1: 0.2156, em: 0.5479, avg: 0.5168, multirc_loss: 0.6914
03/19 07:18:01 PM: ***** Step 300 / Validation 6 *****
03/19 07:18:01 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 07:18:01 PM: Validating...
03/19 07:18:04 PM: Evaluate: task multirc, batch 14 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 2.8395
03/19 07:18:14 PM: Evaluate: task multirc, batch 64 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0091, avg: 0.0045, multirc_loss: 3.0021
03/19 07:18:24 PM: Evaluate: task multirc, batch 112 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 2.7082
03/19 07:18:34 PM: Evaluate: task multirc, batch 164 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 2.5544
03/19 07:18:45 PM: Evaluate: task multirc, batch 216 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0060, avg: 0.0030, multirc_loss: 2.4815
03/19 07:18:55 PM: Evaluate: task multirc, batch 271 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0047, avg: 0.0023, multirc_loss: 2.4968
03/19 07:19:05 PM: Evaluate: task multirc, batch 318 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0060, avg: 0.0030, multirc_loss: 2.4543
03/19 07:19:16 PM: Evaluate: task multirc, batch 364 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0035, avg: 0.0018, multirc_loss: 2.5021
03/19 07:19:26 PM: Evaluate: task multirc, batch 412 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0030, avg: 0.0015, multirc_loss: 2.5501
03/19 07:19:37 PM: Evaluate: task multirc, batch 460 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0027, avg: 0.0013, multirc_loss: 2.5577
03/19 07:19:48 PM: Evaluate: task multirc, batch 515 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0018, multirc_loss: 2.5290
03/19 07:19:58 PM: Evaluate: task multirc, batch 575 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0044, avg: 0.0022, multirc_loss: 2.5110
03/19 07:20:03 PM: Updating LR scheduler:
03/19 07:20:03 PM: 	Best result seen so far for macro_avg: 0.002
03/19 07:20:03 PM: 	# validation passes without improvement: 1
03/19 07:20:03 PM: multirc_loss: training: 0.691060 validation: 2.521040
03/19 07:20:03 PM: macro_avg: validation: 0.001574
03/19 07:20:03 PM: micro_avg: validation: 0.001574
03/19 07:20:03 PM: multirc_ans_f1: training: 0.486322 validation: 0.000000
03/19 07:20:03 PM: multirc_qst_f1: training: 0.214689 validation: 0.000000
03/19 07:20:03 PM: multirc_em: training: 0.550847 validation: 0.003148
03/19 07:20:03 PM: multirc_avg: training: 0.518585 validation: 0.001574
03/19 07:20:03 PM: Global learning rate: 0.00075
03/19 07:20:03 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3
03/19 07:20:09 PM: Update 302: task multirc, steps since last val 2 (total steps = 302): ans_f1: 0.6667, qst_f1: 0.3125, em: 0.6875, avg: 0.6771, multirc_loss: 0.6811
03/19 07:20:19 PM: Update 307: task multirc, steps since last val 7 (total steps = 307): ans_f1: 0.6333, qst_f1: 0.3272, em: 0.5926, avg: 0.6130, multirc_loss: 0.6626
03/19 07:20:30 PM: Update 312: task multirc, steps since last val 12 (total steps = 312): ans_f1: 0.6296, qst_f1: 0.3477, em: 0.5699, avg: 0.5998, multirc_loss: 0.6789
03/19 07:20:40 PM: Update 317: task multirc, steps since last val 17 (total steps = 317): ans_f1: 0.6259, qst_f1: 0.3333, em: 0.5865, avg: 0.6062, multirc_loss: 0.6745
03/19 07:20:51 PM: Update 322: task multirc, steps since last val 22 (total steps = 322): ans_f1: 0.6096, qst_f1: 0.3136, em: 0.5680, avg: 0.5888, multirc_loss: 0.6815
03/19 07:21:02 PM: Update 327: task multirc, steps since last val 27 (total steps = 327): ans_f1: 0.5909, qst_f1: 0.2961, em: 0.5631, avg: 0.5770, multirc_loss: 0.6865
03/19 07:21:12 PM: Update 332: task multirc, steps since last val 32 (total steps = 332): ans_f1: 0.5681, qst_f1: 0.2851, em: 0.5413, avg: 0.5547, multirc_loss: 0.6928
03/19 07:21:23 PM: Update 337: task multirc, steps since last val 37 (total steps = 337): ans_f1: 0.5442, qst_f1: 0.2616, em: 0.5412, avg: 0.5427, multirc_loss: 0.6885
03/19 07:21:33 PM: Update 342: task multirc, steps since last val 42 (total steps = 342): ans_f1: 0.5304, qst_f1: 0.2516, em: 0.5382, avg: 0.5343, multirc_loss: 0.6881
03/19 07:21:44 PM: Update 347: task multirc, steps since last val 47 (total steps = 347): ans_f1: 0.5060, qst_f1: 0.2338, em: 0.5333, avg: 0.5196, multirc_loss: 0.6903
03/19 07:21:51 PM: ***** Step 350 / Validation 7 *****
03/19 07:21:51 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 07:21:51 PM: Validating...
03/19 07:21:54 PM: Evaluate: task multirc, batch 24 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 3.4690
03/19 07:22:04 PM: Evaluate: task multirc, batch 88 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 3.3363
03/19 07:22:15 PM: Evaluate: task multirc, batch 147 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 3.0585
03/19 07:22:25 PM: Evaluate: task multirc, batch 208 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0062, avg: 0.0031, multirc_loss: 2.9874
03/19 07:22:35 PM: Evaluate: task multirc, batch 271 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0047, avg: 0.0023, multirc_loss: 2.9721
03/19 07:22:46 PM: Evaluate: task multirc, batch 335 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0038, avg: 0.0019, multirc_loss: 2.9432
03/19 07:22:56 PM: Evaluate: task multirc, batch 398 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0048, avg: 0.0024, multirc_loss: 2.9882
03/19 07:23:07 PM: Evaluate: task multirc, batch 462 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0027, avg: 0.0013, multirc_loss: 3.0466
03/19 07:23:17 PM: Evaluate: task multirc, batch 522 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0048, avg: 0.0024, multirc_loss: 3.0137
03/19 07:23:28 PM: Evaluate: task multirc, batch 589 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 2.9834
03/19 07:23:31 PM: Updating LR scheduler:
03/19 07:23:31 PM: 	Best result seen so far for macro_avg: 0.002
03/19 07:23:31 PM: 	# validation passes without improvement: 0
03/19 07:23:31 PM: Ran out of early stopping patience. Stopping training.
03/19 07:23:31 PM: multirc_loss: training: 0.692352 validation: 3.000933
03/19 07:23:31 PM: macro_avg: validation: 0.001574
03/19 07:23:31 PM: micro_avg: validation: 0.001574
03/19 07:23:31 PM: multirc_ans_f1: training: 0.497175 validation: 0.000000
03/19 07:23:31 PM: multirc_qst_f1: training: 0.228311 validation: 0.000000
03/19 07:23:31 PM: multirc_em: training: 0.528767 validation: 0.003148
03/19 07:23:31 PM: multirc_avg: training: 0.512971 validation: 0.001574
03/19 07:23:31 PM: Global learning rate: 0.000375
03/19 07:23:31 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3
03/19 07:23:32 PM: Stopped training after 7 validation checks
03/19 07:23:32 PM: Trained multirc for 350 steps or 0.103 epochs
03/19 07:23:32 PM: ***** VALIDATION RESULTS *****
03/19 07:23:32 PM: multirc_avg (for best val pass 1): multirc_loss: 1.66885, macro_avg: 0.00157, micro_avg: 0.00157, multirc_ans_f1: 0.00000, multirc_qst_f1: 0.00000, multirc_em: 0.00315, multirc_avg: 0.00157
03/19 07:23:32 PM: micro_avg (for best val pass 1): multirc_loss: 1.66885, macro_avg: 0.00157, micro_avg: 0.00157, multirc_ans_f1: 0.00000, multirc_qst_f1: 0.00000, multirc_em: 0.00315, multirc_avg: 0.00157
03/19 07:23:32 PM: macro_avg (for best val pass 1): multirc_loss: 1.66885, macro_avg: 0.00157, micro_avg: 0.00157, multirc_ans_f1: 0.00000, multirc_qst_f1: 0.00000, multirc_em: 0.00315, multirc_avg: 0.00157
03/19 07:23:32 PM: Evaluating...
03/19 07:23:33 PM: Loaded model state from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3/model_state_pretrain_val_1.best.th
03/19 07:23:33 PM: Evaluating on: multirc, split: val
03/19 07:24:03 PM: 	Task multirc: batch 202
03/19 07:24:33 PM: 	Task multirc: batch 395
03/19 07:25:03 PM: 	Task multirc: batch 593
03/19 07:25:05 PM: Task 'multirc': sorting predictions by 'idx'
03/19 07:25:05 PM: Finished evaluating on: multirc
03/19 07:25:05 PM: Writing results for split 'val' to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/results.tsv
03/19 07:25:05 PM: micro_avg: 0.002, macro_avg: 0.002, multirc_ans_f1: 0.000, multirc_qst_f1: 0.000, multirc_em: 0.003, multirc_avg: 0.002
03/19 07:25:05 PM: Done!
03/19 07:34:48 PM: Git branch: develop
03/19 07:34:48 PM: Git SHA: dd631065bacec9ae53e4fc1832cfe49321974afb
03/19 07:34:49 PM: Parsed args: 
{
  "batch_size": 8,
  "d_word": 50,
  "do_target_task_training": 0,
  "exp_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/",
  "exp_name": "mutlirc_bert_cased_exp",
  "input_module": "bert-base-cased",
  "load_model": 0,
  "local_log_path": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3/log.log",
  "lr": 0.005,
  "max_epochs": 10,
  "max_seq_len": 10,
  "max_vals": 20,
  "max_word_v_size": 1000,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "pretrain_tasks": "multirc",
  "random_seed": 22,
  "remote_log_name": "mutlirc_bert_cased_exp__mutlirc_bert_cased_run3",
  "run_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3",
  "run_name": "mutlirc_bert_cased_run3",
  "sent_enc": "bow",
  "skip_embs": 0,
  "target_tasks": "multirc",
  "target_train_max_vals": 10,
  "target_train_val_interval": 10,
  "transfer_paradigm": "finetune",
  "val_interval": 50
}
03/19 07:34:49 PM: Saved config to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3/params.conf
03/19 07:34:49 PM: Using random seed 22
03/19 07:34:49 PM: Loading tasks...
03/19 07:34:49 PM: Writing pre-preprocessed tasks to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/
03/19 07:34:49 PM: 	Loaded existing task multirc
03/19 07:34:49 PM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
03/19 07:34:49 PM: 	Finished loading tasks: multirc.
03/19 07:34:49 PM: Loading token dictionary from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab.
03/19 07:34:49 PM: 	Loaded vocab from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab
03/19 07:34:49 PM: 	Vocab namespace chars: size 100
03/19 07:34:49 PM: 	Vocab namespace bert_cased: size 28998
03/19 07:34:49 PM: 	Vocab namespace tokens: size 1004
03/19 07:34:49 PM: 	Finished building vocab.
03/19 07:34:49 PM: 	Task 'multirc', split 'train': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__train_data
03/19 07:34:49 PM: 	Task 'multirc', split 'val': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__val_data
03/19 07:34:49 PM: 	Task 'multirc', split 'test': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__test_data
03/19 07:34:49 PM: 	Finished indexing tasks
03/19 07:34:49 PM: 	Creating trimmed pretraining-only version of multirc train.
03/19 07:34:49 PM: 	Creating trimmed target-only version of multirc train.
03/19 07:34:49 PM: 	  Training on multirc
03/19 07:34:49 PM: 	  Evaluating on multirc
03/19 07:34:49 PM: 	Finished loading tasks in 0.131s
03/19 07:34:49 PM: 	 Tasks: ['multirc']
03/19 07:34:49 PM: Building model...
03/19 07:34:49 PM: Using BERT model (bert-base-cased).
03/19 07:34:49 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
03/19 07:34:49 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

03/19 07:34:50 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/19 07:34:52 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/19 07:34:52 PM: Initializing parameters
03/19 07:34:52 PM: Done initializing parameters; the following parameters are using their default initialization from their code
03/19 07:34:52 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
03/19 07:34:52 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
03/19 07:34:52 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
03/19 07:34:52 PM:    _text_field_embedder.model.pooler.dense.bias
03/19 07:34:52 PM:    _text_field_embedder.model.pooler.dense.weight
03/19 07:34:52 PM: 	Task 'multirc' params: {
  "cls_type": "mlp",
  "d_hid": 512,
  "pool_type": "max",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
03/19 07:34:52 PM: Model specification:
03/19 07:34:52 PM: MultiTaskModel(
  (sent_encoder): BoWSentEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=768, out_features=512, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.2)
        (4): Linear(in_features=512, out_features=2, bias=True)
      )
    )
  )
)
03/19 07:34:52 PM: Model parameters:
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 07:34:52 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 07:34:52 PM: 	multirc_mdl.classifier.classifier.0.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/19 07:34:52 PM: 	multirc_mdl.classifier.classifier.0.bias: Trainable parameter, count 512 with torch.Size([512])
03/19 07:34:52 PM: 	multirc_mdl.classifier.classifier.2.weight: Trainable parameter, count 512 with torch.Size([512])
03/19 07:34:52 PM: 	multirc_mdl.classifier.classifier.2.bias: Trainable parameter, count 512 with torch.Size([512])
03/19 07:34:52 PM: 	multirc_mdl.classifier.classifier.4.weight: Trainable parameter, count 1024 with torch.Size([2, 512])
03/19 07:34:52 PM: 	multirc_mdl.classifier.classifier.4.bias: Trainable parameter, count 2 with torch.Size([2])
03/19 07:34:52 PM: Total number of parameters: 108706050 (1.08706e+08)
03/19 07:34:52 PM: Number of trainable parameters: 108706050 (1.08706e+08)
03/19 07:34:52 PM: Finished building model in 2.686s
03/19 07:34:52 PM: Will run the following steps for this experiment:
Training model on tasks: multirc 
Evaluating model on tasks: multirc 

03/19 07:34:52 PM: Training...
03/19 07:34:52 PM: patience = 5
03/19 07:34:52 PM: val_interval = 50
03/19 07:34:52 PM: max_vals = 20
03/19 07:34:52 PM: cuda_device = -1
03/19 07:34:52 PM: grad_norm = 5.0
03/19 07:34:52 PM: grad_clipping = None
03/19 07:34:52 PM: lr_decay = 0.99
03/19 07:34:52 PM: min_lr = 1e-06
03/19 07:34:52 PM: keep_all_checkpoints = 0
03/19 07:34:52 PM: val_data_limit = 5000
03/19 07:34:52 PM: max_epochs = 10
03/19 07:34:52 PM: dec_val_scale = 250
03/19 07:34:52 PM: training_data_fraction = 1
03/19 07:34:52 PM: accumulation_steps = 1
03/19 07:34:52 PM: type = bert_adam
03/19 07:34:52 PM: parameter_groups = None
03/19 07:34:52 PM: Number of trainable parameters: 108706050
03/19 07:34:52 PM: infer_type_and_cast = True
03/19 07:34:52 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 07:34:52 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 07:34:52 PM: lr = 0.005
03/19 07:34:52 PM: t_total = 1000
03/19 07:34:52 PM: warmup = 0.1
03/19 07:34:52 PM: type = reduce_on_plateau
03/19 07:34:52 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 07:34:52 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 07:34:52 PM: mode = max
03/19 07:34:52 PM: factor = 0.5
03/19 07:34:52 PM: patience = 1
03/19 07:34:52 PM: threshold = 0.0001
03/19 07:34:52 PM: threshold_mode = abs
03/19 07:34:52 PM: verbose = True
03/19 07:34:52 PM: Starting training without restoring from a checkpoint.
03/19 07:34:52 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/Users/hpaila/Projects/NLP/multi_rc/baseline/jiant/__main__.py", line 588, in main
    phase="pretrain",
  File "/Users/hpaila/Projects/NLP/multi_rc/baseline/jiant/trainer.py", line 526, in train
    check_for_previous_checkpoints(self._serialization_dir, tasks, phase, load_model)
  File "/Users/hpaila/Projects/NLP/multi_rc/baseline/jiant/utils/utils.py", line 154, in check_for_previous_checkpoints
    % serialization_dir,
  File "/Users/hpaila/Projects/NLP/multi_rc/baseline/jiant/utils/utils.py", line 484, in assert_for_log
    assert condition, error_message
AssertionError: There are existing checkpoints in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run3 which will be overwritten. If you are restoring from a run, or would like to train from an existing checkpoint, Use load_model = 1 to load the checkpoints instead. If you don't want them, delete them or change your experiment name.
