03/19 09:55:32 PM: Git branch: develop
03/19 09:55:32 PM: Git SHA: 23cd1ee178a5b6267ac9d6fd5cba8f1d5ae0c538
03/19 09:55:32 PM: Parsed args: 
{
  "batch_size": 8,
  "d_word": 50,
  "do_target_task_training": 0,
  "exp_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/",
  "exp_name": "mutlirc_bert_cased_exp",
  "input_module": "bert-base-cased",
  "load_model": 0,
  "local_log_path": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6/log.log",
  "lr": 0.6,
  "max_epochs": 10,
  "max_seq_len": 10,
  "max_vals": 20,
  "max_word_v_size": 1000,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "pretrain_tasks": "multirc",
  "random_seed": 22,
  "remote_log_name": "mutlirc_bert_cased_exp__mutlirc_bert_cased_run6",
  "run_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6",
  "run_name": "mutlirc_bert_cased_run6",
  "sent_enc": "bow",
  "skip_embs": 0,
  "target_tasks": "multirc",
  "target_train_max_vals": 10,
  "target_train_val_interval": 10,
  "transfer_paradigm": "finetune",
  "val_interval": 50
}
03/19 09:55:32 PM: Saved config to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6/params.conf
03/19 09:55:32 PM: Using random seed 22
03/19 09:55:32 PM: Loading tasks...
03/19 09:55:32 PM: Writing pre-preprocessed tasks to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/
03/19 09:55:32 PM: 	Loaded existing task multirc
03/19 09:55:32 PM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
03/19 09:55:32 PM: 	Finished loading tasks: multirc.
03/19 09:55:32 PM: Loading token dictionary from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab.
03/19 09:55:32 PM: 	Loaded vocab from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab
03/19 09:55:32 PM: 	Vocab namespace chars: size 100
03/19 09:55:32 PM: 	Vocab namespace bert_cased: size 28998
03/19 09:55:32 PM: 	Vocab namespace tokens: size 1004
03/19 09:55:32 PM: 	Finished building vocab.
03/19 09:55:32 PM: 	Task 'multirc', split 'train': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__train_data
03/19 09:55:32 PM: 	Task 'multirc', split 'val': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__val_data
03/19 09:55:32 PM: 	Task 'multirc', split 'test': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__test_data
03/19 09:55:32 PM: 	Finished indexing tasks
03/19 09:55:32 PM: 	Creating trimmed pretraining-only version of multirc train.
03/19 09:55:32 PM: 	Creating trimmed target-only version of multirc train.
03/19 09:55:32 PM: 	  Training on multirc
03/19 09:55:32 PM: 	  Evaluating on multirc
03/19 09:55:32 PM: 	Finished loading tasks in 0.066s
03/19 09:55:32 PM: 	 Tasks: ['multirc']
03/19 09:55:32 PM: Building model...
03/19 09:55:32 PM: Using BERT model (bert-base-cased).
03/19 09:55:33 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
03/19 09:55:33 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

03/19 09:55:33 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/19 09:55:35 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/19 09:55:35 PM: Initializing parameters
03/19 09:55:35 PM: Done initializing parameters; the following parameters are using their default initialization from their code
03/19 09:55:35 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
03/19 09:55:35 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
03/19 09:55:35 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
03/19 09:55:35 PM:    _text_field_embedder.model.pooler.dense.bias
03/19 09:55:35 PM:    _text_field_embedder.model.pooler.dense.weight
03/19 09:55:35 PM: 	Task 'multirc' params: {
  "cls_type": "mlp",
  "d_hid": 512,
  "pool_type": "max",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
03/19 09:55:35 PM: Model specification:
03/19 09:55:35 PM: MultiTaskModel(
  (sent_encoder): BoWSentEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=768, out_features=512, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.2)
        (4): Linear(in_features=512, out_features=2, bias=True)
      )
    )
  )
)
03/19 09:55:35 PM: Model parameters:
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:55:35 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:55:35 PM: 	multirc_mdl.classifier.classifier.0.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/19 09:55:35 PM: 	multirc_mdl.classifier.classifier.0.bias: Trainable parameter, count 512 with torch.Size([512])
03/19 09:55:35 PM: 	multirc_mdl.classifier.classifier.2.weight: Trainable parameter, count 512 with torch.Size([512])
03/19 09:55:35 PM: 	multirc_mdl.classifier.classifier.2.bias: Trainable parameter, count 512 with torch.Size([512])
03/19 09:55:35 PM: 	multirc_mdl.classifier.classifier.4.weight: Trainable parameter, count 1024 with torch.Size([2, 512])
03/19 09:55:35 PM: 	multirc_mdl.classifier.classifier.4.bias: Trainable parameter, count 2 with torch.Size([2])
03/19 09:55:35 PM: Total number of parameters: 108706050 (1.08706e+08)
03/19 09:55:35 PM: Number of trainable parameters: 108706050 (1.08706e+08)
03/19 09:55:35 PM: Finished building model in 2.430s
03/19 09:55:35 PM: Will run the following steps for this experiment:
Training model on tasks: multirc 
Evaluating model on tasks: multirc 

03/19 09:55:35 PM: Training...
03/19 09:55:35 PM: patience = 5
03/19 09:55:35 PM: val_interval = 50
03/19 09:55:35 PM: max_vals = 20
03/19 09:55:35 PM: cuda_device = -1
03/19 09:55:35 PM: grad_norm = 5.0
03/19 09:55:35 PM: grad_clipping = None
03/19 09:55:35 PM: lr_decay = 0.99
03/19 09:55:35 PM: min_lr = 1e-06
03/19 09:55:35 PM: keep_all_checkpoints = 0
03/19 09:55:35 PM: val_data_limit = 5000
03/19 09:55:35 PM: max_epochs = 10
03/19 09:55:35 PM: dec_val_scale = 250
03/19 09:55:35 PM: training_data_fraction = 1
03/19 09:55:35 PM: accumulation_steps = 1
03/19 09:55:35 PM: type = bert_adam
03/19 09:55:35 PM: parameter_groups = None
03/19 09:55:35 PM: Number of trainable parameters: 108706050
03/19 09:55:35 PM: infer_type_and_cast = True
03/19 09:55:35 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 09:55:35 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 09:55:35 PM: lr = 0.6
03/19 09:55:35 PM: t_total = 1000
03/19 09:55:35 PM: warmup = 0.1
03/19 09:55:35 PM: type = reduce_on_plateau
03/19 09:55:35 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 09:55:35 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 09:55:35 PM: mode = max
03/19 09:55:35 PM: factor = 0.5
03/19 09:55:35 PM: patience = 1
03/19 09:55:35 PM: threshold = 0.0001
03/19 09:55:35 PM: threshold_mode = abs
03/19 09:55:35 PM: verbose = True
03/19 09:55:35 PM: Starting training without restoring from a checkpoint.
03/19 09:55:35 PM: Training examples per task, before any subsampling: {'multirc': 27243}
03/19 09:55:35 PM: Beginning training with stopping criteria based on metric: multirc_avg
03/19 09:55:45 PM: Update 4: task multirc, steps since last val 4 (total steps = 4): ans_f1: 0.6429, qst_f1: 0.2581, em: 0.6774, avg: 0.6601, multirc_loss: 2.4119
03/19 09:55:56 PM: Update 8: task multirc, steps since last val 8 (total steps = 8): ans_f1: 0.5263, qst_f1: 0.2222, em: 0.5714, avg: 0.5489, multirc_loss: 8.7468
03/19 09:56:09 PM: Update 12: task multirc, steps since last val 12 (total steps = 12): ans_f1: 0.5169, qst_f1: 0.2340, em: 0.5426, avg: 0.5297, multirc_loss: 10.7784
03/19 09:56:26 PM: Update 15: task multirc, steps since last val 15 (total steps = 15): ans_f1: 0.5299, qst_f1: 0.2542, em: 0.5339, avg: 0.5319, multirc_loss: 14.5808
03/19 09:56:36 PM: Update 19: task multirc, steps since last val 19 (total steps = 19): ans_f1: 0.5034, qst_f1: 0.2394, em: 0.5101, avg: 0.5067, multirc_loss: 18.3152
03/19 09:56:47 PM: Update 23: task multirc, steps since last val 23 (total steps = 23): ans_f1: 0.4795, qst_f1: 0.2228, em: 0.5000, avg: 0.4898, multirc_loss: 20.7467
03/19 09:56:57 PM: Update 27: task multirc, steps since last val 27 (total steps = 27): ans_f1: 0.4683, qst_f1: 0.2233, em: 0.4833, avg: 0.4758, multirc_loss: 22.9562
03/19 09:57:09 PM: Update 32: task multirc, steps since last val 32 (total steps = 32): ans_f1: 0.4715, qst_f1: 0.2304, em: 0.4878, avg: 0.4797, multirc_loss: 24.8249
03/19 09:57:21 PM: Update 37: task multirc, steps since last val 37 (total steps = 37): ans_f1: 0.4840, qst_f1: 0.2389, em: 0.4982, avg: 0.4911, multirc_loss: 24.5343
03/19 09:57:33 PM: Update 43: task multirc, steps since last val 43 (total steps = 43): ans_f1: 0.4821, qst_f1: 0.2458, em: 0.4719, avg: 0.4770, multirc_loss: 28.2717
03/19 09:57:46 PM: Update 47: task multirc, steps since last val 47 (total steps = 47): ans_f1: 0.4878, qst_f1: 0.2512, em: 0.4755, avg: 0.4817, multirc_loss: 29.6199
03/19 09:57:53 PM: ***** Step 50 / Validation 1 *****
03/19 09:57:53 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 09:57:53 PM: Validating...
03/19 09:57:56 PM: Evaluate: task multirc, batch 21 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0256, avg: 0.0128, multirc_loss: 204.4281
03/19 09:58:06 PM: Evaluate: task multirc, batch 83 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 195.8076
03/19 09:58:17 PM: Evaluate: task multirc, batch 146 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0044, avg: 0.0022, multirc_loss: 178.5245
03/19 09:58:27 PM: Evaluate: task multirc, batch 206 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0031, avg: 0.0016, multirc_loss: 175.4013
03/19 09:58:37 PM: Evaluate: task multirc, batch 261 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0048, avg: 0.0024, multirc_loss: 172.7065
03/19 09:58:47 PM: Evaluate: task multirc, batch 318 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0060, avg: 0.0030, multirc_loss: 170.3567
03/19 09:58:58 PM: Evaluate: task multirc, batch 376 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 172.7580
03/19 09:59:08 PM: Evaluate: task multirc, batch 436 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0029, avg: 0.0014, multirc_loss: 176.8819
03/19 09:59:19 PM: Evaluate: task multirc, batch 495 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0050, avg: 0.0025, multirc_loss: 176.4483
03/19 09:59:29 PM: Evaluate: task multirc, batch 550 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0046, avg: 0.0023, multirc_loss: 175.8081
03/19 09:59:40 PM: Evaluate: task multirc, batch 604 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 174.6438
03/19 09:59:41 PM: Best result seen so far for multirc.
03/19 09:59:41 PM: Best result seen so far for micro.
03/19 09:59:41 PM: Best result seen so far for macro.
03/19 09:59:41 PM: Updating LR scheduler:
03/19 09:59:41 PM: 	Best result seen so far for macro_avg: 0.002
03/19 09:59:41 PM: 	# validation passes without improvement: 0
03/19 09:59:41 PM: multirc_loss: training: 30.965123 validation: 174.995151
03/19 09:59:41 PM: macro_avg: validation: 0.001574
03/19 09:59:41 PM: micro_avg: validation: 0.001574
03/19 09:59:41 PM: multirc_ans_f1: training: 0.479592 validation: 0.000000
03/19 09:59:41 PM: multirc_qst_f1: training: 0.248858 validation: 0.000000
03/19 09:59:41 PM: multirc_em: training: 0.471233 validation: 0.003148
03/19 09:59:41 PM: multirc_avg: training: 0.475412 validation: 0.001574
03/19 09:59:41 PM: Global learning rate: 0.6
03/19 09:59:41 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6
03/19 09:59:51 PM: Update 54: task multirc, steps since last val 4 (total steps = 54): ans_f1: 0.4000, qst_f1: 0.1562, em: 0.5312, avg: 0.4656, multirc_loss: 177.5256
03/19 10:00:03 PM: Update 58: task multirc, steps since last val 8 (total steps = 58): ans_f1: 0.5484, qst_f1: 0.2581, em: 0.5645, avg: 0.5565, multirc_loss: 124.7171
03/19 10:00:13 PM: Update 63: task multirc, steps since last val 13 (total steps = 63): ans_f1: 0.5049, qst_f1: 0.2451, em: 0.5098, avg: 0.5073, multirc_loss: 159.7156
03/19 10:00:25 PM: Update 69: task multirc, steps since last val 19 (total steps = 69): ans_f1: 0.4868, qst_f1: 0.2460, em: 0.4690, avg: 0.4779, multirc_loss: 182.1938
03/19 10:00:37 PM: Update 75: task multirc, steps since last val 25 (total steps = 75): ans_f1: 0.4444, qst_f1: 0.2199, em: 0.4309, avg: 0.4376, multirc_loss: 230.4617
03/19 10:00:47 PM: Update 81: task multirc, steps since last val 31 (total steps = 81): ans_f1: 0.4754, qst_f1: 0.2387, em: 0.4672, avg: 0.4713, multirc_loss: 233.1151
03/19 10:00:58 PM: Update 87: task multirc, steps since last val 37 (total steps = 87): ans_f1: 0.4739, qst_f1: 0.2320, em: 0.4725, avg: 0.4732, multirc_loss: 243.4632
03/19 10:01:09 PM: Update 93: task multirc, steps since last val 43 (total steps = 93): ans_f1: 0.4836, qst_f1: 0.2409, em: 0.4679, avg: 0.4758, multirc_loss: 253.0271
03/19 10:01:20 PM: Update 99: task multirc, steps since last val 49 (total steps = 99): ans_f1: 0.4737, qst_f1: 0.2393, em: 0.4556, avg: 0.4646, multirc_loss: 266.4825
03/19 10:01:21 PM: ***** Step 100 / Validation 2 *****
03/19 10:01:22 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 10:01:22 PM: Validating...
03/19 10:01:30 PM: Evaluate: task multirc, batch 52 (606): ans_f1: 0.6752, qst_f1: 0.6707, em: 0.0213, avg: 0.3482, multirc_loss: 56.2662
03/19 10:01:40 PM: Evaluate: task multirc, batch 103 (606): ans_f1: 0.6301, qst_f1: 0.6285, em: 0.0062, avg: 0.3182, multirc_loss: 61.9646
03/19 10:01:50 PM: Evaluate: task multirc, batch 158 (606): ans_f1: 0.6064, qst_f1: 0.5985, em: 0.0083, avg: 0.3073, multirc_loss: 64.8130
03/19 10:02:01 PM: Evaluate: task multirc, batch 209 (606): ans_f1: 0.5979, qst_f1: 0.5809, em: 0.0031, avg: 0.3005, multirc_loss: 65.8102
03/19 10:02:11 PM: Evaluate: task multirc, batch 255 (606): ans_f1: 0.5902, qst_f1: 0.5762, em: 0.0025, avg: 0.2963, multirc_loss: 66.7061
03/19 10:02:21 PM: Evaluate: task multirc, batch 303 (606): ans_f1: 0.5907, qst_f1: 0.5777, em: 0.0021, avg: 0.2964, multirc_loss: 66.6470
03/19 10:02:32 PM: Evaluate: task multirc, batch 349 (606): ans_f1: 0.5892, qst_f1: 0.5775, em: 0.0018, avg: 0.2955, multirc_loss: 66.8215
03/19 10:02:43 PM: Evaluate: task multirc, batch 380 (606): ans_f1: 0.5942, qst_f1: 0.5805, em: 0.0017, avg: 0.2980, multirc_loss: 66.2391
03/19 10:02:54 PM: Evaluate: task multirc, batch 415 (606): ans_f1: 0.6039, qst_f1: 0.5929, em: 0.0121, avg: 0.3080, multirc_loss: 65.1109
03/19 10:03:05 PM: Evaluate: task multirc, batch 450 (606): ans_f1: 0.6076, qst_f1: 0.5970, em: 0.0110, avg: 0.3093, multirc_loss: 64.6681
03/19 10:03:15 PM: Evaluate: task multirc, batch 481 (606): ans_f1: 0.6040, qst_f1: 0.5947, em: 0.0103, avg: 0.3072, multirc_loss: 65.0923
03/19 10:03:26 PM: Evaluate: task multirc, batch 516 (606): ans_f1: 0.6009, qst_f1: 0.5918, em: 0.0098, avg: 0.3053, multirc_loss: 65.4579
03/19 10:03:37 PM: Evaluate: task multirc, batch 553 (606): ans_f1: 0.6011, qst_f1: 0.5923, em: 0.0091, avg: 0.3051, multirc_loss: 65.4354
03/19 10:03:47 PM: Evaluate: task multirc, batch 596 (606): ans_f1: 0.5968, qst_f1: 0.5885, em: 0.0085, avg: 0.3027, multirc_loss: 65.9364
03/19 10:03:50 PM: Best result seen so far for multirc.
03/19 10:03:50 PM: Best result seen so far for micro.
03/19 10:03:50 PM: Best result seen so far for macro.
03/19 10:03:50 PM: Updating LR scheduler:
03/19 10:03:50 PM: 	Best result seen so far for macro_avg: 0.304
03/19 10:03:50 PM: 	# validation passes without improvement: 0
03/19 10:03:50 PM: multirc_loss: training: 268.942739 validation: 65.629342
03/19 10:03:50 PM: macro_avg: validation: 0.303923
03/19 10:03:50 PM: micro_avg: validation: 0.303923
03/19 10:03:50 PM: multirc_ans_f1: training: 0.479592 validation: 0.599451
03/19 10:03:50 PM: multirc_qst_f1: training: 0.243662 validation: 0.590961
03/19 10:03:50 PM: multirc_em: training: 0.453521 validation: 0.008395
03/19 10:03:50 PM: multirc_avg: training: 0.466556 validation: 0.303923
03/19 10:03:50 PM: Global learning rate: 0.6
03/19 10:03:50 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6
03/19 10:03:59 PM: Update 103: task multirc, steps since last val 3 (total steps = 103): ans_f1: 0.6667, qst_f1: 0.2917, em: 0.7083, avg: 0.6875, multirc_loss: 345.3197
03/19 10:04:11 PM: Update 108: task multirc, steps since last val 8 (total steps = 108): ans_f1: 0.5385, qst_f1: 0.2222, em: 0.6190, avg: 0.5788, multirc_loss: 230.7087
03/19 10:04:21 PM: Update 113: task multirc, steps since last val 13 (total steps = 113): ans_f1: 0.5057, qst_f1: 0.2157, em: 0.5784, avg: 0.5421, multirc_loss: 217.4867
03/19 10:04:33 PM: Update 118: task multirc, steps since last val 18 (total steps = 118): ans_f1: 0.5344, qst_f1: 0.2500, em: 0.5714, avg: 0.5529, multirc_loss: 211.3772
03/19 10:04:44 PM: Update 123: task multirc, steps since last val 23 (total steps = 123): ans_f1: 0.5357, qst_f1: 0.2524, em: 0.5706, avg: 0.5532, multirc_loss: 222.6594
03/19 10:04:55 PM: Update 129: task multirc, steps since last val 29 (total steps = 129): ans_f1: 0.5126, qst_f1: 0.2288, em: 0.5773, avg: 0.5449, multirc_loss: 221.9466
03/19 10:05:07 PM: Update 135: task multirc, steps since last val 35 (total steps = 135): ans_f1: 0.5145, qst_f1: 0.2289, em: 0.5736, avg: 0.5441, multirc_loss: 212.2229
03/19 10:05:17 PM: Update 140: task multirc, steps since last val 40 (total steps = 140): ans_f1: 0.4924, qst_f1: 0.2100, em: 0.5700, avg: 0.5312, multirc_loss: 225.0643
03/19 10:05:29 PM: Update 146: task multirc, steps since last val 46 (total steps = 146): ans_f1: 0.4870, qst_f1: 0.2141, em: 0.5543, avg: 0.5206, multirc_loss: 252.7879
03/19 10:05:36 PM: ***** Step 150 / Validation 3 *****
03/19 10:05:37 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 10:05:37 PM: Validating...
03/19 10:05:39 PM: Evaluate: task multirc, batch 13 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0435, avg: 0.0217, multirc_loss: 203.2503
03/19 10:05:49 PM: Evaluate: task multirc, batch 69 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 219.3900
03/19 10:05:59 PM: Evaluate: task multirc, batch 114 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 203.7703
03/19 10:06:09 PM: Evaluate: task multirc, batch 157 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 191.4372
03/19 10:06:20 PM: Evaluate: task multirc, batch 202 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 190.2118
03/19 10:06:30 PM: Evaluate: task multirc, batch 243 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0053, avg: 0.0026, multirc_loss: 183.2633
03/19 10:06:41 PM: Evaluate: task multirc, batch 287 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0044, avg: 0.0022, multirc_loss: 185.8554
03/19 10:06:51 PM: Evaluate: task multirc, batch 337 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0038, avg: 0.0019, multirc_loss: 184.7422
03/19 10:07:01 PM: Evaluate: task multirc, batch 388 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0033, avg: 0.0017, multirc_loss: 186.8475
03/19 10:07:12 PM: Evaluate: task multirc, batch 442 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0028, avg: 0.0014, multirc_loss: 191.1699
03/19 10:07:23 PM: Evaluate: task multirc, batch 491 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0038, avg: 0.0019, multirc_loss: 190.1418
03/19 10:07:33 PM: Evaluate: task multirc, batch 538 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0035, avg: 0.0017, multirc_loss: 189.5948
03/19 10:07:44 PM: Evaluate: task multirc, batch 592 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0043, avg: 0.0021, multirc_loss: 187.1783
03/19 10:07:47 PM: Updating LR scheduler:
03/19 10:07:47 PM: 	Best result seen so far for macro_avg: 0.304
03/19 10:07:47 PM: 	# validation passes without improvement: 1
03/19 10:07:47 PM: multirc_loss: training: 247.936303 validation: 188.485868
03/19 10:07:47 PM: macro_avg: validation: 0.001574
03/19 10:07:47 PM: micro_avg: validation: 0.001574
03/19 10:07:47 PM: multirc_ans_f1: training: 0.488235 validation: 0.000000
03/19 10:07:47 PM: multirc_qst_f1: training: 0.219512 validation: 0.000000
03/19 10:07:47 PM: multirc_em: training: 0.550136 validation: 0.003148
03/19 10:07:47 PM: multirc_avg: training: 0.519185 validation: 0.001574
03/19 10:07:47 PM: Global learning rate: 0.6
03/19 10:07:47 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6
03/19 10:07:55 PM: Update 153: task multirc, steps since last val 3 (total steps = 153): ans_f1: 0.2500, qst_f1: 0.1250, em: 0.2500, avg: 0.2500, multirc_loss: 237.1569
03/19 10:08:05 PM: Update 158: task multirc, steps since last val 8 (total steps = 158): ans_f1: 0.4928, qst_f1: 0.2656, em: 0.4531, avg: 0.4729, multirc_loss: 166.3603
03/19 10:08:16 PM: Update 163: task multirc, steps since last val 13 (total steps = 163): ans_f1: 0.4952, qst_f1: 0.2574, em: 0.4851, avg: 0.4902, multirc_loss: 141.8785
03/19 10:08:27 PM: Update 169: task multirc, steps since last val 19 (total steps = 169): ans_f1: 0.4429, qst_f1: 0.2075, em: 0.4830, avg: 0.4629, multirc_loss: 123.6545
03/19 10:08:38 PM: Update 175: task multirc, steps since last val 25 (total steps = 175): ans_f1: 0.4516, qst_f1: 0.2092, em: 0.4792, avg: 0.4654, multirc_loss: 124.7701
03/19 10:08:50 PM: Update 181: task multirc, steps since last val 31 (total steps = 181): ans_f1: 0.4364, qst_f1: 0.1942, em: 0.4831, avg: 0.4597, multirc_loss: 116.4794
03/19 10:09:01 PM: Update 187: task multirc, steps since last val 37 (total steps = 187): ans_f1: 0.4509, qst_f1: 0.2159, em: 0.4635, avg: 0.4572, multirc_loss: 114.6697
03/19 10:09:14 PM: Update 193: task multirc, steps since last val 43 (total steps = 193): ans_f1: 0.4324, qst_f1: 0.1931, em: 0.4825, avg: 0.4575, multirc_loss: 146.3254
03/19 10:09:24 PM: Update 198: task multirc, steps since last val 48 (total steps = 198): ans_f1: 0.4239, qst_f1: 0.1936, em: 0.4669, avg: 0.4454, multirc_loss: 156.9142
03/19 10:09:29 PM: ***** Step 200 / Validation 4 *****
03/19 10:09:29 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 10:09:29 PM: Validating...
03/19 10:09:34 PM: Evaluate: task multirc, batch 34 (606): ans_f1: 0.6634, qst_f1: 0.6632, em: 0.0154, avg: 0.3394, multirc_loss: 141.6525
03/19 10:09:45 PM: Evaluate: task multirc, batch 83 (606): ans_f1: 0.6477, qst_f1: 0.6448, em: 0.0073, avg: 0.3275, multirc_loss: 146.5483
03/19 10:09:55 PM: Evaluate: task multirc, batch 126 (606): ans_f1: 0.6286, qst_f1: 0.6217, em: 0.0051, avg: 0.3168, multirc_loss: 152.3368
03/19 10:10:05 PM: Evaluate: task multirc, batch 168 (606): ans_f1: 0.6051, qst_f1: 0.5958, em: 0.0079, avg: 0.3065, multirc_loss: 159.2422
03/19 10:10:16 PM: Evaluate: task multirc, batch 210 (606): ans_f1: 0.5977, qst_f1: 0.5816, em: 0.0061, avg: 0.3019, multirc_loss: 161.3765
03/19 10:10:26 PM: Evaluate: task multirc, batch 253 (606): ans_f1: 0.5895, qst_f1: 0.5752, em: 0.0025, avg: 0.2960, multirc_loss: 163.6845
03/19 10:10:37 PM: Evaluate: task multirc, batch 297 (606): ans_f1: 0.5920, qst_f1: 0.5791, em: 0.0021, avg: 0.2971, multirc_loss: 162.9897
03/19 10:10:47 PM: Evaluate: task multirc, batch 340 (606): ans_f1: 0.5903, qst_f1: 0.5775, em: 0.0019, avg: 0.2961, multirc_loss: 163.4691
03/19 10:10:57 PM: Evaluate: task multirc, batch 387 (606): ans_f1: 0.5959, qst_f1: 0.5827, em: 0.0050, avg: 0.3005, multirc_loss: 161.8749
03/19 10:11:08 PM: Evaluate: task multirc, batch 437 (606): ans_f1: 0.6036, qst_f1: 0.5929, em: 0.0114, avg: 0.3075, multirc_loss: 159.6841
03/19 10:11:19 PM: Evaluate: task multirc, batch 489 (606): ans_f1: 0.6026, qst_f1: 0.5940, em: 0.0102, avg: 0.3064, multirc_loss: 159.9572
03/19 10:11:29 PM: Evaluate: task multirc, batch 542 (606): ans_f1: 0.6022, qst_f1: 0.5932, em: 0.0093, avg: 0.3057, multirc_loss: 160.0769
03/19 10:11:40 PM: Evaluate: task multirc, batch 597 (606): ans_f1: 0.5969, qst_f1: 0.5886, em: 0.0085, avg: 0.3027, multirc_loss: 161.5818
03/19 10:11:42 PM: Updating LR scheduler:
03/19 10:11:42 PM: 	Best result seen so far for macro_avg: 0.304
03/19 10:11:42 PM: 	# validation passes without improvement: 0
03/19 10:11:42 PM: multirc_loss: training: 165.628352 validation: 160.864390
03/19 10:11:42 PM: macro_avg: validation: 0.303923
03/19 10:11:42 PM: micro_avg: validation: 0.303923
03/19 10:11:42 PM: multirc_ans_f1: training: 0.426966 validation: 0.599451
03/19 10:11:42 PM: multirc_qst_f1: training: 0.199355 validation: 0.590961
03/19 10:11:42 PM: multirc_em: training: 0.461326 validation: 0.008395
03/19 10:11:42 PM: multirc_avg: training: 0.444146 validation: 0.303923
03/19 10:11:42 PM: Global learning rate: 0.3
03/19 10:11:42 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6
03/19 10:11:51 PM: Update 204: task multirc, steps since last val 4 (total steps = 204): ans_f1: 0.4800, qst_f1: 0.1828, em: 0.5806, avg: 0.5303, multirc_loss: 70.5953
03/19 10:12:02 PM: Update 210: task multirc, steps since last val 10 (total steps = 210): ans_f1: 0.5952, qst_f1: 0.3117, em: 0.5584, avg: 0.5768, multirc_loss: 53.7791
03/19 10:12:14 PM: Update 216: task multirc, steps since last val 16 (total steps = 216): ans_f1: 0.5484, qst_f1: 0.2691, em: 0.5492, avg: 0.5488, multirc_loss: 53.8258
03/19 10:12:25 PM: Update 222: task multirc, steps since last val 22 (total steps = 222): ans_f1: 0.5749, qst_f1: 0.2795, em: 0.5793, avg: 0.5771, multirc_loss: 50.5816
03/19 10:12:37 PM: Update 228: task multirc, steps since last val 28 (total steps = 228): ans_f1: 0.5625, qst_f1: 0.2949, em: 0.5343, avg: 0.5484, multirc_loss: 50.7906
03/19 10:12:48 PM: Update 234: task multirc, steps since last val 34 (total steps = 234): ans_f1: 0.5600, qst_f1: 0.2984, em: 0.5323, avg: 0.5461, multirc_loss: 47.2515
03/19 10:13:00 PM: Update 240: task multirc, steps since last val 40 (total steps = 240): ans_f1: 0.5541, qst_f1: 0.2856, em: 0.5379, avg: 0.5460, multirc_loss: 45.3203
03/19 10:13:12 PM: Update 246: task multirc, steps since last val 46 (total steps = 246): ans_f1: 0.5440, qst_f1: 0.2823, em: 0.5152, avg: 0.5296, multirc_loss: 46.6486
03/19 10:13:20 PM: ***** Step 250 / Validation 5 *****
03/19 10:13:20 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 10:13:20 PM: Validating...
03/19 10:13:22 PM: Evaluate: task multirc, batch 14 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 8.3602
03/19 10:13:32 PM: Evaluate: task multirc, batch 72 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 8.5193
03/19 10:13:42 PM: Evaluate: task multirc, batch 121 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0053, avg: 0.0027, multirc_loss: 8.0071
03/19 10:13:53 PM: Evaluate: task multirc, batch 172 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 7.5105
03/19 10:14:03 PM: Evaluate: task multirc, batch 221 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0088, avg: 0.0044, multirc_loss: 7.2870
03/19 10:14:13 PM: Evaluate: task multirc, batch 271 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0047, avg: 0.0023, multirc_loss: 7.3502
03/19 10:14:24 PM: Evaluate: task multirc, batch 323 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0040, avg: 0.0020, multirc_loss: 7.2539
03/19 10:14:34 PM: Evaluate: task multirc, batch 377 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0051, avg: 0.0026, multirc_loss: 7.3188
03/19 10:14:45 PM: Evaluate: task multirc, batch 433 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0029, avg: 0.0014, multirc_loss: 7.4985
03/19 10:14:55 PM: Evaluate: task multirc, batch 488 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0038, avg: 0.0019, multirc_loss: 7.4795
03/19 10:15:06 PM: Evaluate: task multirc, batch 539 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0047, avg: 0.0023, multirc_loss: 7.4635
03/19 10:15:16 PM: Evaluate: task multirc, batch 588 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 7.3797
03/19 10:15:20 PM: Updating LR scheduler:
03/19 10:15:20 PM: 	Best result seen so far for macro_avg: 0.304
03/19 10:15:20 PM: 	# validation passes without improvement: 1
03/19 10:15:20 PM: multirc_loss: training: 47.031911 validation: 7.421574
03/19 10:15:20 PM: macro_avg: validation: 0.001574
03/19 10:15:20 PM: micro_avg: validation: 0.001574
03/19 10:15:20 PM: multirc_ans_f1: training: 0.534005 validation: 0.000000
03/19 10:15:20 PM: multirc_qst_f1: training: 0.276733 validation: 0.000000
03/19 10:15:20 PM: multirc_em: training: 0.498575 validation: 0.003148
03/19 10:15:20 PM: multirc_avg: training: 0.516290 validation: 0.001574
03/19 10:15:20 PM: Global learning rate: 0.3
03/19 10:15:20 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6
03/19 10:15:27 PM: Update 253: task multirc, steps since last val 3 (total steps = 253): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.5000, avg: 0.2500, multirc_loss: 14.7580
03/19 10:15:39 PM: Update 259: task multirc, steps since last val 9 (total steps = 259): ans_f1: 0.3478, qst_f1: 0.1667, em: 0.3529, avg: 0.3504, multirc_loss: 16.2562
03/19 10:15:49 PM: Update 264: task multirc, steps since last val 14 (total steps = 264): ans_f1: 0.3853, qst_f1: 0.1887, em: 0.3774, avg: 0.3813, multirc_loss: 14.1441
03/19 10:15:59 PM: Update 269: task multirc, steps since last val 19 (total steps = 269): ans_f1: 0.4225, qst_f1: 0.2005, em: 0.4336, avg: 0.4281, multirc_loss: 12.7718
03/19 10:16:10 PM: Update 274: task multirc, steps since last val 24 (total steps = 274): ans_f1: 0.4590, qst_f1: 0.2292, em: 0.4545, avg: 0.4568, multirc_loss: 11.6733
03/19 10:16:20 PM: Update 279: task multirc, steps since last val 29 (total steps = 279): ans_f1: 0.4615, qst_f1: 0.2279, em: 0.4605, avg: 0.4610, multirc_loss: 11.9967
03/19 10:16:30 PM: Update 284: task multirc, steps since last val 34 (total steps = 284): ans_f1: 0.4646, qst_f1: 0.2272, em: 0.4718, avg: 0.4682, multirc_loss: 11.0598
03/19 10:16:41 PM: Update 289: task multirc, steps since last val 39 (total steps = 289): ans_f1: 0.4698, qst_f1: 0.2364, em: 0.4645, avg: 0.4672, multirc_loss: 12.0710
03/19 10:16:53 PM: Update 295: task multirc, steps since last val 45 (total steps = 295): ans_f1: 0.4836, qst_f1: 0.2369, em: 0.4892, avg: 0.4864, multirc_loss: 12.8445
03/19 10:17:03 PM: ***** Step 300 / Validation 6 *****
03/19 10:17:03 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 10:17:03 PM: Validating...
03/19 10:17:03 PM: Evaluate: task multirc, batch 2 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 29.1738
03/19 10:17:13 PM: Evaluate: task multirc, batch 62 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 40.1531
03/19 10:17:24 PM: Evaluate: task multirc, batch 111 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 35.7444
03/19 10:17:34 PM: Evaluate: task multirc, batch 160 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 33.8537
03/19 10:17:44 PM: Evaluate: task multirc, batch 210 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0061, avg: 0.0031, multirc_loss: 33.1562
03/19 10:17:54 PM: Evaluate: task multirc, batch 262 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0048, avg: 0.0024, multirc_loss: 32.8854
03/19 10:18:05 PM: Evaluate: task multirc, batch 312 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0041, avg: 0.0020, multirc_loss: 32.5400
03/19 10:18:15 PM: Evaluate: task multirc, batch 365 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0035, avg: 0.0018, multirc_loss: 33.0103
03/19 10:18:26 PM: Evaluate: task multirc, batch 417 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0030, avg: 0.0015, multirc_loss: 33.6046
03/19 10:18:36 PM: Evaluate: task multirc, batch 471 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0020, multirc_loss: 33.6747
03/19 10:18:47 PM: Evaluate: task multirc, batch 516 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0018, multirc_loss: 33.4141
03/19 10:18:57 PM: Evaluate: task multirc, batch 556 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 33.4064
03/19 10:19:08 PM: Evaluate: task multirc, batch 604 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 33.2310
03/19 10:19:09 PM: Updating LR scheduler:
03/19 10:19:09 PM: 	Best result seen so far for macro_avg: 0.304
03/19 10:19:09 PM: 	# validation passes without improvement: 0
03/19 10:19:09 PM: multirc_loss: training: 17.445420 validation: 33.297878
03/19 10:19:09 PM: macro_avg: validation: 0.001574
03/19 10:19:09 PM: micro_avg: validation: 0.001574
03/19 10:19:09 PM: multirc_ans_f1: training: 0.461538 validation: 0.000000
03/19 10:19:09 PM: multirc_qst_f1: training: 0.223944 validation: 0.000000
03/19 10:19:09 PM: multirc_em: training: 0.484507 validation: 0.003148
03/19 10:19:09 PM: multirc_avg: training: 0.473023 validation: 0.001574
03/19 10:19:09 PM: Global learning rate: 0.15
03/19 10:19:09 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6
03/19 10:19:19 PM: Update 304: task multirc, steps since last val 4 (total steps = 304): ans_f1: 0.4516, qst_f1: 0.2188, em: 0.4688, avg: 0.4602, multirc_loss: 35.3474
03/19 10:19:29 PM: Update 309: task multirc, steps since last val 9 (total steps = 309): ans_f1: 0.3529, qst_f1: 0.1690, em: 0.3803, avg: 0.3666, multirc_loss: 42.0741
03/19 10:19:40 PM: Update 314: task multirc, steps since last val 14 (total steps = 314): ans_f1: 0.3269, qst_f1: 0.1545, em: 0.3636, avg: 0.3453, multirc_loss: 38.3177
03/19 10:19:50 PM: Update 319: task multirc, steps since last val 19 (total steps = 319): ans_f1: 0.3521, qst_f1: 0.1678, em: 0.3810, avg: 0.3665, multirc_loss: 38.3487
03/19 10:20:02 PM: Update 325: task multirc, steps since last val 25 (total steps = 325): ans_f1: 0.3511, qst_f1: 0.1720, em: 0.3617, avg: 0.3564, multirc_loss: 33.9133
03/19 10:20:13 PM: Update 330: task multirc, steps since last val 30 (total steps = 330): ans_f1: 0.3379, qst_f1: 0.1600, em: 0.3644, avg: 0.3512, multirc_loss: 29.2541
03/19 10:20:24 PM: Update 336: task multirc, steps since last val 36 (total steps = 336): ans_f1: 0.3241, qst_f1: 0.1491, em: 0.3722, avg: 0.3481, multirc_loss: 25.3716
03/19 10:20:35 PM: Update 342: task multirc, steps since last val 42 (total steps = 342): ans_f1: 0.3322, qst_f1: 0.1574, em: 0.3681, avg: 0.3502, multirc_loss: 22.5976
03/19 10:20:46 PM: Update 348: task multirc, steps since last val 48 (total steps = 348): ans_f1: 0.3353, qst_f1: 0.1628, em: 0.3721, avg: 0.3537, multirc_loss: 20.7001
03/19 10:20:50 PM: ***** Step 350 / Validation 7 *****
03/19 10:20:50 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 10:20:50 PM: Validating...
03/19 10:20:56 PM: Evaluate: task multirc, batch 44 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 10.7810
03/19 10:21:06 PM: Evaluate: task multirc, batch 106 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 9.7548
03/19 10:21:16 PM: Evaluate: task multirc, batch 166 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 9.2471
03/19 10:21:27 PM: Evaluate: task multirc, batch 223 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0087, avg: 0.0043, multirc_loss: 8.9509
03/19 10:21:37 PM: Evaluate: task multirc, batch 279 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0045, avg: 0.0023, multirc_loss: 9.0074
03/19 10:21:47 PM: Evaluate: task multirc, batch 332 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0057, avg: 0.0029, multirc_loss: 8.9581
03/19 10:21:58 PM: Evaluate: task multirc, batch 390 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0049, avg: 0.0025, multirc_loss: 9.0609
03/19 10:22:08 PM: Evaluate: task multirc, batch 450 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0041, avg: 0.0021, multirc_loss: 9.3037
03/19 10:22:19 PM: Evaluate: task multirc, batch 512 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0018, multirc_loss: 9.1608
03/19 10:22:29 PM: Evaluate: task multirc, batch 572 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0033, avg: 0.0017, multirc_loss: 9.0898
03/19 10:22:35 PM: Updating LR scheduler:
03/19 10:22:35 PM: 	Best result seen so far for macro_avg: 0.304
03/19 10:22:35 PM: 	# validation passes without improvement: 1
03/19 10:22:35 PM: multirc_loss: training: 20.360991 validation: 9.125074
03/19 10:22:35 PM: macro_avg: validation: 0.001574
03/19 10:22:35 PM: micro_avg: validation: 0.001574
03/19 10:22:35 PM: multirc_ans_f1: training: 0.327684 validation: 0.000000
03/19 10:22:35 PM: multirc_qst_f1: training: 0.156425 validation: 0.000000
03/19 10:22:35 PM: multirc_em: training: 0.374302 validation: 0.003148
03/19 10:22:35 PM: multirc_avg: training: 0.350993 validation: 0.001574
03/19 10:22:35 PM: Global learning rate: 0.15
03/19 10:22:35 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6
03/19 10:22:40 PM: Update 352: task multirc, steps since last val 2 (total steps = 352): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.5625, avg: 0.2812, multirc_loss: 6.2795
03/19 10:22:52 PM: Update 358: task multirc, steps since last val 8 (total steps = 358): ans_f1: 0.6000, qst_f1: 0.3280, em: 0.5556, avg: 0.5778, multirc_loss: 3.6611
03/19 10:23:02 PM: Update 364: task multirc, steps since last val 14 (total steps = 364): ans_f1: 0.5849, qst_f1: 0.2722, em: 0.5963, avg: 0.5906, multirc_loss: 3.5511
03/19 10:23:12 PM: Update 370: task multirc, steps since last val 20 (total steps = 370): ans_f1: 0.5455, qst_f1: 0.2251, em: 0.6169, avg: 0.5812, multirc_loss: 3.1602
03/19 10:23:24 PM: Update 377: task multirc, steps since last val 27 (total steps = 377): ans_f1: 0.5089, qst_f1: 0.2013, em: 0.6039, avg: 0.5564, multirc_loss: 3.2896
03/19 10:23:34 PM: Update 383: task multirc, steps since last val 33 (total steps = 383): ans_f1: 0.4932, qst_f1: 0.2114, em: 0.5691, avg: 0.5311, multirc_loss: 3.8066
03/19 10:23:46 PM: Update 390: task multirc, steps since last val 40 (total steps = 390): ans_f1: 0.4982, qst_f1: 0.2270, em: 0.5495, avg: 0.5239, multirc_loss: 4.0053
03/19 10:23:57 PM: Update 397: task multirc, steps since last val 47 (total steps = 397): ans_f1: 0.4940, qst_f1: 0.2277, em: 0.5298, avg: 0.5119, multirc_loss: 4.1929
03/19 10:24:02 PM: ***** Step 400 / Validation 8 *****
03/19 10:24:02 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 10:24:02 PM: Validating...
03/19 10:24:07 PM: Evaluate: task multirc, batch 38 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 11.6476
03/19 10:24:18 PM: Evaluate: task multirc, batch 107 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 10.7660
03/19 10:24:28 PM: Evaluate: task multirc, batch 173 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 10.1829
03/19 10:24:38 PM: Evaluate: task multirc, batch 237 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0054, avg: 0.0027, multirc_loss: 9.8572
03/19 10:24:48 PM: Evaluate: task multirc, batch 304 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0042, avg: 0.0021, multirc_loss: 9.8349
03/19 10:24:59 PM: Evaluate: task multirc, batch 372 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0035, avg: 0.0017, multirc_loss: 9.9046
03/19 10:25:09 PM: Evaluate: task multirc, batch 438 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0028, avg: 0.0014, multirc_loss: 10.1521
03/19 10:25:20 PM: Evaluate: task multirc, batch 506 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0019, multirc_loss: 10.0970
03/19 10:25:30 PM: Evaluate: task multirc, batch 567 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0045, avg: 0.0022, multirc_loss: 10.0136
03/19 10:25:39 PM: Updating LR scheduler:
03/19 10:25:39 PM: 	Best result seen so far for macro_avg: 0.304
03/19 10:25:39 PM: 	# validation passes without improvement: 0
03/19 10:25:39 PM: Ran out of early stopping patience. Stopping training.
03/19 10:25:39 PM: multirc_loss: training: 4.243653 validation: 10.036663
03/19 10:25:39 PM: macro_avg: validation: 0.001574
03/19 10:25:39 PM: micro_avg: validation: 0.001574
03/19 10:25:39 PM: multirc_ans_f1: training: 0.498599 validation: 0.000000
03/19 10:25:39 PM: multirc_qst_f1: training: 0.234093 validation: 0.000000
03/19 10:25:39 PM: multirc_em: training: 0.521368 validation: 0.003148
03/19 10:25:39 PM: multirc_avg: training: 0.509983 validation: 0.001574
03/19 10:25:39 PM: Global learning rate: 0.075
03/19 10:25:39 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6
03/19 10:25:40 PM: Stopped training after 8 validation checks
03/19 10:25:41 PM: Trained multirc for 400 steps or 0.117 epochs
03/19 10:25:41 PM: ***** VALIDATION RESULTS *****
03/19 10:25:41 PM: multirc_avg (for best val pass 2): multirc_loss: 65.62934, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/19 10:25:41 PM: micro_avg (for best val pass 2): multirc_loss: 65.62934, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/19 10:25:41 PM: macro_avg (for best val pass 2): multirc_loss: 65.62934, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/19 10:25:41 PM: Evaluating...
03/19 10:25:41 PM: Loaded model state from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run6/model_state_pretrain_val_2.best.th
03/19 10:25:41 PM: Evaluating on: multirc, split: val
03/19 10:26:11 PM: 	Task multirc: batch 183
03/19 10:26:41 PM: 	Task multirc: batch 369
03/19 10:27:11 PM: 	Task multirc: batch 561
03/19 10:27:18 PM: Task 'multirc': sorting predictions by 'idx'
03/19 10:27:18 PM: Finished evaluating on: multirc
03/19 10:27:18 PM: Writing results for split 'val' to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/results.tsv
03/19 10:27:18 PM: micro_avg: 0.304, macro_avg: 0.304, multirc_ans_f1: 0.599, multirc_qst_f1: 0.591, multirc_em: 0.008, multirc_avg: 0.304
03/19 10:27:18 PM: Done!
