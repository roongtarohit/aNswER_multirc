03/18 05:51:01 PM: Git branch: master
03/18 05:51:01 PM: Git SHA: 65797ee7cccf490898288d415b70e8408b0b3699
03/18 05:51:01 PM: Parsed args: 
{
  "batch_size": 8,
  "classifier_hid_dim": 32,
  "d_word": 50,
  "do_target_task_training": 0,
  "exp_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/",
  "exp_name": "mutlirc_bert_cased_exp",
  "input_module": "bert-base-cased",
  "load_model": 0,
  "local_log_path": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run/log.log",
  "lr": 0.0003,
  "max_seq_len": 10,
  "max_vals": 100,
  "max_word_v_size": 1000,
  "pair_attn": 0,
  "pretrain_tasks": "multirc",
  "random_seed": 42,
  "remote_log_name": "mutlirc_bert_cased_exp__mutlirc_bert_cased_run",
  "run_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run",
  "run_name": "mutlirc_bert_cased_run",
  "sent_enc": "bow",
  "skip_embs": 0,
  "target_tasks": "multirc",
  "target_train_max_vals": 10,
  "target_train_val_interval": 10,
  "transfer_paradigm": "finetune",
  "val_interval": 50
}
03/18 05:51:01 PM: Saved config to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run/params.conf
03/18 05:51:01 PM: Using random seed 42
03/18 05:51:01 PM: Loading tasks...
03/18 05:51:01 PM: Writing pre-preprocessed tasks to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/
03/18 05:51:01 PM: 	Creating task multirc from scratch.
03/18 05:51:01 PM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
03/18 05:51:01 PM: 	Finished loading tasks: multirc.
03/18 05:51:01 PM: 	Building vocab from scratch.
03/18 05:51:01 PM: 	Counting units for task multirc.
03/18 05:51:01 PM: 	Loading Tokenizer bert-base-cased
03/18 05:51:01 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/hpaila/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/18 05:51:07 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/hpaila/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/18 05:51:07 PM: Added transformers vocab (bert-base-cased): 28996 tokens
03/18 05:51:07 PM: 	Saved vocab to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab
03/18 05:51:07 PM: Loading token dictionary from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab.
03/18 05:51:07 PM: 	Loaded vocab from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab
03/18 05:51:07 PM: 	Vocab namespace chars: size 100
03/18 05:51:07 PM: 	Vocab namespace bert_cased: size 28998
03/18 05:51:07 PM: 	Vocab namespace tokens: size 1004
03/18 05:51:07 PM: 	Finished building vocab.
03/18 05:51:07 PM: 	Task multirc (train): Indexing from scratch.
03/18 05:51:15 PM: 	Task multirc (train): Saved 27243 instances to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__train_data
03/18 05:51:15 PM: 	Task multirc (val): Indexing from scratch.
03/18 05:51:16 PM: 	Task multirc (val): Saved 4848 instances to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__val_data
03/18 05:51:16 PM: 	Task multirc (test): Indexing from scratch.
03/18 05:51:19 PM: 	Task multirc (test): Saved 9693 instances to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__test_data
03/18 05:51:19 PM: 	Finished indexing tasks
03/18 05:51:19 PM: 	Creating trimmed pretraining-only version of multirc train.
03/18 05:51:19 PM: 	Creating trimmed target-only version of multirc train.
03/18 05:51:19 PM: 	  Training on multirc
03/18 05:51:19 PM: 	  Evaluating on multirc
03/18 05:51:19 PM: 	Finished loading tasks in 18.041s
03/18 05:51:19 PM: 	 Tasks: ['multirc']
03/18 05:51:19 PM: Building model...
03/18 05:51:19 PM: Using BERT model (bert-base-cased).
03/18 05:51:19 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /var/folders/mq/hgvjbtjn4td2k7bkt_nhjd8r0000gn/T/tmp0i6d8ani
03/18 05:51:19 PM: copying /var/folders/mq/hgvjbtjn4td2k7bkt_nhjd8r0000gn/T/tmp0i6d8ani to cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
03/18 05:51:19 PM: creating metadata file for /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
03/18 05:51:19 PM: removing temp file /var/folders/mq/hgvjbtjn4td2k7bkt_nhjd8r0000gn/T/tmp0i6d8ani
03/18 05:51:19 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
03/18 05:51:19 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

03/18 05:51:20 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /var/folders/mq/hgvjbtjn4td2k7bkt_nhjd8r0000gn/T/tmp4b3ya51f
03/18 05:51:42 PM: copying /var/folders/mq/hgvjbtjn4td2k7bkt_nhjd8r0000gn/T/tmp4b3ya51f to cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/18 05:51:43 PM: creating metadata file for /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/18 05:51:43 PM: removing temp file /var/folders/mq/hgvjbtjn4td2k7bkt_nhjd8r0000gn/T/tmp4b3ya51f
03/18 05:51:43 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/18 05:51:44 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /var/folders/mq/hgvjbtjn4td2k7bkt_nhjd8r0000gn/T/tmpx4zekeif
03/18 05:51:45 PM: copying /var/folders/mq/hgvjbtjn4td2k7bkt_nhjd8r0000gn/T/tmpx4zekeif to cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/18 05:51:45 PM: creating metadata file for /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/18 05:51:45 PM: removing temp file /var/folders/mq/hgvjbtjn4td2k7bkt_nhjd8r0000gn/T/tmpx4zekeif
03/18 05:51:45 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/18 05:51:45 PM: Initializing parameters
03/18 05:51:45 PM: Done initializing parameters; the following parameters are using their default initialization from their code
03/18 05:51:45 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
03/18 05:51:45 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
03/18 05:51:45 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
03/18 05:51:45 PM:    _text_field_embedder.model.pooler.dense.bias
03/18 05:51:45 PM:    _text_field_embedder.model.pooler.dense.weight
03/18 05:51:45 PM: 	Task 'multirc' params: {
  "cls_type": "mlp",
  "d_hid": 32,
  "pool_type": "max",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
03/18 05:51:45 PM: Model specification:
03/18 05:51:45 PM: MultiTaskModel(
  (sent_encoder): BoWSentEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=768, out_features=32, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([32]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.2)
        (4): Linear(in_features=32, out_features=2, bias=True)
      )
    )
  )
)
03/18 05:51:45 PM: Model parameters:
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/18 05:51:45 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/18 05:51:45 PM: 	multirc_mdl.classifier.classifier.0.weight: Trainable parameter, count 24576 with torch.Size([32, 768])
03/18 05:51:45 PM: 	multirc_mdl.classifier.classifier.0.bias: Trainable parameter, count 32 with torch.Size([32])
03/18 05:51:45 PM: 	multirc_mdl.classifier.classifier.2.weight: Trainable parameter, count 32 with torch.Size([32])
03/18 05:51:45 PM: 	multirc_mdl.classifier.classifier.2.bias: Trainable parameter, count 32 with torch.Size([32])
03/18 05:51:45 PM: 	multirc_mdl.classifier.classifier.4.weight: Trainable parameter, count 64 with torch.Size([2, 32])
03/18 05:51:45 PM: 	multirc_mdl.classifier.classifier.4.bias: Trainable parameter, count 2 with torch.Size([2])
03/18 05:51:45 PM: Total number of parameters: 108335010 (1.08335e+08)
03/18 05:51:45 PM: Number of trainable parameters: 108335010 (1.08335e+08)
03/18 05:51:45 PM: Finished building model in 25.635s
03/18 05:51:45 PM: Will run the following steps for this experiment:
Training model on tasks: multirc 
Evaluating model on tasks: multirc 

03/18 05:51:45 PM: Training...
03/18 05:51:45 PM: patience = 5
03/18 05:51:45 PM: val_interval = 50
03/18 05:51:45 PM: max_vals = 100
03/18 05:51:45 PM: cuda_device = -1
03/18 05:51:45 PM: grad_norm = 5.0
03/18 05:51:45 PM: grad_clipping = None
03/18 05:51:45 PM: lr_decay = 0.99
03/18 05:51:45 PM: min_lr = 1e-06
03/18 05:51:45 PM: keep_all_checkpoints = 0
03/18 05:51:45 PM: val_data_limit = 5000
03/18 05:51:45 PM: max_epochs = -1
03/18 05:51:45 PM: dec_val_scale = 250
03/18 05:51:45 PM: training_data_fraction = 1
03/18 05:51:45 PM: accumulation_steps = 1
03/18 05:51:45 PM: type = adam
03/18 05:51:45 PM: parameter_groups = None
03/18 05:51:45 PM: Number of trainable parameters: 108335010
03/18 05:51:45 PM: infer_type_and_cast = True
03/18 05:51:45 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/18 05:51:45 PM: CURRENTLY DEFINED PARAMETERS: 
03/18 05:51:45 PM: lr = 0.0003
03/18 05:51:45 PM: amsgrad = True
03/18 05:51:45 PM: type = reduce_on_plateau
03/18 05:51:45 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/18 05:51:45 PM: CURRENTLY DEFINED PARAMETERS: 
03/18 05:51:45 PM: mode = max
03/18 05:51:45 PM: factor = 0.5
03/18 05:51:45 PM: patience = 1
03/18 05:51:45 PM: threshold = 0.0001
03/18 05:51:45 PM: threshold_mode = abs
03/18 05:51:45 PM: verbose = True
03/18 05:51:45 PM: Starting training without restoring from a checkpoint.
03/18 05:51:45 PM: Training examples per task, before any subsampling: {'multirc': 27243}
03/18 05:51:45 PM: Beginning training with stopping criteria based on metric: multirc_avg
03/18 05:51:55 PM: Update 6: task multirc, steps since last val 6 (total steps = 6): ans_f1: 0.5965, qst_f1: 0.3696, em: 0.5000, avg: 0.5482, multirc_loss: 0.8173
03/18 05:52:05 PM: Update 13: task multirc, steps since last val 13 (total steps = 13): ans_f1: 0.4835, qst_f1: 0.2157, em: 0.5392, avg: 0.5114, multirc_loss: 0.7939
03/18 05:52:16 PM: Update 21: task multirc, steps since last val 21 (total steps = 21): ans_f1: 0.5443, qst_f1: 0.2667, em: 0.5563, avg: 0.5503, multirc_loss: 0.7513
03/18 05:52:27 PM: Update 28: task multirc, steps since last val 28 (total steps = 28): ans_f1: 0.5776, qst_f1: 0.3104, em: 0.5450, avg: 0.5613, multirc_loss: 0.7412
03/18 05:52:37 PM: Update 35: task multirc, steps since last val 35 (total steps = 35): ans_f1: 0.5859, qst_f1: 0.3262, em: 0.5349, avg: 0.5604, multirc_loss: 0.7287
03/18 05:52:48 PM: Update 43: task multirc, steps since last val 43 (total steps = 43): ans_f1: 0.6209, qst_f1: 0.3774, em: 0.5414, avg: 0.5811, multirc_loss: 0.7152
03/18 05:52:59 PM: Update 50: task multirc, steps since last val 50 (total steps = 50): ans_f1: 0.6356, qst_f1: 0.4006, em: 0.5406, avg: 0.5881, multirc_loss: 0.7123
03/18 05:52:59 PM: ***** Step 50 / Validation 1 *****
03/18 05:52:59 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/18 05:52:59 PM: Validating...
03/18 05:53:09 PM: Evaluate: task multirc, batch 73 (606): ans_f1: 0.6575, qst_f1: 0.6525, em: 0.0081, avg: 0.3328, multirc_loss: 0.7067
03/18 05:53:19 PM: Evaluate: task multirc, batch 143 (606): ans_f1: 0.6108, qst_f1: 0.6024, em: 0.0045, avg: 0.3077, multirc_loss: 0.7213
03/18 05:53:29 PM: Evaluate: task multirc, batch 209 (606): ans_f1: 0.5979, qst_f1: 0.5809, em: 0.0031, avg: 0.3005, multirc_loss: 0.7251
03/18 05:53:40 PM: Evaluate: task multirc, batch 274 (606): ans_f1: 0.5958, qst_f1: 0.5835, em: 0.0023, avg: 0.2990, multirc_loss: 0.7258
03/18 05:53:50 PM: Evaluate: task multirc, batch 339 (606): ans_f1: 0.5908, qst_f1: 0.5779, em: 0.0019, avg: 0.2963, multirc_loss: 0.7272
03/18 05:54:00 PM: Evaluate: task multirc, batch 406 (606): ans_f1: 0.6018, qst_f1: 0.5893, em: 0.0047, avg: 0.3032, multirc_loss: 0.7240
03/18 05:54:11 PM: Evaluate: task multirc, batch 476 (606): ans_f1: 0.6046, qst_f1: 0.5953, em: 0.0104, avg: 0.3075, multirc_loss: 0.7231
03/18 05:54:21 PM: Evaluate: task multirc, batch 544 (606): ans_f1: 0.6027, qst_f1: 0.5936, em: 0.0092, avg: 0.3059, multirc_loss: 0.7237
03/18 05:54:31 PM: Best result seen so far for multirc.
03/18 05:54:31 PM: Best result seen so far for micro.
03/18 05:54:31 PM: Best result seen so far for macro.
03/18 05:54:31 PM: Updating LR scheduler:
03/18 05:54:31 PM: 	Best result seen so far for macro_avg: 0.304
03/18 05:54:31 PM: 	# validation passes without improvement: 0
03/18 05:54:31 PM: multirc_loss: training: 0.712306 validation: 0.724672
03/18 05:54:31 PM: macro_avg: validation: 0.303923
03/18 05:54:31 PM: micro_avg: validation: 0.303923
03/18 05:54:31 PM: multirc_ans_f1: training: 0.635593 validation: 0.599451
03/18 05:54:31 PM: multirc_qst_f1: training: 0.400560 validation: 0.590961
03/18 05:54:31 PM: multirc_em: training: 0.540616 validation: 0.008395
03/18 05:54:31 PM: multirc_avg: training: 0.588105 validation: 0.303923
03/18 05:54:31 PM: Global learning rate: 0.0003
03/18 05:54:31 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run
03/18 05:54:34 PM: Update 51: task multirc, steps since last val 1 (total steps = 51): ans_f1: 0.5000, qst_f1: 0.2500, em: 0.5000, avg: 0.5000, multirc_loss: 0.6980
03/18 05:54:44 PM: Update 58: task multirc, steps since last val 8 (total steps = 58): ans_f1: 0.5070, qst_f1: 0.2812, em: 0.4531, avg: 0.4801, multirc_loss: 0.7296
03/18 05:54:55 PM: Update 65: task multirc, steps since last val 15 (total steps = 65): ans_f1: 0.4112, qst_f1: 0.1864, em: 0.4661, avg: 0.4387, multirc_loss: 0.7225
03/18 05:55:05 PM: Update 72: task multirc, steps since last val 22 (total steps = 72): ans_f1: 0.3378, qst_f1: 0.1462, em: 0.4327, avg: 0.3853, multirc_loss: 0.7507
03/18 05:55:15 PM: Update 79: task multirc, steps since last val 29 (total steps = 79): ans_f1: 0.3646, qst_f1: 0.1562, em: 0.4550, avg: 0.4098, multirc_loss: 0.7321
03/18 05:55:27 PM: Update 87: task multirc, steps since last val 37 (total steps = 87): ans_f1: 0.3775, qst_f1: 0.1655, em: 0.4681, avg: 0.4228, multirc_loss: 0.7309
03/18 05:55:38 PM: Update 95: task multirc, steps since last val 45 (total steps = 95): ans_f1: 0.3922, qst_f1: 0.1756, em: 0.4702, avg: 0.4312, multirc_loss: 0.7297
03/18 05:55:45 PM: ***** Step 100 / Validation 2 *****
03/18 05:55:45 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/18 05:55:45 PM: Validating...
03/18 05:55:48 PM: Evaluate: task multirc, batch 21 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0256, avg: 0.0128, multirc_loss: 0.6940
03/18 05:55:58 PM: Evaluate: task multirc, batch 86 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6922
03/18 05:56:08 PM: Evaluate: task multirc, batch 148 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6889
03/18 05:56:19 PM: Evaluate: task multirc, batch 204 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0063, avg: 0.0032, multirc_loss: 0.6883
03/18 05:56:29 PM: Evaluate: task multirc, batch 264 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0048, avg: 0.0024, multirc_loss: 0.6877
03/18 05:56:39 PM: Evaluate: task multirc, batch 327 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0019, multirc_loss: 0.6874
03/18 05:56:50 PM: Evaluate: task multirc, batch 390 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0049, avg: 0.0025, multirc_loss: 0.6879
03/18 05:57:00 PM: Evaluate: task multirc, batch 444 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0028, avg: 0.0014, multirc_loss: 0.6886
03/18 05:57:11 PM: Evaluate: task multirc, batch 500 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0038, avg: 0.0019, multirc_loss: 0.6884
03/18 05:57:22 PM: Evaluate: task multirc, batch 555 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 0.6882
03/18 05:57:32 PM: Evaluate: task multirc, batch 595 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 0.6879
03/18 05:57:35 PM: Updating LR scheduler:
03/18 05:57:35 PM: 	Best result seen so far for macro_avg: 0.304
03/18 05:57:35 PM: 	# validation passes without improvement: 1
03/18 05:57:35 PM: multirc_loss: training: 0.726893 validation: 0.688098
03/18 05:57:35 PM: macro_avg: validation: 0.001574
03/18 05:57:35 PM: micro_avg: validation: 0.001574
03/18 05:57:35 PM: multirc_ans_f1: training: 0.402332 validation: 0.000000
03/18 05:57:35 PM: multirc_qst_f1: training: 0.180822 validation: 0.000000
03/18 05:57:35 PM: multirc_em: training: 0.460274 validation: 0.003148
03/18 05:57:35 PM: multirc_avg: training: 0.431303 validation: 0.001574
03/18 05:57:35 PM: Global learning rate: 0.0003
03/18 05:57:35 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run
03/18 05:57:43 PM: Update 104: task multirc, steps since last val 4 (total steps = 104): ans_f1: 0.4516, qst_f1: 0.2188, em: 0.4688, avg: 0.4602, multirc_loss: 0.7369
03/18 05:57:54 PM: Update 110: task multirc, steps since last val 10 (total steps = 110): ans_f1: 0.5385, qst_f1: 0.2607, em: 0.5385, avg: 0.5385, multirc_loss: 0.6783
03/18 05:58:05 PM: Update 117: task multirc, steps since last val 17 (total steps = 117): ans_f1: 0.5075, qst_f1: 0.2436, em: 0.4923, avg: 0.4999, multirc_loss: 0.6989
03/18 05:58:15 PM: Update 124: task multirc, steps since last val 24 (total steps = 124): ans_f1: 0.5027, qst_f1: 0.2413, em: 0.4917, avg: 0.4972, multirc_loss: 0.6978
03/18 05:58:26 PM: Update 130: task multirc, steps since last val 30 (total steps = 130): ans_f1: 0.4716, qst_f1: 0.2252, em: 0.4844, avg: 0.4780, multirc_loss: 0.7098
03/18 05:58:37 PM: Update 136: task multirc, steps since last val 36 (total steps = 136): ans_f1: 0.4535, qst_f1: 0.2155, em: 0.4737, avg: 0.4636, multirc_loss: 0.7188
03/18 05:58:48 PM: Update 142: task multirc, steps since last val 42 (total steps = 142): ans_f1: 0.4356, qst_f1: 0.2039, em: 0.4638, avg: 0.4497, multirc_loss: 0.7167
03/18 05:58:59 PM: Update 149: task multirc, steps since last val 49 (total steps = 149): ans_f1: 0.4269, qst_f1: 0.1957, em: 0.4714, avg: 0.4492, multirc_loss: 0.7134
03/18 05:59:01 PM: ***** Step 150 / Validation 3 *****
03/18 05:59:01 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/18 05:59:01 PM: Validating...
03/18 05:59:09 PM: Evaluate: task multirc, batch 56 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7039
03/18 05:59:19 PM: Evaluate: task multirc, batch 118 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6916
03/18 05:59:30 PM: Evaluate: task multirc, batch 182 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 0.6855
03/18 05:59:40 PM: Evaluate: task multirc, batch 247 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0052, avg: 0.0026, multirc_loss: 0.6811
03/18 05:59:50 PM: Evaluate: task multirc, batch 310 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0041, avg: 0.0021, multirc_loss: 0.6814
03/18 06:00:00 PM: Evaluate: task multirc, batch 371 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0052, avg: 0.0026, multirc_loss: 0.6824
03/18 06:00:11 PM: Evaluate: task multirc, batch 432 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0029, avg: 0.0014, multirc_loss: 0.6843
03/18 06:00:22 PM: Evaluate: task multirc, batch 493 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0038, avg: 0.0019, multirc_loss: 0.6843
03/18 06:00:32 PM: Evaluate: task multirc, batch 554 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 0.6837
03/18 06:00:41 PM: Updating LR scheduler:
03/18 06:00:41 PM: 	Best result seen so far for macro_avg: 0.304
03/18 06:00:41 PM: 	# validation passes without improvement: 0
03/18 06:00:41 PM: multirc_loss: training: 0.718299 validation: 0.683502
03/18 06:00:41 PM: macro_avg: validation: 0.001574
03/18 06:00:41 PM: micro_avg: validation: 0.001574
03/18 06:00:41 PM: multirc_ans_f1: training: 0.418338 validation: 0.000000
03/18 06:00:41 PM: multirc_qst_f1: training: 0.192958 validation: 0.000000
03/18 06:00:41 PM: multirc_em: training: 0.459155 validation: 0.003148
03/18 06:00:41 PM: multirc_avg: training: 0.438747 validation: 0.001574
03/18 06:00:41 PM: Global learning rate: 0.00015
03/18 06:00:41 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run
03/18 06:00:45 PM: Update 151: task multirc, steps since last val 1 (total steps = 151): ans_f1: 0.4000, qst_f1: 0.1250, em: 0.6250, avg: 0.5125, multirc_loss: 0.6709
03/18 06:00:55 PM: Update 158: task multirc, steps since last val 8 (total steps = 158): ans_f1: 0.4000, qst_f1: 0.1406, em: 0.5781, avg: 0.4891, multirc_loss: 0.6741
03/18 06:01:05 PM: Update 165: task multirc, steps since last val 15 (total steps = 165): ans_f1: 0.3696, qst_f1: 0.1429, em: 0.5126, avg: 0.4411, multirc_loss: 0.6879
03/18 06:01:16 PM: Update 172: task multirc, steps since last val 22 (total steps = 172): ans_f1: 0.4085, qst_f1: 0.1716, em: 0.5089, avg: 0.4587, multirc_loss: 0.6967
03/18 06:01:26 PM: Update 179: task multirc, steps since last val 29 (total steps = 179): ans_f1: 0.3886, qst_f1: 0.1530, em: 0.5182, avg: 0.4534, multirc_loss: 0.6885
03/18 06:01:37 PM: Update 186: task multirc, steps since last val 36 (total steps = 186): ans_f1: 0.3946, qst_f1: 0.1623, em: 0.5093, avg: 0.4520, multirc_loss: 0.6925
03/18 06:01:48 PM: Update 193: task multirc, steps since last val 43 (total steps = 193): ans_f1: 0.3602, qst_f1: 0.1465, em: 0.4873, avg: 0.4237, multirc_loss: 0.7003
03/18 06:01:59 PM: Update 200: task multirc, steps since last val 50 (total steps = 200): ans_f1: 0.3766, qst_f1: 0.1543, em: 0.4904, avg: 0.4335, multirc_loss: 0.7034
03/18 06:01:59 PM: ***** Step 200 / Validation 4 *****
03/18 06:01:59 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/18 06:01:59 PM: Validating...
03/18 06:02:09 PM: Evaluate: task multirc, batch 55 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7031
03/18 06:02:20 PM: Evaluate: task multirc, batch 109 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6899
03/18 06:02:30 PM: Evaluate: task multirc, batch 163 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6849
03/18 06:02:40 PM: Evaluate: task multirc, batch 214 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0060, avg: 0.0030, multirc_loss: 0.6826
03/18 06:02:50 PM: Evaluate: task multirc, batch 269 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0047, avg: 0.0023, multirc_loss: 0.6826
03/18 06:03:01 PM: Evaluate: task multirc, batch 322 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0040, avg: 0.0020, multirc_loss: 0.6816
03/18 06:03:11 PM: Evaluate: task multirc, batch 363 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0035, avg: 0.0018, multirc_loss: 0.6830
03/18 06:03:22 PM: Evaluate: task multirc, batch 413 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0030, avg: 0.0015, multirc_loss: 0.6846
03/18 06:03:32 PM: Evaluate: task multirc, batch 455 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0027, avg: 0.0014, multirc_loss: 0.6851
03/18 06:03:43 PM: Evaluate: task multirc, batch 504 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0019, multirc_loss: 0.6842
03/18 06:03:53 PM: Evaluate: task multirc, batch 557 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 0.6839
03/18 06:04:03 PM: Updating LR scheduler:
03/18 06:04:03 PM: 	Best result seen so far for macro_avg: 0.304
03/18 06:04:03 PM: 	# validation passes without improvement: 1
03/18 06:04:03 PM: multirc_loss: training: 0.703442 validation: 0.683624
03/18 06:04:03 PM: macro_avg: validation: 0.001574
03/18 06:04:03 PM: micro_avg: validation: 0.001574
03/18 06:04:03 PM: multirc_ans_f1: training: 0.376623 validation: 0.000000
03/18 06:04:03 PM: multirc_qst_f1: training: 0.154270 validation: 0.000000
03/18 06:04:03 PM: multirc_em: training: 0.490358 validation: 0.003148
03/18 06:04:03 PM: multirc_avg: training: 0.433491 validation: 0.001574
03/18 06:04:03 PM: Global learning rate: 0.00015
03/18 06:04:03 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run
03/18 06:04:07 PM: Update 201: task multirc, steps since last val 1 (total steps = 201): ans_f1: 0.4000, qst_f1: 0.2500, em: 0.2500, avg: 0.3250, multirc_loss: 0.8327
03/18 06:04:18 PM: Update 208: task multirc, steps since last val 8 (total steps = 208): ans_f1: 0.3137, qst_f1: 0.1270, em: 0.4444, avg: 0.3791, multirc_loss: 0.7439
03/18 06:04:28 PM: Update 215: task multirc, steps since last val 15 (total steps = 215): ans_f1: 0.3297, qst_f1: 0.1261, em: 0.4874, avg: 0.4085, multirc_loss: 0.7238
03/18 06:04:40 PM: Update 222: task multirc, steps since last val 22 (total steps = 222): ans_f1: 0.3140, qst_f1: 0.1098, em: 0.5202, avg: 0.4171, multirc_loss: 0.7025
03/18 06:04:52 PM: Update 229: task multirc, steps since last val 29 (total steps = 229): ans_f1: 0.3614, qst_f1: 0.1357, em: 0.5204, avg: 0.4409, multirc_loss: 0.6966
03/18 06:05:03 PM: Update 236: task multirc, steps since last val 36 (total steps = 236): ans_f1: 0.3524, qst_f1: 0.1358, em: 0.5037, avg: 0.4280, multirc_loss: 0.7022
03/18 06:05:14 PM: Update 243: task multirc, steps since last val 43 (total steps = 243): ans_f1: 0.3665, qst_f1: 0.1412, em: 0.5171, avg: 0.4418, multirc_loss: 0.6975
03/18 06:05:25 PM: Update 250: task multirc, steps since last val 50 (total steps = 250): ans_f1: 0.3490, qst_f1: 0.1381, em: 0.4946, avg: 0.4218, multirc_loss: 0.7065
03/18 06:05:25 PM: ***** Step 250 / Validation 5 *****
03/18 06:05:25 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/18 06:05:25 PM: Validating...
03/18 06:05:35 PM: Evaluate: task multirc, batch 55 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7081
03/18 06:05:45 PM: Evaluate: task multirc, batch 112 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6913
03/18 06:05:56 PM: Evaluate: task multirc, batch 169 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6843
03/18 06:06:06 PM: Evaluate: task multirc, batch 230 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0084, avg: 0.0042, multirc_loss: 0.6801
03/18 06:06:16 PM: Evaluate: task multirc, batch 294 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0043, avg: 0.0022, multirc_loss: 0.6811
03/18 06:06:27 PM: Evaluate: task multirc, batch 361 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 0.6819
03/18 06:06:37 PM: Evaluate: task multirc, batch 428 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0029, avg: 0.0015, multirc_loss: 0.6836
03/18 06:06:48 PM: Evaluate: task multirc, batch 494 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0038, avg: 0.0019, multirc_loss: 0.6838
03/18 06:06:58 PM: Evaluate: task multirc, batch 556 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 0.6832
03/18 06:07:07 PM: Updating LR scheduler:
03/18 06:07:07 PM: 	Best result seen so far for macro_avg: 0.304
03/18 06:07:07 PM: 	# validation passes without improvement: 0
03/18 06:07:07 PM: multirc_loss: training: 0.706541 validation: 0.682812
03/18 06:07:07 PM: macro_avg: validation: 0.001574
03/18 06:07:07 PM: micro_avg: validation: 0.001574
03/18 06:07:07 PM: multirc_ans_f1: training: 0.348993 validation: 0.000000
03/18 06:07:07 PM: multirc_qst_f1: training: 0.138134 validation: 0.000000
03/18 06:07:07 PM: multirc_em: training: 0.494565 validation: 0.003148
03/18 06:07:07 PM: multirc_avg: training: 0.421779 validation: 0.001574
03/18 06:07:07 PM: Global learning rate: 7.5e-05
03/18 06:07:07 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run
03/18 06:07:10 PM: Update 251: task multirc, steps since last val 1 (total steps = 251): ans_f1: 0.5000, qst_f1: 0.1250, em: 0.7500, avg: 0.6250, multirc_loss: 0.6007
03/18 06:07:20 PM: Update 258: task multirc, steps since last val 8 (total steps = 258): ans_f1: 0.3200, qst_f1: 0.1250, em: 0.4688, avg: 0.3944, multirc_loss: 0.7261
03/18 06:07:30 PM: Update 265: task multirc, steps since last val 15 (total steps = 265): ans_f1: 0.3441, qst_f1: 0.1345, em: 0.4958, avg: 0.4199, multirc_loss: 0.7195
03/18 06:07:41 PM: Update 272: task multirc, steps since last val 22 (total steps = 272): ans_f1: 0.3077, qst_f1: 0.1198, em: 0.4790, avg: 0.3934, multirc_loss: 0.7052
03/18 06:07:52 PM: Update 279: task multirc, steps since last val 29 (total steps = 279): ans_f1: 0.3000, qst_f1: 0.1240, em: 0.4372, avg: 0.3686, multirc_loss: 0.7203
03/18 06:08:03 PM: Update 286: task multirc, steps since last val 36 (total steps = 286): ans_f1: 0.3391, qst_f1: 0.1476, em: 0.4504, avg: 0.3948, multirc_loss: 0.7208
03/18 06:08:14 PM: Update 293: task multirc, steps since last val 43 (total steps = 293): ans_f1: 0.3624, qst_f1: 0.1651, em: 0.4409, avg: 0.4016, multirc_loss: 0.7220
03/18 06:08:24 PM: Update 300: task multirc, steps since last val 50 (total steps = 300): ans_f1: 0.3686, qst_f1: 0.1671, em: 0.4518, avg: 0.4102, multirc_loss: 0.7177
03/18 06:08:24 PM: ***** Step 300 / Validation 6 *****
03/18 06:08:24 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/18 06:08:24 PM: Validating...
03/18 06:08:34 PM: Evaluate: task multirc, batch 57 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6971
03/18 06:08:45 PM: Evaluate: task multirc, batch 97 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6909
03/18 06:08:55 PM: Evaluate: task multirc, batch 150 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6881
03/18 06:09:05 PM: Evaluate: task multirc, batch 195 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0033, avg: 0.0017, multirc_loss: 0.6882
03/18 06:09:15 PM: Evaluate: task multirc, batch 245 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0052, avg: 0.0026, multirc_loss: 0.6859
03/18 06:09:26 PM: Evaluate: task multirc, batch 302 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0042, avg: 0.0021, multirc_loss: 0.6862
03/18 06:09:36 PM: Evaluate: task multirc, batch 359 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 0.6867
03/18 06:09:47 PM: Evaluate: task multirc, batch 424 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0030, avg: 0.0015, multirc_loss: 0.6874
03/18 06:09:57 PM: Evaluate: task multirc, batch 491 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0038, avg: 0.0019, multirc_loss: 0.6875
03/18 06:10:08 PM: Evaluate: task multirc, batch 557 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 0.6873
03/18 06:10:15 PM: Updating LR scheduler:
03/18 06:10:15 PM: 	Best result seen so far for macro_avg: 0.304
03/18 06:10:15 PM: 	# validation passes without improvement: 1
03/18 06:10:15 PM: multirc_loss: training: 0.717713 validation: 0.687098
03/18 06:10:15 PM: macro_avg: validation: 0.001574
03/18 06:10:15 PM: micro_avg: validation: 0.001574
03/18 06:10:15 PM: multirc_ans_f1: training: 0.368580 validation: 0.000000
03/18 06:10:15 PM: multirc_qst_f1: training: 0.167126 validation: 0.000000
03/18 06:10:15 PM: multirc_em: training: 0.451791 validation: 0.003148
03/18 06:10:15 PM: multirc_avg: training: 0.410185 validation: 0.001574
03/18 06:10:15 PM: Global learning rate: 7.5e-05
03/18 06:10:15 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run
03/18 06:10:18 PM: Update 301: task multirc, steps since last val 1 (total steps = 301): ans_f1: 0.2500, qst_f1: 0.1250, em: 0.2500, avg: 0.2500, multirc_loss: 0.8121
03/18 06:10:30 PM: Update 309: task multirc, steps since last val 9 (total steps = 309): ans_f1: 0.5455, qst_f1: 0.2952, em: 0.5143, avg: 0.5299, multirc_loss: 0.7134
03/18 06:10:40 PM: Update 316: task multirc, steps since last val 16 (total steps = 316): ans_f1: 0.5109, qst_f1: 0.2769, em: 0.4758, avg: 0.4934, multirc_loss: 0.7325
03/18 06:10:51 PM: Update 323: task multirc, steps since last val 23 (total steps = 323): ans_f1: 0.5572, qst_f1: 0.3123, em: 0.5115, avg: 0.5344, multirc_loss: 0.7186
03/18 06:11:01 PM: Update 330: task multirc, steps since last val 30 (total steps = 330): ans_f1: 0.5649, qst_f1: 0.3201, em: 0.5177, avg: 0.5413, multirc_loss: 0.7127
03/18 06:11:13 PM: Update 337: task multirc, steps since last val 37 (total steps = 337): ans_f1: 0.5652, qst_f1: 0.3187, em: 0.5092, avg: 0.5372, multirc_loss: 0.7113
03/18 06:11:24 PM: Update 344: task multirc, steps since last val 44 (total steps = 344): ans_f1: 0.5722, qst_f1: 0.3219, em: 0.5171, avg: 0.5447, multirc_loss: 0.7086
03/18 06:11:33 PM: ***** Step 350 / Validation 7 *****
03/18 06:11:33 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/18 06:11:33 PM: Validating...
03/18 06:11:34 PM: Evaluate: task multirc, batch 6 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.1000, avg: 0.0500, multirc_loss: 0.6910
03/18 06:11:44 PM: Evaluate: task multirc, batch 61 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6936
03/18 06:11:54 PM: Evaluate: task multirc, batch 111 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6922
03/18 06:12:04 PM: Evaluate: task multirc, batch 161 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6915
03/18 06:12:15 PM: Evaluate: task multirc, batch 214 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0060, avg: 0.0030, multirc_loss: 0.6912
03/18 06:12:25 PM: Evaluate: task multirc, batch 270 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0047, avg: 0.0023, multirc_loss: 0.6912
03/18 06:12:36 PM: Evaluate: task multirc, batch 329 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0019, multirc_loss: 0.6911
03/18 06:12:46 PM: Evaluate: task multirc, batch 387 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0033, avg: 0.0017, multirc_loss: 0.6912
03/18 06:12:57 PM: Evaluate: task multirc, batch 450 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0041, avg: 0.0021, multirc_loss: 0.6916
03/18 06:13:07 PM: Evaluate: task multirc, batch 515 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0018, multirc_loss: 0.6914
03/18 06:13:18 PM: Evaluate: task multirc, batch 573 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0044, avg: 0.0022, multirc_loss: 0.6913
03/18 06:13:23 PM: Updating LR scheduler:
03/18 06:13:23 PM: 	Best result seen so far for macro_avg: 0.304
03/18 06:13:23 PM: 	# validation passes without improvement: 0
03/18 06:13:23 PM: Ran out of early stopping patience. Stopping training.
03/18 06:13:23 PM: multirc_loss: training: 0.708965 validation: 0.691331
03/18 06:13:23 PM: macro_avg: validation: 0.001574
03/18 06:13:23 PM: micro_avg: validation: 0.001574
03/18 06:13:23 PM: multirc_ans_f1: training: 0.558140 validation: 0.000000
03/18 06:13:23 PM: multirc_qst_f1: training: 0.316527 validation: 0.000000
03/18 06:13:23 PM: multirc_em: training: 0.501401 validation: 0.003148
03/18 06:13:23 PM: multirc_avg: training: 0.529770 validation: 0.001574
03/18 06:13:23 PM: Global learning rate: 3.75e-05
03/18 06:13:23 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run
03/18 06:13:25 PM: Stopped training after 7 validation checks
03/18 06:13:25 PM: Trained multirc for 350 steps or 0.103 epochs
03/18 06:13:25 PM: ***** VALIDATION RESULTS *****
03/18 06:13:25 PM: multirc_avg (for best val pass 1): multirc_loss: 0.72467, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/18 06:13:25 PM: micro_avg (for best val pass 1): multirc_loss: 0.72467, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/18 06:13:25 PM: macro_avg (for best val pass 1): multirc_loss: 0.72467, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/18 06:13:25 PM: Evaluating...
03/18 06:13:25 PM: Loaded model state from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run/model_state_pretrain_val_1.best.th
03/18 06:13:25 PM: Evaluating on: multirc, split: val
03/18 06:13:55 PM: 	Task multirc: batch 196
03/18 06:14:25 PM: 	Task multirc: batch 367
03/18 06:14:56 PM: 	Task multirc: batch 533
03/18 06:15:09 PM: Task 'multirc': sorting predictions by 'idx'
03/18 06:15:09 PM: Finished evaluating on: multirc
03/18 06:15:09 PM: Writing results for split 'val' to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/results.tsv
03/18 06:15:09 PM: micro_avg: 0.304, macro_avg: 0.304, multirc_ans_f1: 0.599, multirc_qst_f1: 0.591, multirc_em: 0.008, multirc_avg: 0.304
03/18 06:15:09 PM: Done!
