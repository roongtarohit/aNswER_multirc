03/19 06:17:54 PM: Git branch: develop
03/19 06:17:54 PM: Git SHA: dd631065bacec9ae53e4fc1832cfe49321974afb
03/19 06:17:54 PM: Parsed args: 
{
  "batch_size": 8,
  "d_word": 50,
  "do_target_task_training": 0,
  "exp_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/",
  "exp_name": "mutlirc_bert_cased_exp",
  "input_module": "bert-base-cased",
  "load_model": 0,
  "local_log_path": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2/log.log",
  "lr": 0.003,
  "max_epochs": 5,
  "max_seq_len": 10,
  "max_vals": 10,
  "max_word_v_size": 1000,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "pretrain_tasks": "multirc",
  "random_seed": 402,
  "remote_log_name": "mutlirc_bert_cased_exp__mutlirc_bert_cased_run2",
  "run_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2",
  "run_name": "mutlirc_bert_cased_run2",
  "sent_enc": "bow",
  "skip_embs": 0,
  "target_tasks": "multirc",
  "target_train_max_vals": 10,
  "target_train_val_interval": 10,
  "transfer_paradigm": "finetune",
  "val_interval": 50
}
03/19 06:17:54 PM: Saved config to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2/params.conf
03/19 06:17:54 PM: Using random seed 402
03/19 06:17:54 PM: Loading tasks...
03/19 06:17:54 PM: Writing pre-preprocessed tasks to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/
03/19 06:17:54 PM: 	Loaded existing task multirc
03/19 06:17:54 PM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
03/19 06:17:54 PM: 	Finished loading tasks: multirc.
03/19 06:17:54 PM: Loading token dictionary from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab.
03/19 06:17:54 PM: 	Loaded vocab from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab
03/19 06:17:54 PM: 	Vocab namespace chars: size 100
03/19 06:17:54 PM: 	Vocab namespace bert_cased: size 28998
03/19 06:17:54 PM: 	Vocab namespace tokens: size 1004
03/19 06:17:54 PM: 	Finished building vocab.
03/19 06:17:54 PM: 	Task 'multirc', split 'train': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__train_data
03/19 06:17:54 PM: 	Task 'multirc', split 'val': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__val_data
03/19 06:17:54 PM: 	Task 'multirc', split 'test': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__test_data
03/19 06:17:54 PM: 	Finished indexing tasks
03/19 06:17:54 PM: 	Creating trimmed pretraining-only version of multirc train.
03/19 06:17:54 PM: 	Creating trimmed target-only version of multirc train.
03/19 06:17:54 PM: 	  Training on multirc
03/19 06:17:54 PM: 	  Evaluating on multirc
03/19 06:17:54 PM: 	Finished loading tasks in 0.094s
03/19 06:17:54 PM: 	 Tasks: ['multirc']
03/19 06:17:54 PM: Building model...
03/19 06:17:54 PM: Using BERT model (bert-base-cased).
03/19 06:17:55 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
03/19 06:17:55 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

03/19 06:17:55 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/19 06:17:57 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/19 06:17:57 PM: Initializing parameters
03/19 06:17:57 PM: Done initializing parameters; the following parameters are using their default initialization from their code
03/19 06:17:57 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
03/19 06:17:57 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
03/19 06:17:57 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
03/19 06:17:57 PM:    _text_field_embedder.model.pooler.dense.bias
03/19 06:17:57 PM:    _text_field_embedder.model.pooler.dense.weight
03/19 06:17:57 PM: 	Task 'multirc' params: {
  "cls_type": "mlp",
  "d_hid": 512,
  "pool_type": "max",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
03/19 06:17:57 PM: Model specification:
03/19 06:17:57 PM: MultiTaskModel(
  (sent_encoder): BoWSentEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=768, out_features=512, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.2)
        (4): Linear(in_features=512, out_features=2, bias=True)
      )
    )
  )
)
03/19 06:17:57 PM: Model parameters:
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:17:57 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:17:57 PM: 	multirc_mdl.classifier.classifier.0.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/19 06:17:57 PM: 	multirc_mdl.classifier.classifier.0.bias: Trainable parameter, count 512 with torch.Size([512])
03/19 06:17:57 PM: 	multirc_mdl.classifier.classifier.2.weight: Trainable parameter, count 512 with torch.Size([512])
03/19 06:17:57 PM: 	multirc_mdl.classifier.classifier.2.bias: Trainable parameter, count 512 with torch.Size([512])
03/19 06:17:57 PM: 	multirc_mdl.classifier.classifier.4.weight: Trainable parameter, count 1024 with torch.Size([2, 512])
03/19 06:17:57 PM: 	multirc_mdl.classifier.classifier.4.bias: Trainable parameter, count 2 with torch.Size([2])
03/19 06:17:57 PM: Total number of parameters: 108706050 (1.08706e+08)
03/19 06:17:57 PM: Number of trainable parameters: 108706050 (1.08706e+08)
03/19 06:17:57 PM: Finished building model in 2.244s
03/19 06:17:57 PM: Will run the following steps for this experiment:
Training model on tasks: multirc 
Evaluating model on tasks: multirc 

03/19 06:17:57 PM: Training...
03/19 06:17:57 PM: patience = 5
03/19 06:17:57 PM: val_interval = 50
03/19 06:17:57 PM: max_vals = 10
03/19 06:17:57 PM: cuda_device = -1
03/19 06:17:57 PM: grad_norm = 5.0
03/19 06:17:57 PM: grad_clipping = None
03/19 06:17:57 PM: lr_decay = 0.99
03/19 06:17:57 PM: min_lr = 1e-06
03/19 06:17:57 PM: keep_all_checkpoints = 0
03/19 06:17:57 PM: val_data_limit = 5000
03/19 06:17:57 PM: max_epochs = 5
03/19 06:17:57 PM: dec_val_scale = 250
03/19 06:17:57 PM: training_data_fraction = 1
03/19 06:17:57 PM: accumulation_steps = 1
03/19 06:17:57 PM: type = bert_adam
03/19 06:17:57 PM: parameter_groups = None
03/19 06:17:57 PM: Number of trainable parameters: 108706050
03/19 06:17:57 PM: infer_type_and_cast = True
03/19 06:17:57 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 06:17:57 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 06:17:57 PM: lr = 0.003
03/19 06:17:57 PM: t_total = 500
03/19 06:17:57 PM: warmup = 0.1
03/19 06:17:57 PM: type = reduce_on_plateau
03/19 06:17:57 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 06:17:57 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 06:17:57 PM: mode = max
03/19 06:17:57 PM: factor = 0.5
03/19 06:17:57 PM: patience = 1
03/19 06:17:57 PM: threshold = 0.0001
03/19 06:17:57 PM: threshold_mode = abs
03/19 06:17:57 PM: verbose = True
03/19 06:17:57 PM: Starting training without restoring from a checkpoint.
03/19 06:17:57 PM: Training examples per task, before any subsampling: {'multirc': 27243}
03/19 06:17:57 PM: Beginning training with stopping criteria based on metric: multirc_avg
03/19 06:18:07 PM: Update 4: task multirc, steps since last val 4 (total steps = 4): ans_f1: 0.3871, qst_f1: 0.1935, em: 0.4194, avg: 0.4032, multirc_loss: 1.8268
03/19 06:18:18 PM: Update 9: task multirc, steps since last val 9 (total steps = 9): ans_f1: 0.5952, qst_f1: 0.3623, em: 0.5217, avg: 0.5585, multirc_loss: 1.3724
03/19 06:18:28 PM: Update 14: task multirc, steps since last val 14 (total steps = 14): ans_f1: 0.4860, qst_f1: 0.2385, em: 0.5046, avg: 0.4953, multirc_loss: 1.2129
03/19 06:18:40 PM: Update 19: task multirc, steps since last val 19 (total steps = 19): ans_f1: 0.5031, qst_f1: 0.2694, em: 0.4658, avg: 0.4844, multirc_loss: 1.1562
03/19 06:18:51 PM: Update 24: task multirc, steps since last val 24 (total steps = 24): ans_f1: 0.4835, qst_f1: 0.2368, em: 0.4973, avg: 0.4904, multirc_loss: 1.1025
03/19 06:19:02 PM: Update 29: task multirc, steps since last val 29 (total steps = 29): ans_f1: 0.4424, qst_f1: 0.2160, em: 0.4583, avg: 0.4504, multirc_loss: 1.0809
03/19 06:19:13 PM: Update 33: task multirc, steps since last val 33 (total steps = 33): ans_f1: 0.4362, qst_f1: 0.2104, em: 0.4590, avg: 0.4476, multirc_loss: 1.0611
03/19 06:19:24 PM: Update 38: task multirc, steps since last val 38 (total steps = 38): ans_f1: 0.4260, qst_f1: 0.2036, em: 0.4500, avg: 0.4380, multirc_loss: 1.1227
03/19 06:19:35 PM: Update 43: task multirc, steps since last val 43 (total steps = 43): ans_f1: 0.4479, qst_f1: 0.2197, em: 0.4650, avg: 0.4565, multirc_loss: 1.1163
03/19 06:19:45 PM: Update 48: task multirc, steps since last val 48 (total steps = 48): ans_f1: 0.4654, qst_f1: 0.2303, em: 0.4653, avg: 0.4653, multirc_loss: 1.1334
03/19 06:19:49 PM: ***** Step 50 / Validation 1 *****
03/19 06:19:49 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 06:19:49 PM: Validating...
03/19 06:19:55 PM: Evaluate: task multirc, batch 46 (606): ans_f1: 0.6667, qst_f1: 0.6620, em: 0.0122, avg: 0.3394, multirc_loss: 0.7340
03/19 06:20:06 PM: Evaluate: task multirc, batch 116 (606): ans_f1: 0.6343, qst_f1: 0.6308, em: 0.0112, avg: 0.3228, multirc_loss: 0.7544
03/19 06:20:16 PM: Evaluate: task multirc, batch 181 (606): ans_f1: 0.6097, qst_f1: 0.5980, em: 0.0073, avg: 0.3085, multirc_loss: 0.7693
03/19 06:20:26 PM: Evaluate: task multirc, batch 240 (606): ans_f1: 0.5903, qst_f1: 0.5764, em: 0.0027, avg: 0.2965, multirc_loss: 0.7807
03/19 06:20:36 PM: Evaluate: task multirc, batch 299 (606): ans_f1: 0.5917, qst_f1: 0.5789, em: 0.0021, avg: 0.2969, multirc_loss: 0.7799
03/19 06:20:47 PM: Evaluate: task multirc, batch 361 (606): ans_f1: 0.5960, qst_f1: 0.5831, em: 0.0018, avg: 0.2989, multirc_loss: 0.7774
03/19 06:20:57 PM: Evaluate: task multirc, batch 428 (606): ans_f1: 0.6022, qst_f1: 0.5910, em: 0.0118, avg: 0.3070, multirc_loss: 0.7738
03/19 06:21:07 PM: Evaluate: task multirc, batch 499 (606): ans_f1: 0.6035, qst_f1: 0.5942, em: 0.0101, avg: 0.3068, multirc_loss: 0.7730
03/19 06:21:18 PM: Evaluate: task multirc, batch 570 (606): ans_f1: 0.5982, qst_f1: 0.5902, em: 0.0089, avg: 0.3036, multirc_loss: 0.7761
03/19 06:21:24 PM: Best result seen so far for multirc.
03/19 06:21:24 PM: Best result seen so far for micro.
03/19 06:21:24 PM: Best result seen so far for macro.
03/19 06:21:24 PM: Updating LR scheduler:
03/19 06:21:24 PM: 	Best result seen so far for macro_avg: 0.304
03/19 06:21:24 PM: 	# validation passes without improvement: 0
03/19 06:21:24 PM: multirc_loss: training: 1.135870 validation: 0.775388
03/19 06:21:24 PM: macro_avg: validation: 0.303923
03/19 06:21:24 PM: micro_avg: validation: 0.303923
03/19 06:21:24 PM: multirc_ans_f1: training: 0.456522 validation: 0.599451
03/19 06:21:24 PM: multirc_qst_f1: training: 0.219760 validation: 0.590961
03/19 06:21:24 PM: multirc_em: training: 0.468144 validation: 0.008395
03/19 06:21:24 PM: multirc_avg: training: 0.462333 validation: 0.303923
03/19 06:21:24 PM: Global learning rate: 0.003
03/19 06:21:24 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2
03/19 06:21:29 PM: Update 52: task multirc, steps since last val 2 (total steps = 52): ans_f1: 0.6364, qst_f1: 0.4375, em: 0.5000, avg: 0.5682, multirc_loss: 0.8684
03/19 06:21:40 PM: Update 57: task multirc, steps since last val 7 (total steps = 57): ans_f1: 0.4839, qst_f1: 0.2679, em: 0.4286, avg: 0.4562, multirc_loss: 1.7079
03/19 06:21:52 PM: Update 62: task multirc, steps since last val 12 (total steps = 62): ans_f1: 0.4516, qst_f1: 0.2188, em: 0.4688, avg: 0.4602, multirc_loss: 1.7085
03/19 06:22:02 PM: Update 66: task multirc, steps since last val 16 (total steps = 66): ans_f1: 0.4593, qst_f1: 0.2446, em: 0.4274, avg: 0.4433, multirc_loss: 2.4873
03/19 06:22:14 PM: Update 71: task multirc, steps since last val 21 (total steps = 71): ans_f1: 0.4405, qst_f1: 0.2257, em: 0.4348, avg: 0.4376, multirc_loss: 2.4413
03/19 06:22:26 PM: Update 76: task multirc, steps since last val 26 (total steps = 76): ans_f1: 0.3936, qst_f1: 0.1826, em: 0.4472, avg: 0.4204, multirc_loss: 2.4500
03/19 06:22:38 PM: Update 80: task multirc, steps since last val 30 (total steps = 80): ans_f1: 0.4292, qst_f1: 0.2164, em: 0.4342, avg: 0.4317, multirc_loss: 2.3802
03/19 06:22:49 PM: Update 85: task multirc, steps since last val 35 (total steps = 85): ans_f1: 0.4259, qst_f1: 0.2096, em: 0.4470, avg: 0.4364, multirc_loss: 2.1995
03/19 06:23:00 PM: Update 90: task multirc, steps since last val 40 (total steps = 90): ans_f1: 0.4252, qst_f1: 0.2104, em: 0.4485, avg: 0.4369, multirc_loss: 2.0525
03/19 06:23:11 PM: Update 95: task multirc, steps since last val 45 (total steps = 95): ans_f1: 0.4340, qst_f1: 0.2153, em: 0.4543, avg: 0.4441, multirc_loss: 1.9170
03/19 06:23:23 PM: Update 100: task multirc, steps since last val 50 (total steps = 100): ans_f1: 0.4247, qst_f1: 0.2096, em: 0.4499, avg: 0.4373, multirc_loss: 1.8113
03/19 06:23:23 PM: ***** Step 100 / Validation 2 *****
03/19 06:23:23 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 06:23:23 PM: Validating...
03/19 06:23:33 PM: Evaluate: task multirc, batch 67 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0087, avg: 0.0043, multirc_loss: 1.1710
03/19 06:23:43 PM: Evaluate: task multirc, batch 128 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 1.0767
03/19 06:23:53 PM: Evaluate: task multirc, batch 194 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 1.0397
03/19 06:24:04 PM: Evaluate: task multirc, batch 253 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0051, avg: 0.0025, multirc_loss: 0.9945
03/19 06:24:14 PM: Evaluate: task multirc, batch 312 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0041, avg: 0.0020, multirc_loss: 0.9951
03/19 06:24:24 PM: Evaluate: task multirc, batch 379 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 1.0047
03/19 06:24:35 PM: Evaluate: task multirc, batch 450 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0041, avg: 0.0021, multirc_loss: 1.0333
03/19 06:24:45 PM: Evaluate: task multirc, batch 519 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0049, avg: 0.0024, multirc_loss: 1.0191
03/19 06:24:56 PM: Evaluate: task multirc, batch 585 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0033, avg: 0.0016, multirc_loss: 1.0111
03/19 06:24:59 PM: Updating LR scheduler:
03/19 06:24:59 PM: 	Best result seen so far for macro_avg: 0.304
03/19 06:24:59 PM: 	# validation passes without improvement: 1
03/19 06:24:59 PM: multirc_loss: training: 1.811251 validation: 1.015626
03/19 06:24:59 PM: macro_avg: validation: 0.001574
03/19 06:24:59 PM: micro_avg: validation: 0.001574
03/19 06:24:59 PM: multirc_ans_f1: training: 0.424731 validation: 0.000000
03/19 06:24:59 PM: multirc_qst_f1: training: 0.209575 validation: 0.000000
03/19 06:24:59 PM: multirc_em: training: 0.449864 validation: 0.003148
03/19 06:24:59 PM: multirc_avg: training: 0.437298 validation: 0.001574
03/19 06:24:59 PM: Global learning rate: 0.003
03/19 06:24:59 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2
03/19 06:25:07 PM: Update 103: task multirc, steps since last val 3 (total steps = 103): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.5417, avg: 0.2708, multirc_loss: 1.0001
03/19 06:25:17 PM: Update 108: task multirc, steps since last val 8 (total steps = 108): ans_f1: 0.5217, qst_f1: 0.2812, em: 0.4844, avg: 0.5031, multirc_loss: 0.9188
03/19 06:25:27 PM: Update 113: task multirc, steps since last val 13 (total steps = 113): ans_f1: 0.4138, qst_f1: 0.1748, em: 0.5049, avg: 0.4593, multirc_loss: 0.8812
03/19 06:25:39 PM: Update 118: task multirc, steps since last val 18 (total steps = 118): ans_f1: 0.4341, qst_f1: 0.1958, em: 0.4895, avg: 0.4618, multirc_loss: 0.9008
03/19 06:25:50 PM: Update 123: task multirc, steps since last val 23 (total steps = 123): ans_f1: 0.4000, qst_f1: 0.1694, em: 0.4917, avg: 0.4459, multirc_loss: 0.8830
03/19 06:26:02 PM: Update 128: task multirc, steps since last val 28 (total steps = 128): ans_f1: 0.3736, qst_f1: 0.1535, em: 0.4884, avg: 0.4310, multirc_loss: 0.9080
03/19 06:26:15 PM: Update 133: task multirc, steps since last val 33 (total steps = 133): ans_f1: 0.3486, qst_f1: 0.1460, em: 0.4600, avg: 0.4043, multirc_loss: 0.9192
03/19 06:26:26 PM: Update 137: task multirc, steps since last val 37 (total steps = 137): ans_f1: 0.3515, qst_f1: 0.1445, em: 0.4712, avg: 0.4113, multirc_loss: 0.8989
03/19 06:26:36 PM: Update 141: task multirc, steps since last val 41 (total steps = 141): ans_f1: 0.3574, qst_f1: 0.1491, em: 0.4686, avg: 0.4130, multirc_loss: 0.8987
03/19 06:26:47 PM: Update 145: task multirc, steps since last val 45 (total steps = 145): ans_f1: 0.4153, qst_f1: 0.1843, em: 0.4727, avg: 0.4440, multirc_loss: 0.9378
03/19 06:27:00 PM: Update 150: task multirc, steps since last val 50 (total steps = 150): ans_f1: 0.4314, qst_f1: 0.1989, em: 0.4725, avg: 0.4520, multirc_loss: 0.9390
03/19 06:27:00 PM: ***** Step 150 / Validation 3 *****
03/19 06:27:00 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 06:27:00 PM: Validating...
03/19 06:27:10 PM: Evaluate: task multirc, batch 73 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7255
03/19 06:27:20 PM: Evaluate: task multirc, batch 144 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6965
03/19 06:27:30 PM: Evaluate: task multirc, batch 213 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0091, avg: 0.0045, multirc_loss: 0.6885
03/19 06:27:41 PM: Evaluate: task multirc, batch 282 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0045, avg: 0.0022, multirc_loss: 0.6881
03/19 06:27:51 PM: Evaluate: task multirc, batch 352 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 0.6870
03/19 06:28:01 PM: Evaluate: task multirc, batch 419 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0030, avg: 0.0015, multirc_loss: 0.6933
03/19 06:28:12 PM: Evaluate: task multirc, batch 481 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0019, multirc_loss: 0.6938
03/19 06:28:22 PM: Evaluate: task multirc, batch 542 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0035, avg: 0.0017, multirc_loss: 0.6928
03/19 06:28:33 PM: Evaluate: task multirc, batch 605 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 0.6909
03/19 06:28:33 PM: Updating LR scheduler:
03/19 06:28:33 PM: 	Best result seen so far for macro_avg: 0.304
03/19 06:28:33 PM: 	# validation passes without improvement: 0
03/19 06:28:33 PM: multirc_loss: training: 0.939036 validation: 0.691243
03/19 06:28:33 PM: macro_avg: validation: 0.001574
03/19 06:28:33 PM: micro_avg: validation: 0.001574
03/19 06:28:33 PM: multirc_ans_f1: training: 0.431373 validation: 0.000000
03/19 06:28:33 PM: multirc_qst_f1: training: 0.198901 validation: 0.000000
03/19 06:28:33 PM: multirc_em: training: 0.472527 validation: 0.003148
03/19 06:28:33 PM: multirc_avg: training: 0.451950 validation: 0.001574
03/19 06:28:33 PM: Global learning rate: 0.0015
03/19 06:28:33 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2
03/19 06:28:44 PM: Update 155: task multirc, steps since last val 5 (total steps = 155): ans_f1: 0.1600, qst_f1: 0.0500, em: 0.4750, avg: 0.3175, multirc_loss: 0.8218
03/19 06:28:55 PM: Update 160: task multirc, steps since last val 10 (total steps = 160): ans_f1: 0.5176, qst_f1: 0.2838, em: 0.5000, avg: 0.5088, multirc_loss: 0.8895
03/19 06:29:05 PM: Update 165: task multirc, steps since last val 15 (total steps = 165): ans_f1: 0.5693, qst_f1: 0.3363, em: 0.5133, avg: 0.5413, multirc_loss: 0.9144
03/19 06:29:16 PM: Update 170: task multirc, steps since last val 20 (total steps = 170): ans_f1: 0.5256, qst_f1: 0.2680, em: 0.5270, avg: 0.5263, multirc_loss: 0.8486
03/19 06:29:27 PM: Update 175: task multirc, steps since last val 25 (total steps = 175): ans_f1: 0.4751, qst_f1: 0.2222, em: 0.5108, avg: 0.4929, multirc_loss: 0.8274
03/19 06:29:37 PM: Update 180: task multirc, steps since last val 30 (total steps = 180): ans_f1: 0.5214, qst_f1: 0.2631, em: 0.5157, avg: 0.5185, multirc_loss: 0.8288
03/19 06:29:48 PM: Update 185: task multirc, steps since last val 35 (total steps = 185): ans_f1: 0.5211, qst_f1: 0.2727, em: 0.4862, avg: 0.5036, multirc_loss: 0.8510
03/19 06:29:59 PM: Update 190: task multirc, steps since last val 40 (total steps = 190): ans_f1: 0.5093, qst_f1: 0.2669, em: 0.4755, avg: 0.4924, multirc_loss: 0.8494
03/19 06:30:11 PM: Update 195: task multirc, steps since last val 45 (total steps = 195): ans_f1: 0.5042, qst_f1: 0.2594, em: 0.4719, avg: 0.4880, multirc_loss: 0.8345
03/19 06:30:21 PM: Update 200: task multirc, steps since last val 50 (total steps = 200): ans_f1: 0.5301, qst_f1: 0.2857, em: 0.4686, avg: 0.4993, multirc_loss: 0.8336
03/19 06:30:21 PM: ***** Step 200 / Validation 4 *****
03/19 06:30:21 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 06:30:21 PM: Validating...
03/19 06:30:31 PM: Evaluate: task multirc, batch 69 (606): ans_f1: 0.6651, qst_f1: 0.6574, em: 0.0085, avg: 0.3368, multirc_loss: 0.6949
03/19 06:30:41 PM: Evaluate: task multirc, batch 131 (606): ans_f1: 0.6229, qst_f1: 0.6148, em: 0.0049, avg: 0.3139, multirc_loss: 0.7001
03/19 06:30:52 PM: Evaluate: task multirc, batch 193 (606): ans_f1: 0.6096, qst_f1: 0.5949, em: 0.0034, avg: 0.3065, multirc_loss: 0.7017
03/19 06:31:02 PM: Evaluate: task multirc, batch 238 (606): ans_f1: 0.5912, qst_f1: 0.5769, em: 0.0027, avg: 0.2969, multirc_loss: 0.7038
03/19 06:31:13 PM: Evaluate: task multirc, batch 285 (606): ans_f1: 0.5943, qst_f1: 0.5814, em: 0.0022, avg: 0.2983, multirc_loss: 0.7034
03/19 06:31:23 PM: Evaluate: task multirc, batch 335 (606): ans_f1: 0.5913, qst_f1: 0.5783, em: 0.0019, avg: 0.2966, multirc_loss: 0.7038
03/19 06:31:33 PM: Evaluate: task multirc, batch 388 (606): ans_f1: 0.5958, qst_f1: 0.5822, em: 0.0033, avg: 0.2996, multirc_loss: 0.7033
03/19 06:31:44 PM: Evaluate: task multirc, batch 450 (606): ans_f1: 0.6076, qst_f1: 0.5970, em: 0.0110, avg: 0.3093, multirc_loss: 0.7019
03/19 06:31:54 PM: Evaluate: task multirc, batch 516 (606): ans_f1: 0.6009, qst_f1: 0.5918, em: 0.0098, avg: 0.3053, multirc_loss: 0.7027
03/19 06:32:05 PM: Evaluate: task multirc, batch 582 (606): ans_f1: 0.5970, qst_f1: 0.5889, em: 0.0087, avg: 0.3028, multirc_loss: 0.7031
03/19 06:32:09 PM: Updating LR scheduler:
03/19 06:32:09 PM: 	Best result seen so far for macro_avg: 0.304
03/19 06:32:09 PM: 	# validation passes without improvement: 1
03/19 06:32:09 PM: multirc_loss: training: 0.833626 validation: 0.702857
03/19 06:32:09 PM: macro_avg: validation: 0.303923
03/19 06:32:09 PM: micro_avg: validation: 0.303923
03/19 06:32:09 PM: multirc_ans_f1: training: 0.530120 validation: 0.599451
03/19 06:32:09 PM: multirc_qst_f1: training: 0.285714 validation: 0.590961
03/19 06:32:09 PM: multirc_em: training: 0.468571 validation: 0.008395
03/19 06:32:09 PM: multirc_avg: training: 0.499346 validation: 0.303923
03/19 06:32:09 PM: Global learning rate: 0.0015
03/19 06:32:09 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2
03/19 06:32:16 PM: Update 203: task multirc, steps since last val 3 (total steps = 203): ans_f1: 0.3810, qst_f1: 0.1667, em: 0.4583, avg: 0.4196, multirc_loss: 0.8366
03/19 06:32:27 PM: Update 208: task multirc, steps since last val 8 (total steps = 208): ans_f1: 0.2000, qst_f1: 0.0635, em: 0.4921, avg: 0.3460, multirc_loss: 0.8398
03/19 06:32:37 PM: Update 213: task multirc, steps since last val 13 (total steps = 213): ans_f1: 0.2353, qst_f1: 0.0784, em: 0.4902, avg: 0.3627, multirc_loss: 0.8198
03/19 06:32:48 PM: Update 218: task multirc, steps since last val 18 (total steps = 218): ans_f1: 0.3178, qst_f1: 0.1206, em: 0.4823, avg: 0.4000, multirc_loss: 0.7958
03/19 06:32:59 PM: Update 223: task multirc, steps since last val 23 (total steps = 223): ans_f1: 0.3784, qst_f1: 0.1573, em: 0.4944, avg: 0.4364, multirc_loss: 0.7944
03/19 06:33:10 PM: Update 228: task multirc, steps since last val 28 (total steps = 228): ans_f1: 0.3687, qst_f1: 0.1541, em: 0.4811, avg: 0.4249, multirc_loss: 0.8086
03/19 06:33:20 PM: Update 233: task multirc, steps since last val 33 (total steps = 233): ans_f1: 0.3679, qst_f1: 0.1553, em: 0.4739, avg: 0.4209, multirc_loss: 0.7969
03/19 06:33:31 PM: Update 238: task multirc, steps since last val 38 (total steps = 238): ans_f1: 0.3760, qst_f1: 0.1643, em: 0.4610, avg: 0.4185, multirc_loss: 0.7955
03/19 06:33:41 PM: Update 243: task multirc, steps since last val 43 (total steps = 243): ans_f1: 0.3806, qst_f1: 0.1709, em: 0.4525, avg: 0.4166, multirc_loss: 0.8023
03/19 06:33:52 PM: Update 248: task multirc, steps since last val 48 (total steps = 248): ans_f1: 0.3683, qst_f1: 0.1633, em: 0.4556, avg: 0.4119, multirc_loss: 0.8050
03/19 06:33:57 PM: ***** Step 250 / Validation 5 *****
03/19 06:33:57 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 06:33:57 PM: Validating...
03/19 06:34:02 PM: Evaluate: task multirc, batch 29 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7996
03/19 06:34:12 PM: Evaluate: task multirc, batch 91 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7651
03/19 06:34:22 PM: Evaluate: task multirc, batch 160 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7307
03/19 06:34:32 PM: Evaluate: task multirc, batch 221 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0088, avg: 0.0044, multirc_loss: 0.7175
03/19 06:34:43 PM: Evaluate: task multirc, batch 282 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0045, avg: 0.0022, multirc_loss: 0.7195
03/19 06:34:53 PM: Evaluate: task multirc, batch 345 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0019, multirc_loss: 0.7147
03/19 06:35:03 PM: Evaluate: task multirc, batch 412 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0030, avg: 0.0015, multirc_loss: 0.7288
03/19 06:35:14 PM: Evaluate: task multirc, batch 478 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0019, multirc_loss: 0.7289
03/19 06:35:24 PM: Evaluate: task multirc, batch 543 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0035, avg: 0.0017, multirc_loss: 0.7272
03/19 06:35:35 PM: Updating LR scheduler:
03/19 06:35:35 PM: 	Best result seen so far for macro_avg: 0.304
03/19 06:35:35 PM: 	# validation passes without improvement: 0
03/19 06:35:35 PM: multirc_loss: training: 0.799073 validation: 0.724405
03/19 06:35:35 PM: macro_avg: validation: 0.001574
03/19 06:35:35 PM: micro_avg: validation: 0.001574
03/19 06:35:35 PM: multirc_ans_f1: training: 0.361371 validation: 0.000000
03/19 06:35:35 PM: multirc_qst_f1: training: 0.156593 validation: 0.000000
03/19 06:35:35 PM: multirc_em: training: 0.461538 validation: 0.003148
03/19 06:35:35 PM: multirc_avg: training: 0.411455 validation: 0.001574
03/19 06:35:35 PM: Global learning rate: 0.00075
03/19 06:35:35 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2
03/19 06:35:38 PM: Update 251: task multirc, steps since last val 1 (total steps = 251): ans_f1: 0.6000, qst_f1: 0.3750, em: 0.5000, avg: 0.5500, multirc_loss: 0.9841
03/19 06:35:48 PM: Update 256: task multirc, steps since last val 6 (total steps = 256): ans_f1: 0.2963, qst_f1: 0.0870, em: 0.5870, avg: 0.4416, multirc_loss: 0.7592
03/19 06:36:00 PM: Update 262: task multirc, steps since last val 12 (total steps = 262): ans_f1: 0.3492, qst_f1: 0.1111, em: 0.5444, avg: 0.4468, multirc_loss: 0.7345
03/19 06:36:10 PM: Update 267: task multirc, steps since last val 17 (total steps = 267): ans_f1: 0.3505, qst_f1: 0.1260, em: 0.5118, avg: 0.4312, multirc_loss: 0.7407
03/19 06:36:22 PM: Update 272: task multirc, steps since last val 22 (total steps = 272): ans_f1: 0.4085, qst_f1: 0.1636, em: 0.4969, avg: 0.4527, multirc_loss: 0.7354
03/19 06:36:32 PM: Update 277: task multirc, steps since last val 27 (total steps = 277): ans_f1: 0.4066, qst_f1: 0.1683, em: 0.4750, avg: 0.4408, multirc_loss: 0.7495
03/19 06:36:42 PM: Update 282: task multirc, steps since last val 32 (total steps = 282): ans_f1: 0.4074, qst_f1: 0.1746, em: 0.4632, avg: 0.4353, multirc_loss: 0.7451
03/19 06:36:54 PM: Update 288: task multirc, steps since last val 38 (total steps = 288): ans_f1: 0.4143, qst_f1: 0.1758, em: 0.4872, avg: 0.4508, multirc_loss: 0.7271
03/19 06:37:05 PM: Update 293: task multirc, steps since last val 43 (total steps = 293): ans_f1: 0.4251, qst_f1: 0.1841, em: 0.4869, avg: 0.4560, multirc_loss: 0.7250
03/19 06:37:15 PM: Update 298: task multirc, steps since last val 48 (total steps = 298): ans_f1: 0.4425, qst_f1: 0.2023, em: 0.4721, avg: 0.4573, multirc_loss: 0.7317
03/19 06:37:20 PM: ***** Step 300 / Validation 6 *****
03/19 06:37:20 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 06:37:20 PM: Validating...
03/19 06:37:26 PM: Evaluate: task multirc, batch 44 (606): ans_f1: 0.6717, qst_f1: 0.6658, em: 0.0127, avg: 0.3422, multirc_loss: 0.7739
03/19 06:37:36 PM: Evaluate: task multirc, batch 114 (606): ans_f1: 0.6327, qst_f1: 0.6284, em: 0.0057, avg: 0.3192, multirc_loss: 0.8099
03/19 06:37:46 PM: Evaluate: task multirc, batch 178 (606): ans_f1: 0.6080, qst_f1: 0.5964, em: 0.0037, avg: 0.3059, multirc_loss: 0.8317
03/19 06:37:56 PM: Evaluate: task multirc, batch 241 (606): ans_f1: 0.5891, qst_f1: 0.5748, em: 0.0027, avg: 0.2959, multirc_loss: 0.8478
03/19 06:38:07 PM: Evaluate: task multirc, batch 303 (606): ans_f1: 0.5907, qst_f1: 0.5777, em: 0.0021, avg: 0.2964, multirc_loss: 0.8465
03/19 06:38:17 PM: Evaluate: task multirc, batch 367 (606): ans_f1: 0.5956, qst_f1: 0.5812, em: 0.0017, avg: 0.2986, multirc_loss: 0.8424
03/19 06:38:27 PM: Evaluate: task multirc, batch 430 (606): ans_f1: 0.6025, qst_f1: 0.5916, em: 0.0117, avg: 0.3071, multirc_loss: 0.8365
03/19 06:38:38 PM: Evaluate: task multirc, batch 485 (606): ans_f1: 0.6038, qst_f1: 0.5949, em: 0.0102, avg: 0.3070, multirc_loss: 0.8353
03/19 06:38:48 PM: Evaluate: task multirc, batch 541 (606): ans_f1: 0.6021, qst_f1: 0.5926, em: 0.0093, avg: 0.3057, multirc_loss: 0.8368
03/19 06:38:59 PM: Evaluate: task multirc, batch 593 (606): ans_f1: 0.5967, qst_f1: 0.5883, em: 0.0086, avg: 0.3026, multirc_loss: 0.8414
03/19 06:39:02 PM: Updating LR scheduler:
03/19 06:39:02 PM: 	Best result seen so far for macro_avg: 0.304
03/19 06:39:02 PM: 	# validation passes without improvement: 1
03/19 06:39:02 PM: multirc_loss: training: 0.732782 validation: 0.839056
03/19 06:39:02 PM: macro_avg: validation: 0.303923
03/19 06:39:02 PM: micro_avg: validation: 0.303923
03/19 06:39:02 PM: multirc_ans_f1: training: 0.453039 validation: 0.599451
03/19 06:39:02 PM: multirc_qst_f1: training: 0.214218 validation: 0.590961
03/19 06:39:02 PM: multirc_em: training: 0.471751 validation: 0.008395
03/19 06:39:02 PM: multirc_avg: training: 0.462395 validation: 0.303923
03/19 06:39:02 PM: Global learning rate: 0.00075
03/19 06:39:02 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2
03/19 06:39:10 PM: Update 303: task multirc, steps since last val 3 (total steps = 303): ans_f1: 0.8108, qst_f1: 0.6377, em: 0.6957, avg: 0.7532, multirc_loss: 0.5631
03/19 06:39:22 PM: Update 308: task multirc, steps since last val 8 (total steps = 308): ans_f1: 0.7234, qst_f1: 0.5269, em: 0.5806, avg: 0.6520, multirc_loss: 0.6753
03/19 06:39:34 PM: Update 313: task multirc, steps since last val 13 (total steps = 313): ans_f1: 0.6377, qst_f1: 0.4125, em: 0.5050, avg: 0.5713, multirc_loss: 0.7240
03/19 06:39:47 PM: Update 318: task multirc, steps since last val 18 (total steps = 318): ans_f1: 0.5967, qst_f1: 0.3664, em: 0.4823, avg: 0.5395, multirc_loss: 0.7373
03/19 06:39:57 PM: Update 321: task multirc, steps since last val 21 (total steps = 321): ans_f1: 0.6058, qst_f1: 0.3644, em: 0.4907, avg: 0.5482, multirc_loss: 0.7411
03/19 06:40:08 PM: Update 325: task multirc, steps since last val 25 (total steps = 325): ans_f1: 0.6000, qst_f1: 0.3386, em: 0.5158, avg: 0.5579, multirc_loss: 0.7193
03/19 06:40:20 PM: Update 330: task multirc, steps since last val 30 (total steps = 330): ans_f1: 0.5692, qst_f1: 0.2940, em: 0.5240, avg: 0.5466, multirc_loss: 0.7305
03/19 06:40:31 PM: Update 335: task multirc, steps since last val 35 (total steps = 335): ans_f1: 0.5387, qst_f1: 0.2579, em: 0.5283, avg: 0.5335, multirc_loss: 0.7274
03/19 06:40:43 PM: Update 340: task multirc, steps since last val 40 (total steps = 340): ans_f1: 0.5102, qst_f1: 0.2329, em: 0.5232, avg: 0.5167, multirc_loss: 0.7430
03/19 06:40:55 PM: Update 345: task multirc, steps since last val 45 (total steps = 345): ans_f1: 0.5078, qst_f1: 0.2262, em: 0.5345, avg: 0.5212, multirc_loss: 0.7302
03/19 06:41:07 PM: Update 350: task multirc, steps since last val 50 (total steps = 350): ans_f1: 0.4885, qst_f1: 0.2151, em: 0.5205, avg: 0.5045, multirc_loss: 0.7310
03/19 06:41:07 PM: ***** Step 350 / Validation 7 *****
03/19 06:41:07 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 06:41:07 PM: Validating...
03/19 06:41:17 PM: Evaluate: task multirc, batch 60 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0095, avg: 0.0048, multirc_loss: 0.7042
03/19 06:41:27 PM: Evaluate: task multirc, batch 117 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0056, avg: 0.0028, multirc_loss: 0.6920
03/19 06:41:37 PM: Evaluate: task multirc, batch 177 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6851
03/19 06:41:47 PM: Evaluate: task multirc, batch 236 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0054, avg: 0.0027, multirc_loss: 0.6812
03/19 06:41:58 PM: Evaluate: task multirc, batch 292 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0044, avg: 0.0022, multirc_loss: 0.6816
03/19 06:42:08 PM: Evaluate: task multirc, batch 350 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0018, multirc_loss: 0.6808
03/19 06:42:19 PM: Evaluate: task multirc, batch 410 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0031, avg: 0.0015, multirc_loss: 0.6840
03/19 06:42:29 PM: Evaluate: task multirc, batch 469 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0020, multirc_loss: 0.6844
03/19 06:42:40 PM: Evaluate: task multirc, batch 527 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 0.6835
03/19 06:42:50 PM: Evaluate: task multirc, batch 584 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0033, avg: 0.0016, multirc_loss: 0.6825
03/19 06:42:55 PM: Updating LR scheduler:
03/19 06:42:55 PM: 	Best result seen so far for macro_avg: 0.304
03/19 06:42:55 PM: 	# validation passes without improvement: 0
03/19 06:42:55 PM: Ran out of early stopping patience. Stopping training.
03/19 06:42:55 PM: multirc_loss: training: 0.731032 validation: 0.682991
03/19 06:42:55 PM: macro_avg: validation: 0.001574
03/19 06:42:55 PM: micro_avg: validation: 0.001574
03/19 06:42:55 PM: multirc_ans_f1: training: 0.488506 validation: 0.000000
03/19 06:42:55 PM: multirc_qst_f1: training: 0.215068 validation: 0.000000
03/19 06:42:55 PM: multirc_em: training: 0.520548 validation: 0.003148
03/19 06:42:55 PM: multirc_avg: training: 0.504527 validation: 0.001574
03/19 06:42:55 PM: Global learning rate: 0.000375
03/19 06:42:55 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2
03/19 06:42:56 PM: Stopped training after 7 validation checks
03/19 06:42:56 PM: Trained multirc for 350 steps or 0.103 epochs
03/19 06:42:56 PM: ***** VALIDATION RESULTS *****
03/19 06:42:56 PM: multirc_avg (for best val pass 1): multirc_loss: 0.77539, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/19 06:42:56 PM: micro_avg (for best val pass 1): multirc_loss: 0.77539, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/19 06:42:56 PM: macro_avg (for best val pass 1): multirc_loss: 0.77539, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/19 06:42:56 PM: Evaluating...
03/19 06:42:57 PM: Loaded model state from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2/model_state_pretrain_val_1.best.th
03/19 06:42:57 PM: Evaluating on: multirc, split: val
03/19 06:43:27 PM: 	Task multirc: batch 182
03/19 06:43:57 PM: 	Task multirc: batch 373
03/19 06:44:27 PM: 	Task multirc: batch 540
03/19 06:44:39 PM: Task 'multirc': sorting predictions by 'idx'
03/19 06:44:39 PM: Finished evaluating on: multirc
03/19 06:44:39 PM: Writing results for split 'val' to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/results.tsv
03/19 06:44:39 PM: micro_avg: 0.304, macro_avg: 0.304, multirc_ans_f1: 0.599, multirc_qst_f1: 0.591, multirc_em: 0.008, multirc_avg: 0.304
03/19 06:44:39 PM: Done!
03/19 06:58:02 PM: Git branch: develop
03/19 06:58:02 PM: Git SHA: dd631065bacec9ae53e4fc1832cfe49321974afb
03/19 06:58:02 PM: Parsed args: 
{
  "batch_size": 8,
  "classifier": "log_reg",
  "d_word": 50,
  "do_target_task_training": 0,
  "exp_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/",
  "exp_name": "mutlirc_bert_cased_exp",
  "input_module": "bert-base-cased",
  "load_model": 0,
  "local_log_path": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2/log.log",
  "lr": 0.003,
  "max_epochs": 5,
  "max_seq_len": 10,
  "max_vals": 10,
  "max_word_v_size": 1000,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "pretrain_tasks": "multirc",
  "random_seed": 402,
  "remote_log_name": "mutlirc_bert_cased_exp__mutlirc_bert_cased_run2",
  "run_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2",
  "run_name": "mutlirc_bert_cased_run2",
  "sent_enc": "bow",
  "skip_embs": 0,
  "target_tasks": "multirc",
  "target_train_max_vals": 10,
  "target_train_val_interval": 10,
  "transfer_paradigm": "finetune",
  "val_interval": 50
}
03/19 06:58:02 PM: Saved config to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2/params.conf
03/19 06:58:02 PM: Using random seed 402
03/19 06:58:02 PM: Loading tasks...
03/19 06:58:02 PM: Writing pre-preprocessed tasks to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/
03/19 06:58:02 PM: 	Loaded existing task multirc
03/19 06:58:02 PM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
03/19 06:58:02 PM: 	Finished loading tasks: multirc.
03/19 06:58:02 PM: Loading token dictionary from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab.
03/19 06:58:02 PM: 	Loaded vocab from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab
03/19 06:58:02 PM: 	Vocab namespace chars: size 100
03/19 06:58:02 PM: 	Vocab namespace bert_cased: size 28998
03/19 06:58:02 PM: 	Vocab namespace tokens: size 1004
03/19 06:58:02 PM: 	Finished building vocab.
03/19 06:58:02 PM: 	Task 'multirc', split 'train': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__train_data
03/19 06:58:02 PM: 	Task 'multirc', split 'val': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__val_data
03/19 06:58:02 PM: 	Task 'multirc', split 'test': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__test_data
03/19 06:58:02 PM: 	Finished indexing tasks
03/19 06:58:02 PM: 	Creating trimmed pretraining-only version of multirc train.
03/19 06:58:02 PM: 	Creating trimmed target-only version of multirc train.
03/19 06:58:02 PM: 	  Training on multirc
03/19 06:58:02 PM: 	  Evaluating on multirc
03/19 06:58:02 PM: 	Finished loading tasks in 0.082s
03/19 06:58:02 PM: 	 Tasks: ['multirc']
03/19 06:58:02 PM: Building model...
03/19 06:58:02 PM: Using BERT model (bert-base-cased).
03/19 06:58:02 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
03/19 06:58:02 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

03/19 06:58:02 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/19 06:58:04 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/19 06:58:04 PM: Initializing parameters
03/19 06:58:04 PM: Done initializing parameters; the following parameters are using their default initialization from their code
03/19 06:58:04 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
03/19 06:58:04 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
03/19 06:58:04 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
03/19 06:58:04 PM:    _text_field_embedder.model.pooler.dense.bias
03/19 06:58:04 PM:    _text_field_embedder.model.pooler.dense.weight
03/19 06:58:04 PM: 	Task 'multirc' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "max",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
03/19 06:58:04 PM: Model specification:
03/19 06:58:04 PM: MultiTaskModel(
  (sent_encoder): BoWSentEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
03/19 06:58:04 PM: Model parameters:
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 06:58:04 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 06:58:04 PM: 	multirc_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/19 06:58:04 PM: 	multirc_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
03/19 06:58:04 PM: Total number of parameters: 108311810 (1.08312e+08)
03/19 06:58:04 PM: Number of trainable parameters: 108311810 (1.08312e+08)
03/19 06:58:04 PM: Finished building model in 2.176s
03/19 06:58:04 PM: Will run the following steps for this experiment:
Training model on tasks: multirc 
Evaluating model on tasks: multirc 

03/19 06:58:04 PM: Training...
03/19 06:58:04 PM: patience = 5
03/19 06:58:04 PM: val_interval = 50
03/19 06:58:04 PM: max_vals = 10
03/19 06:58:04 PM: cuda_device = -1
03/19 06:58:04 PM: grad_norm = 5.0
03/19 06:58:04 PM: grad_clipping = None
03/19 06:58:04 PM: lr_decay = 0.99
03/19 06:58:04 PM: min_lr = 1e-06
03/19 06:58:04 PM: keep_all_checkpoints = 0
03/19 06:58:04 PM: val_data_limit = 5000
03/19 06:58:04 PM: max_epochs = 5
03/19 06:58:04 PM: dec_val_scale = 250
03/19 06:58:04 PM: training_data_fraction = 1
03/19 06:58:04 PM: accumulation_steps = 1
03/19 06:58:04 PM: type = bert_adam
03/19 06:58:04 PM: parameter_groups = None
03/19 06:58:04 PM: Number of trainable parameters: 108311810
03/19 06:58:04 PM: infer_type_and_cast = True
03/19 06:58:04 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 06:58:04 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 06:58:04 PM: lr = 0.003
03/19 06:58:04 PM: t_total = 500
03/19 06:58:04 PM: warmup = 0.1
03/19 06:58:04 PM: type = reduce_on_plateau
03/19 06:58:04 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 06:58:04 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 06:58:04 PM: mode = max
03/19 06:58:04 PM: factor = 0.5
03/19 06:58:04 PM: patience = 1
03/19 06:58:04 PM: threshold = 0.0001
03/19 06:58:04 PM: threshold_mode = abs
03/19 06:58:04 PM: verbose = True
03/19 06:58:04 PM: Starting training without restoring from a checkpoint.
03/19 06:58:04 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/Users/hpaila/Projects/NLP/multi_rc/baseline/jiant/__main__.py", line 588, in main
    phase="pretrain",
  File "/Users/hpaila/Projects/NLP/multi_rc/baseline/jiant/trainer.py", line 526, in train
    check_for_previous_checkpoints(self._serialization_dir, tasks, phase, load_model)
  File "/Users/hpaila/Projects/NLP/multi_rc/baseline/jiant/utils/utils.py", line 154, in check_for_previous_checkpoints
    % serialization_dir,
  File "/Users/hpaila/Projects/NLP/multi_rc/baseline/jiant/utils/utils.py", line 484, in assert_for_log
    assert condition, error_message
AssertionError: There are existing checkpoints in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run2 which will be overwritten. If you are restoring from a run, or would like to train from an existing checkpoint, Use load_model = 1 to load the checkpoints instead. If you don't want them, delete them or change your experiment name.
