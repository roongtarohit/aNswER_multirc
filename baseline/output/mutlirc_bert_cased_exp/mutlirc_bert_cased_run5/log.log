03/19 09:08:03 PM: Git branch: develop
03/19 09:08:03 PM: Git SHA: 23cd1ee178a5b6267ac9d6fd5cba8f1d5ae0c538
03/19 09:08:04 PM: Parsed args: 
{
  "batch_size": 8,
  "d_word": 50,
  "do_target_task_training": 0,
  "exp_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/",
  "exp_name": "mutlirc_bert_cased_exp",
  "input_module": "bert-base-cased",
  "load_model": 0,
  "local_log_path": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5/log.log",
  "lr": 0.05,
  "max_epochs": 10,
  "max_seq_len": 10,
  "max_vals": 20,
  "max_word_v_size": 1000,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "pretrain_tasks": "multirc",
  "random_seed": 22,
  "remote_log_name": "mutlirc_bert_cased_exp__mutlirc_bert_cased_run5",
  "run_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5",
  "run_name": "mutlirc_bert_cased_run5",
  "sent_enc": "bow",
  "skip_embs": 0,
  "target_tasks": "multirc",
  "target_train_max_vals": 10,
  "target_train_val_interval": 10,
  "transfer_paradigm": "finetune",
  "val_interval": 50
}
03/19 09:08:04 PM: Saved config to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5/params.conf
03/19 09:08:04 PM: Using random seed 22
03/19 09:08:04 PM: Loading tasks...
03/19 09:08:04 PM: Writing pre-preprocessed tasks to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/
03/19 09:08:04 PM: 	Loaded existing task multirc
03/19 09:08:04 PM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
03/19 09:08:04 PM: 	Finished loading tasks: multirc.
03/19 09:08:04 PM: Loading token dictionary from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab.
03/19 09:08:04 PM: 	Loaded vocab from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/vocab
03/19 09:08:04 PM: 	Vocab namespace chars: size 100
03/19 09:08:04 PM: 	Vocab namespace bert_cased: size 28998
03/19 09:08:04 PM: 	Vocab namespace tokens: size 1004
03/19 09:08:04 PM: 	Finished building vocab.
03/19 09:08:04 PM: 	Task 'multirc', split 'train': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__train_data
03/19 09:08:04 PM: 	Task 'multirc', split 'val': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__val_data
03/19 09:08:04 PM: 	Task 'multirc', split 'test': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/preproc/multirc__test_data
03/19 09:08:04 PM: 	Finished indexing tasks
03/19 09:08:04 PM: 	Creating trimmed pretraining-only version of multirc train.
03/19 09:08:04 PM: 	Creating trimmed target-only version of multirc train.
03/19 09:08:04 PM: 	  Training on multirc
03/19 09:08:04 PM: 	  Evaluating on multirc
03/19 09:08:04 PM: 	Finished loading tasks in 0.080s
03/19 09:08:04 PM: 	 Tasks: ['multirc']
03/19 09:08:04 PM: Building model...
03/19 09:08:04 PM: Using BERT model (bert-base-cased).
03/19 09:08:04 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
03/19 09:08:04 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

03/19 09:08:04 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/19 09:08:06 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/19 09:08:06 PM: Initializing parameters
03/19 09:08:06 PM: Done initializing parameters; the following parameters are using their default initialization from their code
03/19 09:08:06 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
03/19 09:08:06 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
03/19 09:08:06 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
03/19 09:08:06 PM:    _text_field_embedder.model.pooler.dense.bias
03/19 09:08:06 PM:    _text_field_embedder.model.pooler.dense.weight
03/19 09:08:06 PM: 	Task 'multirc' params: {
  "cls_type": "mlp",
  "d_hid": 512,
  "pool_type": "max",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
03/19 09:08:06 PM: Model specification:
03/19 09:08:06 PM: MultiTaskModel(
  (sent_encoder): BoWSentEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=768, out_features=512, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.2)
        (4): Linear(in_features=512, out_features=2, bias=True)
      )
    )
  )
)
03/19 09:08:06 PM: Model parameters:
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/19 09:08:06 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/19 09:08:06 PM: 	multirc_mdl.classifier.classifier.0.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/19 09:08:06 PM: 	multirc_mdl.classifier.classifier.0.bias: Trainable parameter, count 512 with torch.Size([512])
03/19 09:08:06 PM: 	multirc_mdl.classifier.classifier.2.weight: Trainable parameter, count 512 with torch.Size([512])
03/19 09:08:06 PM: 	multirc_mdl.classifier.classifier.2.bias: Trainable parameter, count 512 with torch.Size([512])
03/19 09:08:06 PM: 	multirc_mdl.classifier.classifier.4.weight: Trainable parameter, count 1024 with torch.Size([2, 512])
03/19 09:08:06 PM: 	multirc_mdl.classifier.classifier.4.bias: Trainable parameter, count 2 with torch.Size([2])
03/19 09:08:06 PM: Total number of parameters: 108706050 (1.08706e+08)
03/19 09:08:06 PM: Number of trainable parameters: 108706050 (1.08706e+08)
03/19 09:08:06 PM: Finished building model in 2.327s
03/19 09:08:06 PM: Will run the following steps for this experiment:
Training model on tasks: multirc 
Evaluating model on tasks: multirc 

03/19 09:08:06 PM: Training...
03/19 09:08:06 PM: patience = 5
03/19 09:08:06 PM: val_interval = 50
03/19 09:08:06 PM: max_vals = 20
03/19 09:08:06 PM: cuda_device = -1
03/19 09:08:06 PM: grad_norm = 5.0
03/19 09:08:06 PM: grad_clipping = None
03/19 09:08:06 PM: lr_decay = 0.99
03/19 09:08:06 PM: min_lr = 1e-06
03/19 09:08:06 PM: keep_all_checkpoints = 0
03/19 09:08:06 PM: val_data_limit = 5000
03/19 09:08:06 PM: max_epochs = 10
03/19 09:08:06 PM: dec_val_scale = 250
03/19 09:08:06 PM: training_data_fraction = 1
03/19 09:08:06 PM: accumulation_steps = 1
03/19 09:08:06 PM: type = bert_adam
03/19 09:08:06 PM: parameter_groups = None
03/19 09:08:06 PM: Number of trainable parameters: 108706050
03/19 09:08:06 PM: infer_type_and_cast = True
03/19 09:08:06 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 09:08:06 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 09:08:06 PM: lr = 0.05
03/19 09:08:06 PM: t_total = 1000
03/19 09:08:06 PM: warmup = 0.1
03/19 09:08:06 PM: type = reduce_on_plateau
03/19 09:08:06 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/19 09:08:06 PM: CURRENTLY DEFINED PARAMETERS: 
03/19 09:08:06 PM: mode = max
03/19 09:08:06 PM: factor = 0.5
03/19 09:08:06 PM: patience = 1
03/19 09:08:06 PM: threshold = 0.0001
03/19 09:08:06 PM: threshold_mode = abs
03/19 09:08:06 PM: verbose = True
03/19 09:08:06 PM: Starting training without restoring from a checkpoint.
03/19 09:08:06 PM: Training examples per task, before any subsampling: {'multirc': 27243}
03/19 09:08:06 PM: Beginning training with stopping criteria based on metric: multirc_avg
03/19 09:08:18 PM: Update 4: task multirc, steps since last val 4 (total steps = 4): ans_f1: 0.6923, qst_f1: 0.2581, em: 0.7419, avg: 0.7171, multirc_loss: 0.5573
03/19 09:08:30 PM: Update 9: task multirc, steps since last val 9 (total steps = 9): ans_f1: 0.5075, qst_f1: 0.2254, em: 0.5352, avg: 0.5213, multirc_loss: 2.2211
03/19 09:08:40 PM: Update 13: task multirc, steps since last val 13 (total steps = 13): ans_f1: 0.4667, qst_f1: 0.1961, em: 0.5294, avg: 0.4980, multirc_loss: 2.7397
03/19 09:08:51 PM: Update 17: task multirc, steps since last val 17 (total steps = 17): ans_f1: 0.4769, qst_f1: 0.2239, em: 0.4925, avg: 0.4847, multirc_loss: 3.2951
03/19 09:09:03 PM: Update 21: task multirc, steps since last val 21 (total steps = 21): ans_f1: 0.4459, qst_f1: 0.1955, em: 0.5000, avg: 0.4730, multirc_loss: 3.4383
03/19 09:09:16 PM: Update 25: task multirc, steps since last val 25 (total steps = 25): ans_f1: 0.4278, qst_f1: 0.2003, em: 0.4560, avg: 0.4419, multirc_loss: 4.0697
03/19 09:09:28 PM: Update 28: task multirc, steps since last val 28 (total steps = 28): ans_f1: 0.4000, qst_f1: 0.1798, em: 0.4605, avg: 0.4302, multirc_loss: 4.2258
03/19 09:09:40 PM: Update 31: task multirc, steps since last val 31 (total steps = 31): ans_f1: 0.4603, qst_f1: 0.2255, em: 0.4790, avg: 0.4696, multirc_loss: 4.1136
03/19 09:09:51 PM: Update 34: task multirc, steps since last val 34 (total steps = 34): ans_f1: 0.4531, qst_f1: 0.2196, em: 0.4767, avg: 0.4649, multirc_loss: 4.0698
03/19 09:10:01 PM: Update 37: task multirc, steps since last val 37 (total steps = 37): ans_f1: 0.4348, qst_f1: 0.2091, em: 0.4588, avg: 0.4468, multirc_loss: 4.0907
03/19 09:10:11 PM: Update 40: task multirc, steps since last val 40 (total steps = 40): ans_f1: 0.4665, qst_f1: 0.2367, em: 0.4633, avg: 0.4649, multirc_loss: 4.0297
03/19 09:10:22 PM: Update 43: task multirc, steps since last val 43 (total steps = 43): ans_f1: 0.4520, qst_f1: 0.2219, em: 0.4688, avg: 0.4604, multirc_loss: 3.9560
03/19 09:10:33 PM: Update 46: task multirc, steps since last val 46 (total steps = 46): ans_f1: 0.4732, qst_f1: 0.2380, em: 0.4751, avg: 0.4742, multirc_loss: 3.8568
03/19 09:10:44 PM: Update 49: task multirc, steps since last val 49 (total steps = 49): ans_f1: 0.4817, qst_f1: 0.2468, em: 0.4750, avg: 0.4783, multirc_loss: 3.7274
03/19 09:10:47 PM: ***** Step 50 / Validation 1 *****
03/19 09:10:47 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 09:10:47 PM: Validating...
03/19 09:10:54 PM: Evaluate: task multirc, batch 40 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 1.1902
03/19 09:11:04 PM: Evaluate: task multirc, batch 99 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 1.1022
03/19 09:11:14 PM: Evaluate: task multirc, batch 156 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 1.0425
03/19 09:11:24 PM: Evaluate: task multirc, batch 212 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0061, avg: 0.0030, multirc_loss: 1.0215
03/19 09:11:35 PM: Evaluate: task multirc, batch 270 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0047, avg: 0.0023, multirc_loss: 1.0186
03/19 09:11:45 PM: Evaluate: task multirc, batch 332 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0057, avg: 0.0029, multirc_loss: 1.0122
03/19 09:11:55 PM: Evaluate: task multirc, batch 392 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0049, avg: 0.0024, multirc_loss: 1.0241
03/19 09:12:06 PM: Evaluate: task multirc, batch 450 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0041, avg: 0.0021, multirc_loss: 1.0470
03/19 09:12:16 PM: Evaluate: task multirc, batch 511 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0018, multirc_loss: 1.0328
03/19 09:12:27 PM: Evaluate: task multirc, batch 575 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0044, avg: 0.0022, multirc_loss: 1.0254
03/19 09:12:32 PM: Best result seen so far for multirc.
03/19 09:12:32 PM: Best result seen so far for micro.
03/19 09:12:32 PM: Best result seen so far for macro.
03/19 09:12:32 PM: Updating LR scheduler:
03/19 09:12:32 PM: 	Best result seen so far for macro_avg: 0.002
03/19 09:12:32 PM: 	# validation passes without improvement: 0
03/19 09:12:32 PM: multirc_loss: training: 3.664709 validation: 1.029023
03/19 09:12:32 PM: macro_avg: validation: 0.001574
03/19 09:12:32 PM: micro_avg: validation: 0.001574
03/19 09:12:32 PM: multirc_ans_f1: training: 0.480418 validation: 0.000000
03/19 09:12:32 PM: multirc_qst_f1: training: 0.243379 validation: 0.000000
03/19 09:12:32 PM: multirc_em: training: 0.479452 validation: 0.003148
03/19 09:12:32 PM: multirc_avg: training: 0.479935 validation: 0.001574
03/19 09:12:32 PM: Global learning rate: 0.05
03/19 09:12:32 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5
03/19 09:12:40 PM: Update 52: task multirc, steps since last val 2 (total steps = 52): ans_f1: 0.5882, qst_f1: 0.3125, em: 0.5625, avg: 0.5754, multirc_loss: 1.7161
03/19 09:12:50 PM: Update 56: task multirc, steps since last val 6 (total steps = 56): ans_f1: 0.5200, qst_f1: 0.2695, em: 0.4894, avg: 0.5047, multirc_loss: 2.4145
03/19 09:13:03 PM: Update 60: task multirc, steps since last val 10 (total steps = 60): ans_f1: 0.5556, qst_f1: 0.3162, em: 0.5000, avg: 0.5278, multirc_loss: 2.1727
03/19 09:13:16 PM: Update 64: task multirc, steps since last val 14 (total steps = 64): ans_f1: 0.5669, qst_f1: 0.3272, em: 0.5046, avg: 0.5358, multirc_loss: 1.8928
03/19 09:13:28 PM: Update 68: task multirc, steps since last val 18 (total steps = 68): ans_f1: 0.5033, qst_f1: 0.2729, em: 0.4638, avg: 0.4835, multirc_loss: 1.9663
03/19 09:13:40 PM: Update 72: task multirc, steps since last val 22 (total steps = 72): ans_f1: 0.4894, qst_f1: 0.2667, em: 0.4364, avg: 0.4629, multirc_loss: 2.1143
03/19 09:13:52 PM: Update 76: task multirc, steps since last val 26 (total steps = 76): ans_f1: 0.4796, qst_f1: 0.2615, em: 0.4359, avg: 0.4578, multirc_loss: 2.3504
03/19 09:14:03 PM: Update 80: task multirc, steps since last val 30 (total steps = 80): ans_f1: 0.4816, qst_f1: 0.2553, em: 0.4595, avg: 0.4705, multirc_loss: 2.1916
03/19 09:14:16 PM: Update 84: task multirc, steps since last val 34 (total steps = 84): ans_f1: 0.4800, qst_f1: 0.2526, em: 0.4643, avg: 0.4721, multirc_loss: 2.0395
03/19 09:14:28 PM: Update 88: task multirc, steps since last val 38 (total steps = 88): ans_f1: 0.4656, qst_f1: 0.2422, em: 0.4424, avg: 0.4540, multirc_loss: 2.0737
03/19 09:14:40 PM: Update 92: task multirc, steps since last val 42 (total steps = 92): ans_f1: 0.4688, qst_f1: 0.2459, em: 0.4426, avg: 0.4557, multirc_loss: 2.2347
03/19 09:14:52 PM: Update 96: task multirc, steps since last val 46 (total steps = 96): ans_f1: 0.4586, qst_f1: 0.2377, em: 0.4411, avg: 0.4498, multirc_loss: 2.2612
03/19 09:15:04 PM: Update 100: task multirc, steps since last val 50 (total steps = 100): ans_f1: 0.4573, qst_f1: 0.2413, em: 0.4310, avg: 0.4441, multirc_loss: 2.3108
03/19 09:15:04 PM: ***** Step 100 / Validation 2 *****
03/19 09:15:04 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 09:15:04 PM: Validating...
03/19 09:15:14 PM: Evaluate: task multirc, batch 59 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0096, avg: 0.0048, multirc_loss: 3.5805
03/19 09:15:24 PM: Evaluate: task multirc, batch 121 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0053, avg: 0.0027, multirc_loss: 3.1854
03/19 09:15:34 PM: Evaluate: task multirc, batch 181 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 3.0252
03/19 09:15:44 PM: Evaluate: task multirc, batch 238 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0054, avg: 0.0027, multirc_loss: 2.8949
03/19 09:15:55 PM: Evaluate: task multirc, batch 296 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0065, avg: 0.0032, multirc_loss: 2.8986
03/19 09:16:05 PM: Evaluate: task multirc, batch 356 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 2.9187
03/19 09:16:15 PM: Evaluate: task multirc, batch 416 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0030, avg: 0.0015, multirc_loss: 2.9807
03/19 09:16:26 PM: Evaluate: task multirc, batch 475 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0020, multirc_loss: 2.9863
03/19 09:16:36 PM: Evaluate: task multirc, batch 533 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0035, avg: 0.0018, multirc_loss: 2.9671
03/19 09:16:47 PM: Evaluate: task multirc, batch 589 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 2.9353
03/19 09:16:51 PM: Updating LR scheduler:
03/19 09:16:51 PM: 	Best result seen so far for macro_avg: 0.002
03/19 09:16:51 PM: 	# validation passes without improvement: 1
03/19 09:16:51 PM: multirc_loss: training: 2.310790 validation: 2.952584
03/19 09:16:51 PM: macro_avg: validation: 0.001574
03/19 09:16:51 PM: micro_avg: validation: 0.001574
03/19 09:16:51 PM: multirc_ans_f1: training: 0.457286 validation: 0.000000
03/19 09:16:51 PM: multirc_qst_f1: training: 0.241315 validation: 0.000000
03/19 09:16:51 PM: multirc_em: training: 0.430986 validation: 0.003148
03/19 09:16:51 PM: multirc_avg: training: 0.444136 validation: 0.001574
03/19 09:16:51 PM: Global learning rate: 0.05
03/19 09:16:51 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5
03/19 09:16:58 PM: Update 102: task multirc, steps since last val 2 (total steps = 102): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.4375, avg: 0.2188, multirc_loss: 3.8079
03/19 09:17:10 PM: Update 106: task multirc, steps since last val 6 (total steps = 106): ans_f1: 0.3636, qst_f1: 0.1667, em: 0.4167, avg: 0.3902, multirc_loss: 3.5281
03/19 09:17:22 PM: Update 111: task multirc, steps since last val 11 (total steps = 111): ans_f1: 0.4384, qst_f1: 0.1839, em: 0.5287, avg: 0.4835, multirc_loss: 2.5248
03/19 09:17:35 PM: Update 115: task multirc, steps since last val 15 (total steps = 115): ans_f1: 0.4118, qst_f1: 0.1810, em: 0.4914, avg: 0.4516, multirc_loss: 2.7348
03/19 09:17:45 PM: Update 118: task multirc, steps since last val 18 (total steps = 118): ans_f1: 0.4806, qst_f1: 0.2214, em: 0.5286, avg: 0.5046, multirc_loss: 2.6834
03/19 09:17:58 PM: Update 122: task multirc, steps since last val 22 (total steps = 122): ans_f1: 0.4490, qst_f1: 0.1941, em: 0.5353, avg: 0.4921, multirc_loss: 2.6771
03/19 09:18:11 PM: Update 126: task multirc, steps since last val 26 (total steps = 126): ans_f1: 0.4624, qst_f1: 0.1983, em: 0.5450, avg: 0.5037, multirc_loss: 2.7403
03/19 09:18:21 PM: Update 129: task multirc, steps since last val 29 (total steps = 129): ans_f1: 0.4348, qst_f1: 0.1803, em: 0.5455, avg: 0.4901, multirc_loss: 2.7920
03/19 09:18:31 PM: Update 132: task multirc, steps since last val 32 (total steps = 132): ans_f1: 0.4465, qst_f1: 0.1962, em: 0.5267, avg: 0.4866, multirc_loss: 2.8354
03/19 09:18:45 PM: Update 136: task multirc, steps since last val 36 (total steps = 136): ans_f1: 0.4211, qst_f1: 0.1759, em: 0.5351, avg: 0.4781, multirc_loss: 3.1119
03/19 09:18:58 PM: Update 140: task multirc, steps since last val 40 (total steps = 140): ans_f1: 0.4528, qst_f1: 0.1967, em: 0.5367, avg: 0.4947, multirc_loss: 2.9341
03/19 09:19:11 PM: Update 144: task multirc, steps since last val 44 (total steps = 144): ans_f1: 0.4610, qst_f1: 0.2036, em: 0.5380, avg: 0.4995, multirc_loss: 2.8061
03/19 09:19:23 PM: Update 148: task multirc, steps since last val 48 (total steps = 148): ans_f1: 0.4611, qst_f1: 0.2056, em: 0.5324, avg: 0.4967, multirc_loss: 2.6556
03/19 09:19:29 PM: ***** Step 150 / Validation 3 *****
03/19 09:19:29 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 09:19:29 PM: Validating...
03/19 09:19:33 PM: Evaluate: task multirc, batch 23 (606): ans_f1: 0.6569, qst_f1: 0.6343, em: 0.0233, avg: 0.3401, multirc_loss: 0.8981
03/19 09:19:43 PM: Evaluate: task multirc, batch 79 (606): ans_f1: 0.6496, qst_f1: 0.6482, em: 0.0150, avg: 0.3323, multirc_loss: 0.9085
03/19 09:19:53 PM: Evaluate: task multirc, batch 132 (606): ans_f1: 0.6232, qst_f1: 0.6166, em: 0.0097, avg: 0.3165, multirc_loss: 0.9447
03/19 09:20:03 PM: Evaluate: task multirc, batch 188 (606): ans_f1: 0.6100, qst_f1: 0.5972, em: 0.0035, avg: 0.3067, multirc_loss: 0.9623
03/19 09:20:14 PM: Evaluate: task multirc, batch 236 (606): ans_f1: 0.5921, qst_f1: 0.5776, em: 0.0027, avg: 0.2974, multirc_loss: 0.9857
03/19 09:20:24 PM: Evaluate: task multirc, batch 288 (606): ans_f1: 0.5938, qst_f1: 0.5811, em: 0.0022, avg: 0.2980, multirc_loss: 0.9834
03/19 09:20:35 PM: Evaluate: task multirc, batch 342 (606): ans_f1: 0.5901, qst_f1: 0.5774, em: 0.0019, avg: 0.2960, multirc_loss: 0.9883
03/19 09:20:45 PM: Evaluate: task multirc, batch 397 (606): ans_f1: 0.5981, qst_f1: 0.5855, em: 0.0032, avg: 0.3007, multirc_loss: 0.9779
03/19 09:20:56 PM: Evaluate: task multirc, batch 453 (606): ans_f1: 0.6070, qst_f1: 0.5972, em: 0.0110, avg: 0.3090, multirc_loss: 0.9663
03/19 09:21:06 PM: Evaluate: task multirc, batch 507 (606): ans_f1: 0.6016, qst_f1: 0.5923, em: 0.0099, avg: 0.3058, multirc_loss: 0.9733
03/19 09:21:17 PM: Evaluate: task multirc, batch 557 (606): ans_f1: 0.6009, qst_f1: 0.5922, em: 0.0091, avg: 0.3050, multirc_loss: 0.9742
03/19 09:21:26 PM: Best result seen so far for multirc.
03/19 09:21:26 PM: Best result seen so far for micro.
03/19 09:21:26 PM: Best result seen so far for macro.
03/19 09:21:26 PM: Updating LR scheduler:
03/19 09:21:26 PM: 	Best result seen so far for macro_avg: 0.304
03/19 09:21:26 PM: 	# validation passes without improvement: 0
03/19 09:21:26 PM: multirc_loss: training: 2.583156 validation: 0.976148
03/19 09:21:26 PM: macro_avg: validation: 0.303923
03/19 09:21:26 PM: micro_avg: validation: 0.303923
03/19 09:21:26 PM: multirc_ans_f1: training: 0.449848 validation: 0.599451
03/19 09:21:26 PM: multirc_qst_f1: training: 0.196929 validation: 0.590961
03/19 09:21:26 PM: multirc_em: training: 0.528455 validation: 0.008395
03/19 09:21:26 PM: multirc_avg: training: 0.489152 validation: 0.303923
03/19 09:21:26 PM: Global learning rate: 0.05
03/19 09:21:26 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5
03/19 09:21:32 PM: Update 151: task multirc, steps since last val 1 (total steps = 151): ans_f1: 0.9231, qst_f1: 0.7500, em: 0.8750, avg: 0.8990, multirc_loss: 0.3274
03/19 09:21:42 PM: Update 154: task multirc, steps since last val 4 (total steps = 154): ans_f1: 0.7347, qst_f1: 0.5625, em: 0.5938, avg: 0.6642, multirc_loss: 2.3372
03/19 09:21:52 PM: Update 157: task multirc, steps since last val 7 (total steps = 157): ans_f1: 0.6000, qst_f1: 0.3214, em: 0.5714, avg: 0.5857, multirc_loss: 2.5281
03/19 09:22:04 PM: Update 160: task multirc, steps since last val 10 (total steps = 160): ans_f1: 0.6458, qst_f1: 0.3932, em: 0.5641, avg: 0.6050, multirc_loss: 2.1908
03/19 09:22:14 PM: Update 163: task multirc, steps since last val 13 (total steps = 163): ans_f1: 0.6316, qst_f1: 0.3531, em: 0.5842, avg: 0.6079, multirc_loss: 1.8304
03/19 09:22:25 PM: Update 166: task multirc, steps since last val 16 (total steps = 166): ans_f1: 0.5957, qst_f1: 0.3333, em: 0.5440, avg: 0.5699, multirc_loss: 1.7738
03/19 09:22:35 PM: Update 169: task multirc, steps since last val 19 (total steps = 169): ans_f1: 0.5733, qst_f1: 0.2880, em: 0.5646, avg: 0.5690, multirc_loss: 1.7440
03/19 09:22:48 PM: Update 173: task multirc, steps since last val 23 (total steps = 173): ans_f1: 0.5318, qst_f1: 0.2491, em: 0.5449, avg: 0.5384, multirc_loss: 1.7415
03/19 09:22:59 PM: Update 176: task multirc, steps since last val 26 (total steps = 176): ans_f1: 0.5054, qst_f1: 0.2233, em: 0.5400, avg: 0.5227, multirc_loss: 1.7311
03/19 09:23:11 PM: Update 179: task multirc, steps since last val 29 (total steps = 179): ans_f1: 0.4976, qst_f1: 0.2182, em: 0.5381, avg: 0.5178, multirc_loss: 1.6520
03/19 09:23:22 PM: Update 182: task multirc, steps since last val 32 (total steps = 182): ans_f1: 0.4909, qst_f1: 0.2144, em: 0.5353, avg: 0.5131, multirc_loss: 1.5906
03/19 09:23:35 PM: Update 185: task multirc, steps since last val 35 (total steps = 185): ans_f1: 0.4980, qst_f1: 0.2188, em: 0.5367, avg: 0.5173, multirc_loss: 1.5641
03/19 09:23:45 PM: Update 188: task multirc, steps since last val 38 (total steps = 188): ans_f1: 0.5075, qst_f1: 0.2258, em: 0.5426, avg: 0.5250, multirc_loss: 1.5194
03/19 09:23:56 PM: Update 191: task multirc, steps since last val 41 (total steps = 191): ans_f1: 0.4928, qst_f1: 0.2101, em: 0.5512, avg: 0.5220, multirc_loss: 1.6030
03/19 09:24:07 PM: Update 194: task multirc, steps since last val 44 (total steps = 194): ans_f1: 0.4901, qst_f1: 0.2153, em: 0.5373, avg: 0.5137, multirc_loss: 1.7201
03/19 09:24:18 PM: Update 197: task multirc, steps since last val 47 (total steps = 197): ans_f1: 0.4768, qst_f1: 0.2096, em: 0.5263, avg: 0.5015, multirc_loss: 1.8134
03/19 09:24:29 PM: Update 200: task multirc, steps since last val 50 (total steps = 200): ans_f1: 0.4624, qst_f1: 0.2063, em: 0.5083, avg: 0.4854, multirc_loss: 1.8422
03/19 09:24:29 PM: ***** Step 200 / Validation 4 *****
03/19 09:24:29 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 09:24:29 PM: Validating...
03/19 09:24:39 PM: Evaluate: task multirc, batch 53 (606): ans_f1: 0.6791, qst_f1: 0.6719, em: 0.0105, avg: 0.3448, multirc_loss: 1.3153
03/19 09:24:49 PM: Evaluate: task multirc, batch 105 (606): ans_f1: 0.6286, qst_f1: 0.6280, em: 0.0123, avg: 0.3204, multirc_loss: 1.4578
03/19 09:24:59 PM: Evaluate: task multirc, batch 156 (606): ans_f1: 0.6056, qst_f1: 0.5964, em: 0.0042, avg: 0.3049, multirc_loss: 1.5192
03/19 09:25:10 PM: Evaluate: task multirc, batch 201 (606): ans_f1: 0.6048, qst_f1: 0.5887, em: 0.0032, avg: 0.3040, multirc_loss: 1.5214
03/19 09:25:20 PM: Evaluate: task multirc, batch 249 (606): ans_f1: 0.5882, qst_f1: 0.5740, em: 0.0026, avg: 0.2954, multirc_loss: 1.5642
03/19 09:25:30 PM: Evaluate: task multirc, batch 299 (606): ans_f1: 0.5917, qst_f1: 0.5789, em: 0.0021, avg: 0.2969, multirc_loss: 1.5553
03/19 09:25:41 PM: Evaluate: task multirc, batch 350 (606): ans_f1: 0.5905, qst_f1: 0.5787, em: 0.0037, avg: 0.2971, multirc_loss: 1.5584
03/19 09:25:51 PM: Evaluate: task multirc, batch 400 (606): ans_f1: 0.5983, qst_f1: 0.5861, em: 0.0032, avg: 0.3008, multirc_loss: 1.5382
03/19 09:26:02 PM: Evaluate: task multirc, batch 454 (606): ans_f1: 0.6068, qst_f1: 0.5971, em: 0.0110, avg: 0.3089, multirc_loss: 1.5160
03/19 09:26:12 PM: Evaluate: task multirc, batch 507 (606): ans_f1: 0.6016, qst_f1: 0.5923, em: 0.0099, avg: 0.3058, multirc_loss: 1.5296
03/19 09:26:23 PM: Evaluate: task multirc, batch 560 (606): ans_f1: 0.6009, qst_f1: 0.5921, em: 0.0090, avg: 0.3050, multirc_loss: 1.5315
03/19 09:26:32 PM: Updating LR scheduler:
03/19 09:26:32 PM: 	Best result seen so far for macro_avg: 0.304
03/19 09:26:32 PM: 	# validation passes without improvement: 1
03/19 09:26:32 PM: multirc_loss: training: 1.842229 validation: 1.535255
03/19 09:26:32 PM: macro_avg: validation: 0.303923
03/19 09:26:32 PM: micro_avg: validation: 0.303923
03/19 09:26:32 PM: multirc_ans_f1: training: 0.462428 validation: 0.599451
03/19 09:26:32 PM: multirc_qst_f1: training: 0.206262 validation: 0.590961
03/19 09:26:32 PM: multirc_em: training: 0.508287 validation: 0.008395
03/19 09:26:32 PM: multirc_avg: training: 0.485358 validation: 0.303923
03/19 09:26:32 PM: Global learning rate: 0.05
03/19 09:26:32 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5
03/19 09:26:37 PM: Update 201: task multirc, steps since last val 1 (total steps = 201): ans_f1: 0.6667, qst_f1: 0.5000, em: 0.5000, avg: 0.5833, multirc_loss: 1.3262
03/19 09:26:48 PM: Update 204: task multirc, steps since last val 4 (total steps = 204): ans_f1: 0.4348, qst_f1: 0.1613, em: 0.5806, avg: 0.5077, multirc_loss: 0.8382
03/19 09:27:01 PM: Update 208: task multirc, steps since last val 8 (total steps = 208): ans_f1: 0.4561, qst_f1: 0.2131, em: 0.5246, avg: 0.4904, multirc_loss: 1.1943
03/19 09:27:12 PM: Update 211: task multirc, steps since last val 11 (total steps = 211): ans_f1: 0.5169, qst_f1: 0.2706, em: 0.5176, avg: 0.5173, multirc_loss: 1.3757
03/19 09:27:22 PM: Update 214: task multirc, steps since last val 14 (total steps = 214): ans_f1: 0.4854, qst_f1: 0.2336, em: 0.5327, avg: 0.5091, multirc_loss: 1.2560
03/19 09:27:33 PM: Update 217: task multirc, steps since last val 17 (total steps = 217): ans_f1: 0.4960, qst_f1: 0.2396, em: 0.5312, avg: 0.5136, multirc_loss: 1.1616
03/19 09:27:47 PM: Update 221: task multirc, steps since last val 21 (total steps = 221): ans_f1: 0.4969, qst_f1: 0.2521, em: 0.5064, avg: 0.5017, multirc_loss: 1.2942
03/19 09:27:57 PM: Update 224: task multirc, steps since last val 24 (total steps = 224): ans_f1: 0.4706, qst_f1: 0.2197, em: 0.5196, avg: 0.4951, multirc_loss: 1.3251
03/19 09:28:09 PM: Update 227: task multirc, steps since last val 27 (total steps = 227): ans_f1: 0.5121, qst_f1: 0.2609, em: 0.5202, avg: 0.5161, multirc_loss: 1.3777
03/19 09:28:19 PM: Update 230: task multirc, steps since last val 30 (total steps = 230): ans_f1: 0.5259, qst_f1: 0.2737, em: 0.5321, avg: 0.5290, multirc_loss: 1.3488
03/19 09:28:30 PM: Update 233: task multirc, steps since last val 33 (total steps = 233): ans_f1: 0.5000, qst_f1: 0.2479, em: 0.5292, avg: 0.5146, multirc_loss: 1.4925
03/19 09:28:41 PM: Update 236: task multirc, steps since last val 36 (total steps = 236): ans_f1: 0.5108, qst_f1: 0.2630, em: 0.5171, avg: 0.5140, multirc_loss: 1.5793
03/19 09:28:52 PM: Update 239: task multirc, steps since last val 39 (total steps = 239): ans_f1: 0.4898, qst_f1: 0.2471, em: 0.5070, avg: 0.4984, multirc_loss: 1.7082
03/19 09:29:04 PM: Update 242: task multirc, steps since last val 42 (total steps = 242): ans_f1: 0.4752, qst_f1: 0.2331, em: 0.5083, avg: 0.4918, multirc_loss: 1.7506
03/19 09:29:15 PM: Update 245: task multirc, steps since last val 45 (total steps = 245): ans_f1: 0.5000, qst_f1: 0.2572, em: 0.5093, avg: 0.5047, multirc_loss: 1.7632
03/19 09:29:26 PM: Update 248: task multirc, steps since last val 48 (total steps = 248): ans_f1: 0.4871, qst_f1: 0.2426, em: 0.5088, avg: 0.4980, multirc_loss: 1.7151
03/19 09:29:33 PM: ***** Step 250 / Validation 5 *****
03/19 09:29:33 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 09:29:33 PM: Validating...
03/19 09:29:36 PM: Evaluate: task multirc, batch 16 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0345, avg: 0.0172, multirc_loss: 0.9354
03/19 09:29:47 PM: Evaluate: task multirc, batch 67 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0087, avg: 0.0043, multirc_loss: 0.9381
03/19 09:29:57 PM: Evaluate: task multirc, batch 113 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.8777
03/19 09:30:07 PM: Evaluate: task multirc, batch 159 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.8420
03/19 09:30:17 PM: Evaluate: task multirc, batch 207 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0031, avg: 0.0016, multirc_loss: 0.8313
03/19 09:30:28 PM: Evaluate: task multirc, batch 257 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0049, avg: 0.0025, multirc_loss: 0.8205
03/19 09:30:38 PM: Evaluate: task multirc, batch 306 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0042, avg: 0.0021, multirc_loss: 0.8188
03/19 09:30:48 PM: Evaluate: task multirc, batch 356 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 0.8240
03/19 09:30:59 PM: Evaluate: task multirc, batch 405 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0047, avg: 0.0023, multirc_loss: 0.8343
03/19 09:31:09 PM: Evaluate: task multirc, batch 455 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0027, avg: 0.0014, multirc_loss: 0.8418
03/19 09:31:20 PM: Evaluate: task multirc, batch 491 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0038, avg: 0.0019, multirc_loss: 0.8365
03/19 09:31:31 PM: Evaluate: task multirc, batch 536 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0035, avg: 0.0018, multirc_loss: 0.8350
03/19 09:31:41 PM: Evaluate: task multirc, batch 575 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0044, avg: 0.0022, multirc_loss: 0.8286
03/19 09:31:50 PM: Updating LR scheduler:
03/19 09:31:50 PM: 	Best result seen so far for macro_avg: 0.304
03/19 09:31:50 PM: 	# validation passes without improvement: 0
03/19 09:31:50 PM: multirc_loss: training: 1.679239 validation: 0.831079
03/19 09:31:50 PM: macro_avg: validation: 0.001574
03/19 09:31:50 PM: micro_avg: validation: 0.001574
03/19 09:31:50 PM: multirc_ans_f1: training: 0.482192 validation: 0.000000
03/19 09:31:50 PM: multirc_qst_f1: training: 0.241216 validation: 0.000000
03/19 09:31:50 PM: multirc_em: training: 0.501425 validation: 0.003148
03/19 09:31:50 PM: multirc_avg: training: 0.491808 validation: 0.001574
03/19 09:31:50 PM: Global learning rate: 0.025
03/19 09:31:50 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5
03/19 09:31:55 PM: Update 251: task multirc, steps since last val 1 (total steps = 251): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.6250, avg: 0.3125, multirc_loss: 0.6906
03/19 09:32:07 PM: Update 254: task multirc, steps since last val 4 (total steps = 254): ans_f1: 0.2222, qst_f1: 0.0625, em: 0.5625, avg: 0.3924, multirc_loss: 0.8388
03/19 09:32:21 PM: Update 258: task multirc, steps since last val 8 (total steps = 258): ans_f1: 0.6364, qst_f1: 0.3111, em: 0.6000, avg: 0.6182, multirc_loss: 0.7934
03/19 09:32:32 PM: Update 261: task multirc, steps since last val 11 (total steps = 261): ans_f1: 0.5556, qst_f1: 0.2731, em: 0.5181, avg: 0.5368, multirc_loss: 0.8810
03/19 09:32:43 PM: Update 264: task multirc, steps since last val 14 (total steps = 264): ans_f1: 0.5739, qst_f1: 0.2862, em: 0.5377, avg: 0.5558, multirc_loss: 0.8846
03/19 09:32:55 PM: Update 267: task multirc, steps since last val 17 (total steps = 267): ans_f1: 0.5493, qst_f1: 0.2791, em: 0.5039, avg: 0.5266, multirc_loss: 0.8971
03/19 09:33:07 PM: Update 270: task multirc, steps since last val 20 (total steps = 270): ans_f1: 0.5409, qst_f1: 0.2649, em: 0.5166, avg: 0.5287, multirc_loss: 0.8500
03/19 09:33:17 PM: Update 273: task multirc, steps since last val 23 (total steps = 273): ans_f1: 0.5397, qst_f1: 0.2765, em: 0.4882, avg: 0.5140, multirc_loss: 0.8637
03/19 09:33:28 PM: Update 276: task multirc, steps since last val 26 (total steps = 276): ans_f1: 0.5123, qst_f1: 0.2513, em: 0.4921, avg: 0.5022, multirc_loss: 0.8782
03/19 09:33:38 PM: Update 279: task multirc, steps since last val 29 (total steps = 279): ans_f1: 0.5000, qst_f1: 0.2372, em: 0.4977, avg: 0.4988, multirc_loss: 0.8690
03/19 09:33:49 PM: Update 282: task multirc, steps since last val 32 (total steps = 282): ans_f1: 0.4917, qst_f1: 0.2331, em: 0.5000, avg: 0.4958, multirc_loss: 0.8609
03/19 09:34:00 PM: Update 285: task multirc, steps since last val 35 (total steps = 285): ans_f1: 0.4784, qst_f1: 0.2227, em: 0.5000, avg: 0.4892, multirc_loss: 0.8517
03/19 09:34:12 PM: Update 288: task multirc, steps since last val 38 (total steps = 288): ans_f1: 0.5190, qst_f1: 0.2536, em: 0.5181, avg: 0.5186, multirc_loss: 0.8320
03/19 09:34:24 PM: Update 291: task multirc, steps since last val 41 (total steps = 291): ans_f1: 0.5217, qst_f1: 0.2658, em: 0.5034, avg: 0.5126, multirc_loss: 0.9050
03/19 09:34:35 PM: Update 294: task multirc, steps since last val 44 (total steps = 294): ans_f1: 0.5174, qst_f1: 0.2631, em: 0.5063, avg: 0.5119, multirc_loss: 0.9027
03/19 09:34:48 PM: Update 297: task multirc, steps since last val 47 (total steps = 297): ans_f1: 0.5042, qst_f1: 0.2475, em: 0.5118, avg: 0.5080, multirc_loss: 0.8970
03/19 09:34:59 PM: Update 300: task multirc, steps since last val 50 (total steps = 300): ans_f1: 0.4904, qst_f1: 0.2343, em: 0.5211, avg: 0.5057, multirc_loss: 0.8908
03/19 09:34:59 PM: ***** Step 300 / Validation 6 *****
03/19 09:35:00 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 09:35:00 PM: Validating...
03/19 09:35:10 PM: Evaluate: task multirc, batch 37 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7228
03/19 09:35:20 PM: Evaluate: task multirc, batch 76 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7186
03/19 09:35:30 PM: Evaluate: task multirc, batch 125 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7051
03/19 09:35:40 PM: Evaluate: task multirc, batch 174 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6924
03/19 09:35:50 PM: Evaluate: task multirc, batch 228 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0057, avg: 0.0028, multirc_loss: 0.6843
03/19 09:36:01 PM: Evaluate: task multirc, batch 279 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0045, avg: 0.0023, multirc_loss: 0.6862
03/19 09:36:11 PM: Evaluate: task multirc, batch 330 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0019, multirc_loss: 0.6847
03/19 09:36:22 PM: Evaluate: task multirc, batch 381 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 0.6862
03/19 09:36:32 PM: Evaluate: task multirc, batch 434 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0043, avg: 0.0021, multirc_loss: 0.6914
03/19 09:36:43 PM: Evaluate: task multirc, batch 483 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0019, multirc_loss: 0.6915
03/19 09:36:53 PM: Evaluate: task multirc, batch 535 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0035, avg: 0.0018, multirc_loss: 0.6902
03/19 09:37:04 PM: Evaluate: task multirc, batch 587 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 0.6878
03/19 09:37:09 PM: Updating LR scheduler:
03/19 09:37:09 PM: 	Best result seen so far for macro_avg: 0.304
03/19 09:37:09 PM: 	# validation passes without improvement: 1
03/19 09:37:09 PM: multirc_loss: training: 0.890842 validation: 0.689019
03/19 09:37:09 PM: macro_avg: validation: 0.001574
03/19 09:37:09 PM: micro_avg: validation: 0.001574
03/19 09:37:09 PM: multirc_ans_f1: training: 0.490358 validation: 0.000000
03/19 09:37:09 PM: multirc_qst_f1: training: 0.234272 validation: 0.000000
03/19 09:37:09 PM: multirc_em: training: 0.521127 validation: 0.003148
03/19 09:37:09 PM: multirc_avg: training: 0.505742 validation: 0.001574
03/19 09:37:09 PM: Global learning rate: 0.025
03/19 09:37:09 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5
03/19 09:37:17 PM: Update 302: task multirc, steps since last val 2 (total steps = 302): ans_f1: 0.4615, qst_f1: 0.1875, em: 0.5625, avg: 0.5120, multirc_loss: 0.7034
03/19 09:37:28 PM: Update 305: task multirc, steps since last val 5 (total steps = 305): ans_f1: 0.4545, qst_f1: 0.2500, em: 0.4000, avg: 0.4273, multirc_loss: 0.9972
03/19 09:37:38 PM: Update 308: task multirc, steps since last val 8 (total steps = 308): ans_f1: 0.4138, qst_f1: 0.1875, em: 0.4688, avg: 0.4413, multirc_loss: 0.9008
03/19 09:37:49 PM: Update 311: task multirc, steps since last val 11 (total steps = 311): ans_f1: 0.3333, qst_f1: 0.1395, em: 0.4419, avg: 0.3876, multirc_loss: 0.9828
03/19 09:38:02 PM: Update 315: task multirc, steps since last val 15 (total steps = 315): ans_f1: 0.3396, qst_f1: 0.1510, em: 0.4017, avg: 0.3707, multirc_loss: 0.9785
03/19 09:38:13 PM: Update 318: task multirc, steps since last val 18 (total steps = 318): ans_f1: 0.3433, qst_f1: 0.1619, em: 0.3714, avg: 0.3574, multirc_loss: 0.9586
03/19 09:38:24 PM: Update 321: task multirc, steps since last val 21 (total steps = 321): ans_f1: 0.3151, qst_f1: 0.1435, em: 0.3797, avg: 0.3474, multirc_loss: 0.9446
03/19 09:38:36 PM: Update 324: task multirc, steps since last val 24 (total steps = 324): ans_f1: 0.3026, qst_f1: 0.1252, em: 0.4254, avg: 0.3640, multirc_loss: 0.9006
03/19 09:38:47 PM: Update 327: task multirc, steps since last val 27 (total steps = 327): ans_f1: 0.2959, qst_f1: 0.1215, em: 0.4236, avg: 0.3598, multirc_loss: 0.8945
03/19 09:38:57 PM: Update 330: task multirc, steps since last val 30 (total steps = 330): ans_f1: 0.3037, qst_f1: 0.1244, em: 0.4178, avg: 0.3607, multirc_loss: 0.8891
03/19 09:39:08 PM: Update 333: task multirc, steps since last val 33 (total steps = 333): ans_f1: 0.2956, qst_f1: 0.1170, em: 0.4327, avg: 0.3641, multirc_loss: 0.8650
03/19 09:39:21 PM: Update 336: task multirc, steps since last val 36 (total steps = 336): ans_f1: 0.2752, qst_f1: 0.1078, em: 0.4248, avg: 0.3500, multirc_loss: 0.8820
03/19 09:39:32 PM: Update 339: task multirc, steps since last val 39 (total steps = 339): ans_f1: 0.2707, qst_f1: 0.1037, em: 0.4336, avg: 0.3522, multirc_loss: 0.8667
03/19 09:39:43 PM: Update 342: task multirc, steps since last val 42 (total steps = 342): ans_f1: 0.3077, qst_f1: 0.1260, em: 0.4332, avg: 0.3705, multirc_loss: 0.8589
03/19 09:39:54 PM: Update 345: task multirc, steps since last val 45 (total steps = 345): ans_f1: 0.3214, qst_f1: 0.1303, em: 0.4369, avg: 0.3792, multirc_loss: 0.8465
03/19 09:40:04 PM: Update 348: task multirc, steps since last val 48 (total steps = 348): ans_f1: 0.3125, qst_f1: 0.1231, em: 0.4477, avg: 0.3801, multirc_loss: 0.8318
03/19 09:40:12 PM: ***** Step 350 / Validation 7 *****
03/19 09:40:12 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 09:40:12 PM: Validating...
03/19 09:40:15 PM: Evaluate: task multirc, batch 16 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0345, avg: 0.0172, multirc_loss: 0.7953
03/19 09:40:25 PM: Evaluate: task multirc, batch 72 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7873
03/19 09:40:35 PM: Evaluate: task multirc, batch 125 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7576
03/19 09:40:45 PM: Evaluate: task multirc, batch 178 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7372
03/19 09:40:55 PM: Evaluate: task multirc, batch 230 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0084, avg: 0.0042, multirc_loss: 0.7198
03/19 09:41:06 PM: Evaluate: task multirc, batch 281 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0045, avg: 0.0023, multirc_loss: 0.7245
03/19 09:41:16 PM: Evaluate: task multirc, batch 335 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0038, avg: 0.0019, multirc_loss: 0.7215
03/19 09:41:27 PM: Evaluate: task multirc, batch 382 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 0.7243
03/19 09:41:37 PM: Evaluate: task multirc, batch 432 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0029, avg: 0.0014, multirc_loss: 0.7325
03/19 09:41:48 PM: Evaluate: task multirc, batch 482 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0039, avg: 0.0019, multirc_loss: 0.7333
03/19 09:41:58 PM: Evaluate: task multirc, batch 530 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 0.7311
03/19 09:42:09 PM: Evaluate: task multirc, batch 572 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0033, avg: 0.0017, multirc_loss: 0.7276
03/19 09:42:17 PM: Updating LR scheduler:
03/19 09:42:17 PM: 	Best result seen so far for macro_avg: 0.304
03/19 09:42:17 PM: 	# validation passes without improvement: 0
03/19 09:42:17 PM: multirc_loss: training: 0.829713 validation: 0.729076
03/19 09:42:17 PM: macro_avg: validation: 0.001574
03/19 09:42:17 PM: micro_avg: validation: 0.001574
03/19 09:42:17 PM: multirc_ans_f1: training: 0.304054 validation: 0.000000
03/19 09:42:17 PM: multirc_qst_f1: training: 0.118250 validation: 0.000000
03/19 09:42:17 PM: multirc_em: training: 0.446927 validation: 0.003148
03/19 09:42:17 PM: multirc_avg: training: 0.375491 validation: 0.001574
03/19 09:42:17 PM: Global learning rate: 0.0125
03/19 09:42:17 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5
03/19 09:42:22 PM: Update 351: task multirc, steps since last val 1 (total steps = 351): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.5000, avg: 0.2500, multirc_loss: 0.6973
03/19 09:42:32 PM: Update 354: task multirc, steps since last val 4 (total steps = 354): ans_f1: 0.1333, qst_f1: 0.0312, em: 0.5938, avg: 0.3635, multirc_loss: 0.6664
03/19 09:42:44 PM: Update 357: task multirc, steps since last val 7 (total steps = 357): ans_f1: 0.4783, qst_f1: 0.2000, em: 0.5818, avg: 0.5300, multirc_loss: 0.6885
03/19 09:42:54 PM: Update 360: task multirc, steps since last val 10 (total steps = 360): ans_f1: 0.5063, qst_f1: 0.2564, em: 0.5256, avg: 0.5160, multirc_loss: 0.7223
03/19 09:43:05 PM: Update 363: task multirc, steps since last val 13 (total steps = 363): ans_f1: 0.4854, qst_f1: 0.2451, em: 0.5000, avg: 0.4927, multirc_loss: 0.7476
03/19 09:43:15 PM: Update 366: task multirc, steps since last val 16 (total steps = 366): ans_f1: 0.4957, qst_f1: 0.2276, em: 0.5447, avg: 0.5202, multirc_loss: 0.7225
03/19 09:43:29 PM: Update 370: task multirc, steps since last val 20 (total steps = 370): ans_f1: 0.4427, qst_f1: 0.1818, em: 0.5455, avg: 0.4941, multirc_loss: 0.7267
03/19 09:43:39 PM: Update 373: task multirc, steps since last val 23 (total steps = 373): ans_f1: 0.4173, qst_f1: 0.1591, em: 0.5568, avg: 0.4870, multirc_loss: 0.7198
03/19 09:43:50 PM: Update 376: task multirc, steps since last val 26 (total steps = 376): ans_f1: 0.4000, qst_f1: 0.1407, em: 0.5779, avg: 0.4889, multirc_loss: 0.7060
03/19 09:44:01 PM: Update 379: task multirc, steps since last val 29 (total steps = 379): ans_f1: 0.3718, qst_f1: 0.1279, em: 0.5708, avg: 0.4713, multirc_loss: 0.7088
03/19 09:44:12 PM: Update 382: task multirc, steps since last val 32 (total steps = 382): ans_f1: 0.3537, qst_f1: 0.1172, em: 0.5774, avg: 0.4655, multirc_loss: 0.7051
03/19 09:44:23 PM: Update 385: task multirc, steps since last val 35 (total steps = 385): ans_f1: 0.3529, qst_f1: 0.1240, em: 0.5543, avg: 0.4536, multirc_loss: 0.7079
03/19 09:44:34 PM: Update 388: task multirc, steps since last val 38 (total steps = 388): ans_f1: 0.4147, qst_f1: 0.1560, em: 0.5679, avg: 0.4913, multirc_loss: 0.7037
03/19 09:44:44 PM: Update 391: task multirc, steps since last val 41 (total steps = 391): ans_f1: 0.4083, qst_f1: 0.1584, em: 0.5548, avg: 0.4816, multirc_loss: 0.7093
03/19 09:44:56 PM: Update 394: task multirc, steps since last val 44 (total steps = 394): ans_f1: 0.3861, qst_f1: 0.1530, em: 0.5253, avg: 0.4557, multirc_loss: 0.7129
03/19 09:45:06 PM: Update 397: task multirc, steps since last val 47 (total steps = 397): ans_f1: 0.3717, qst_f1: 0.1429, em: 0.5268, avg: 0.4493, multirc_loss: 0.7138
03/19 09:45:17 PM: Update 400: task multirc, steps since last val 50 (total steps = 400): ans_f1: 0.3571, qst_f1: 0.1358, em: 0.5157, avg: 0.4364, multirc_loss: 0.7139
03/19 09:45:17 PM: ***** Step 400 / Validation 8 *****
03/19 09:45:17 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 09:45:17 PM: Validating...
03/19 09:45:27 PM: Evaluate: task multirc, batch 53 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7339
03/19 09:45:37 PM: Evaluate: task multirc, batch 108 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7049
03/19 09:45:47 PM: Evaluate: task multirc, batch 163 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6923
03/19 09:45:57 PM: Evaluate: task multirc, batch 215 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0060, avg: 0.0030, multirc_loss: 0.6861
03/19 09:46:08 PM: Evaluate: task multirc, batch 266 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0071, avg: 0.0036, multirc_loss: 0.6858
03/19 09:46:18 PM: Evaluate: task multirc, batch 313 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0061, avg: 0.0031, multirc_loss: 0.6834
03/19 09:46:29 PM: Evaluate: task multirc, batch 360 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 0.6872
03/19 09:46:39 PM: Evaluate: task multirc, batch 412 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0030, avg: 0.0015, multirc_loss: 0.6917
03/19 09:46:50 PM: Evaluate: task multirc, batch 462 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0027, avg: 0.0013, multirc_loss: 0.6925
03/19 09:47:00 PM: Evaluate: task multirc, batch 507 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0019, multirc_loss: 0.6903
03/19 09:47:11 PM: Evaluate: task multirc, batch 552 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 0.6901
03/19 09:47:22 PM: Evaluate: task multirc, batch 605 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 0.6889
03/19 09:47:23 PM: Updating LR scheduler:
03/19 09:47:23 PM: 	Best result seen so far for macro_avg: 0.304
03/19 09:47:23 PM: 	# validation passes without improvement: 1
03/19 09:47:23 PM: multirc_loss: training: 0.713948 validation: 0.689128
03/19 09:47:23 PM: macro_avg: validation: 0.001574
03/19 09:47:23 PM: micro_avg: validation: 0.001574
03/19 09:47:23 PM: multirc_ans_f1: training: 0.357143 validation: 0.000000
03/19 09:47:23 PM: multirc_qst_f1: training: 0.135802 validation: 0.000000
03/19 09:47:23 PM: multirc_em: training: 0.515670 validation: 0.003148
03/19 09:47:23 PM: multirc_avg: training: 0.436406 validation: 0.001574
03/19 09:47:23 PM: Global learning rate: 0.0125
03/19 09:47:23 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5
03/19 09:47:34 PM: Update 403: task multirc, steps since last val 3 (total steps = 403): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.6667, avg: 0.3333, multirc_loss: 0.6828
03/19 09:47:46 PM: Update 406: task multirc, steps since last val 6 (total steps = 406): ans_f1: 0.0833, qst_f1: 0.0208, em: 0.5417, avg: 0.3125, multirc_loss: 0.7472
03/19 09:48:00 PM: Update 410: task multirc, steps since last val 10 (total steps = 410): ans_f1: 0.3077, qst_f1: 0.1026, em: 0.5385, avg: 0.4231, multirc_loss: 0.7338
03/19 09:48:10 PM: Update 413: task multirc, steps since last val 13 (total steps = 413): ans_f1: 0.4000, qst_f1: 0.1569, em: 0.5294, avg: 0.4647, multirc_loss: 0.7309
03/19 09:48:24 PM: Update 417: task multirc, steps since last val 17 (total steps = 417): ans_f1: 0.4231, qst_f1: 0.1679, em: 0.5496, avg: 0.4863, multirc_loss: 0.7182
03/19 09:48:35 PM: Update 420: task multirc, steps since last val 20 (total steps = 420): ans_f1: 0.3793, qst_f1: 0.1447, em: 0.5329, avg: 0.4561, multirc_loss: 0.7240
03/19 09:48:45 PM: Update 423: task multirc, steps since last val 23 (total steps = 423): ans_f1: 0.3548, qst_f1: 0.1272, em: 0.5434, avg: 0.4491, multirc_loss: 0.7160
03/19 09:48:55 PM: Update 426: task multirc, steps since last val 26 (total steps = 426): ans_f1: 0.3235, qst_f1: 0.1134, em: 0.5309, avg: 0.4272, multirc_loss: 0.7246
03/19 09:49:06 PM: Update 429: task multirc, steps since last val 29 (total steps = 429): ans_f1: 0.3056, qst_f1: 0.1019, em: 0.5463, avg: 0.4259, multirc_loss: 0.7213
03/19 09:49:17 PM: Update 432: task multirc, steps since last val 32 (total steps = 432): ans_f1: 0.2895, qst_f1: 0.0921, em: 0.5565, avg: 0.4230, multirc_loss: 0.7142
03/19 09:49:28 PM: Update 435: task multirc, steps since last val 35 (total steps = 435): ans_f1: 0.2750, qst_f1: 0.0843, em: 0.5632, avg: 0.4191, multirc_loss: 0.7096
03/19 09:49:38 PM: Update 438: task multirc, steps since last val 38 (total steps = 438): ans_f1: 0.2558, qst_f1: 0.0789, em: 0.5484, avg: 0.4021, multirc_loss: 0.7152
03/19 09:49:49 PM: Update 441: task multirc, steps since last val 41 (total steps = 441): ans_f1: 0.2513, qst_f1: 0.0795, em: 0.5331, avg: 0.3922, multirc_loss: 0.7212
03/19 09:50:00 PM: Update 444: task multirc, steps since last val 44 (total steps = 444): ans_f1: 0.2617, qst_f1: 0.0865, em: 0.5188, avg: 0.3902, multirc_loss: 0.7212
03/19 09:50:11 PM: Update 447: task multirc, steps since last val 47 (total steps = 447): ans_f1: 0.2893, qst_f1: 0.0996, em: 0.5059, avg: 0.3976, multirc_loss: 0.7209
03/19 09:50:22 PM: Update 450: task multirc, steps since last val 50 (total steps = 450): ans_f1: 0.3030, qst_f1: 0.1071, em: 0.5069, avg: 0.4050, multirc_loss: 0.7198
03/19 09:50:22 PM: ***** Step 450 / Validation 9 *****
03/19 09:50:22 PM: multirc: trained on 50 steps (50 batches) since val, 0.015 epochs
03/19 09:50:22 PM: Validating...
03/19 09:50:32 PM: Evaluate: task multirc, batch 53 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6986
03/19 09:50:42 PM: Evaluate: task multirc, batch 103 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6899
03/19 09:50:52 PM: Evaluate: task multirc, batch 155 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6860
03/19 09:51:02 PM: Evaluate: task multirc, batch 202 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 0.6855
03/19 09:51:13 PM: Evaluate: task multirc, batch 252 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0051, avg: 0.0025, multirc_loss: 0.6833
03/19 09:51:23 PM: Evaluate: task multirc, batch 304 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0042, avg: 0.0021, multirc_loss: 0.6835
03/19 09:51:34 PM: Evaluate: task multirc, batch 360 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0036, avg: 0.0018, multirc_loss: 0.6843
03/19 09:51:44 PM: Evaluate: task multirc, batch 413 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0030, avg: 0.0015, multirc_loss: 0.6856
03/19 09:51:55 PM: Evaluate: task multirc, batch 464 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0040, avg: 0.0020, multirc_loss: 0.6858
03/19 09:52:05 PM: Evaluate: task multirc, batch 516 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0037, avg: 0.0018, multirc_loss: 0.6851
03/19 09:52:16 PM: Evaluate: task multirc, batch 565 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0034, avg: 0.0017, multirc_loss: 0.6848
03/19 09:52:27 PM: Evaluate: task multirc, batch 599 (606): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0032, avg: 0.0016, multirc_loss: 0.6844
03/19 09:52:29 PM: Updating LR scheduler:
03/19 09:52:29 PM: 	Best result seen so far for macro_avg: 0.304
03/19 09:52:29 PM: 	# validation passes without improvement: 0
03/19 09:52:29 PM: Ran out of early stopping patience. Stopping training.
03/19 09:52:29 PM: multirc_loss: training: 0.719840 validation: 0.684843
03/19 09:52:29 PM: macro_avg: validation: 0.001574
03/19 09:52:29 PM: micro_avg: validation: 0.001574
03/19 09:52:29 PM: multirc_ans_f1: training: 0.303030 validation: 0.000000
03/19 09:52:29 PM: multirc_qst_f1: training: 0.107110 validation: 0.000000
03/19 09:52:29 PM: multirc_em: training: 0.506925 validation: 0.003148
03/19 09:52:29 PM: multirc_avg: training: 0.404978 validation: 0.001574
03/19 09:52:29 PM: Global learning rate: 0.00625
03/19 09:52:29 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5
03/19 09:52:30 PM: Stopped training after 9 validation checks
03/19 09:52:30 PM: Trained multirc for 450 steps or 0.132 epochs
03/19 09:52:30 PM: ***** VALIDATION RESULTS *****
03/19 09:52:30 PM: multirc_avg (for best val pass 3): multirc_loss: 0.97615, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/19 09:52:30 PM: micro_avg (for best val pass 3): multirc_loss: 0.97615, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/19 09:52:30 PM: macro_avg (for best val pass 3): multirc_loss: 0.97615, macro_avg: 0.30392, micro_avg: 0.30392, multirc_ans_f1: 0.59945, multirc_qst_f1: 0.59096, multirc_em: 0.00839, multirc_avg: 0.30392
03/19 09:52:30 PM: Evaluating...
03/19 09:52:31 PM: Loaded model state from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/mutlirc_bert_cased_run5/model_state_pretrain_val_3.best.th
03/19 09:52:31 PM: Evaluating on: multirc, split: val
03/19 09:53:01 PM: 	Task multirc: batch 142
03/19 09:53:31 PM: 	Task multirc: batch 272
03/19 09:54:01 PM: 	Task multirc: batch 427
03/19 09:54:31 PM: 	Task multirc: batch 541
03/19 09:54:46 PM: Task 'multirc': sorting predictions by 'idx'
03/19 09:54:46 PM: Finished evaluating on: multirc
03/19 09:54:46 PM: Writing results for split 'val' to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/mutlirc_bert_cased_exp/results.tsv
03/19 09:54:46 PM: micro_avg: 0.304, macro_avg: 0.304, multirc_ans_f1: 0.599, multirc_qst_f1: 0.591, multirc_em: 0.008, multirc_avg: 0.304
03/19 09:54:46 PM: Done!
