04/06 06:36:55 PM: Git branch: develop
04/06 06:36:55 PM: Git SHA: 05c928e87bb2c6621302b26bd88d6ec53ac1ff9b
04/06 06:36:56 PM: Parsed args: 
{
  "classifier": "log_reg",
  "do_pretrain": 0,
  "do_target_task_training": 0,
  "dropout": 0.1,
  "dropout_embs": 0.1,
  "exp_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/",
  "exp_name": "bert_uncased_multirc_PT",
  "input_module": "bert-base-cased",
  "load_eval_checkpoint": "output/bert_uncased_multirc_PT/run0/multirc/model_state_target_train_val_7.best.th",
  "load_target_train_checkpoint": "output/bert_uncased_multirc_PT/run0/multirc/model_state_target_train_val_7.best.th",
  "local_log_path": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run_for_logits/log.log",
  "lr": 5e-06,
  "lr_patience": 4,
  "max_epochs": 10,
  "max_seq_len": 256,
  "max_vals": 10,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "remote_log_name": "bert_uncased_multirc_PT__run_for_logits",
  "run_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run_for_logits",
  "run_name": "run_for_logits",
  "s2s": {
    "attention": "none"
  },
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "multirc",
  "target_train_max_vals": 100,
  "target_train_val_interval": 1,
  "transfer_paradigm": "finetune",
  "transformers_output_mode": "top",
  "val_interval": 1,
  "write_preds": "val,test",
  "write_strict_glue_format": 1
}
04/06 06:36:56 PM: Saved config to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run_for_logits/params.conf
04/06 06:36:56 PM: Using random seed 1234
04/06 06:36:56 PM: Loading tasks...
04/06 06:36:56 PM: Writing pre-preprocessed tasks to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/
04/06 06:36:56 PM: 	Loaded existing task multirc
04/06 06:36:56 PM: 	Task 'multirc': |train|=1375 |val|=1186 |test|=1153
04/06 06:36:56 PM: 	Loaded existing task sst
04/06 06:36:56 PM: 	Task 'sst': |train|=67349 |val|=872 |test|=1821
04/06 06:36:56 PM: 	Finished loading tasks: multirc sst.
04/06 06:36:56 PM: Loading token dictionary from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/vocab.
04/06 06:36:56 PM: 	Loaded vocab from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/vocab
04/06 06:36:56 PM: 	Vocab namespace chars: size 116
04/06 06:36:56 PM: 	Vocab namespace bert_cased: size 28998
04/06 06:36:56 PM: 	Vocab namespace tokens: size 18815
04/06 06:36:56 PM: 	Finished building vocab.
04/06 06:36:56 PM: 	Task 'multirc', split 'train': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/multirc__train_data
04/06 06:36:56 PM: 	Task 'multirc', split 'val': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/multirc__val_data
04/06 06:36:56 PM: 	Task 'multirc', split 'test': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/multirc__test_data
04/06 06:36:56 PM: 	Task 'sst', split 'train': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/sst__train_data
04/06 06:36:56 PM: 	Task 'sst', split 'val': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/sst__val_data
04/06 06:36:56 PM: 	Task 'sst', split 'test': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/sst__test_data
04/06 06:36:56 PM: 	Finished indexing tasks
04/06 06:36:56 PM: 	Creating trimmed target-only version of multirc train.
04/06 06:36:56 PM: 	Creating trimmed pretraining-only version of sst train.
04/06 06:36:56 PM: 	  Training on sst
04/06 06:36:56 PM: 	  Evaluating on multirc
04/06 06:36:56 PM: 	Finished loading tasks in 0.286s
04/06 06:36:56 PM: 	 Tasks: ['multirc', 'sst']
04/06 06:36:56 PM: Building model...
04/06 06:36:56 PM: Using BERT model (bert-base-cased).
04/06 06:36:57 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
04/06 06:36:57 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

04/06 06:36:57 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/06 06:36:59 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/06 06:36:59 PM: Initializing parameters
04/06 06:36:59 PM: Done initializing parameters; the following parameters are using their default initialization from their code
04/06 06:36:59 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
04/06 06:36:59 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
04/06 06:36:59 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
04/06 06:36:59 PM:    _text_field_embedder.model.pooler.dense.bias
04/06 06:36:59 PM:    _text_field_embedder.model.pooler.dense.weight
04/06 06:36:59 PM: 	Task 'multirc' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
04/06 06:36:59 PM: 	Task 'sst' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "sst"
}
04/06 06:36:59 PM: Model specification:
04/06 06:36:59 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.1)
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (sst_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
04/06 06:36:59 PM: Model parameters:
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/06 06:36:59 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/06 06:36:59 PM: 	multirc_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
04/06 06:36:59 PM: 	multirc_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
04/06 06:36:59 PM: 	sst_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
04/06 06:36:59 PM: 	sst_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
04/06 06:36:59 PM: Total number of parameters: 108313348 (1.08313e+08)
04/06 06:36:59 PM: Number of trainable parameters: 108313348 (1.08313e+08)
04/06 06:36:59 PM: Finished building model in 3.178s
04/06 06:36:59 PM: Evauluating a target task model on tasks {'multirc'} without training it in this run. It's up to you to ensure that you are loading parameters that were sufficiently trained for this task.
04/06 06:36:59 PM: Will run the following steps for this experiment:
Loading model from path: output/bert_uncased_multirc_PT/run0/multirc/model_state_target_train_val_7.best.th 
Evaluating model on tasks: multirc 

04/06 06:36:59 PM: Evaluating...
04/06 06:37:00 PM: Loaded model state from output/bert_uncased_multirc_PT/run0/multirc/model_state_target_train_val_7.best.th
04/06 06:37:00 PM: Evaluating on: multirc, split: val
04/06 06:37:39 PM: 	Task multirc: batch 3
04/06 06:38:11 PM: 	Task multirc: batch 6
04/06 06:38:49 PM: 	Task multirc: batch 10
04/06 06:39:27 PM: 	Task multirc: batch 15
04/06 06:39:57 PM: 	Task multirc: batch 18
04/06 06:40:34 PM: 	Task multirc: batch 22
04/06 06:41:11 PM: 	Task multirc: batch 26
04/06 06:41:52 PM: 	Task multirc: batch 30
04/06 06:42:29 PM: 	Task multirc: batch 34
04/06 06:43:05 PM: 	Task multirc: batch 38
04/06 06:43:40 PM: 	Task multirc: batch 42
04/06 06:44:15 PM: 	Task multirc: batch 46
04/06 06:44:53 PM: 	Task multirc: batch 50
04/06 06:45:33 PM: 	Task multirc: batch 54
04/06 06:46:14 PM: 	Task multirc: batch 58
04/06 06:46:48 PM: 	Task multirc: batch 62
04/06 06:47:27 PM: 	Task multirc: batch 66
04/06 06:48:03 PM: 	Task multirc: batch 69
04/06 06:48:36 PM: 	Task multirc: batch 72
04/06 06:49:06 PM: 	Task multirc: batch 75
04/06 06:49:38 PM: 	Task multirc: batch 79
04/06 06:50:09 PM: 	Task multirc: batch 83
04/06 06:50:43 PM: 	Task multirc: batch 86
04/06 06:51:17 PM: 	Task multirc: batch 90
04/06 06:51:54 PM: 	Task multirc: batch 94
04/06 06:52:37 PM: 	Task multirc: batch 98
04/06 06:53:07 PM: 	Task multirc: batch 102
04/06 06:53:40 PM: 	Task multirc: batch 106
04/06 06:54:10 PM: 	Task multirc: batch 110
04/06 06:54:45 PM: 	Task multirc: batch 113
04/06 06:55:21 PM: 	Task multirc: batch 117
04/06 06:56:00 PM: 	Task multirc: batch 121
04/06 06:56:38 PM: 	Task multirc: batch 125
04/06 06:57:12 PM: 	Task multirc: batch 129
04/06 06:57:48 PM: 	Task multirc: batch 133
04/06 06:58:19 PM: 	Task multirc: batch 136
04/06 06:58:55 PM: 	Task multirc: batch 141
04/06 06:59:34 PM: 	Task multirc: batch 145
04/06 07:00:05 PM: 	Task multirc: batch 148
04/06 07:00:31 PM: Task 'multirc': sorting predictions by 'idx'
04/06 07:00:31 PM: Finished evaluating on: multirc
04/06 07:00:32 PM: Task 'multirc': Wrote predictions to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run_for_logits
04/06 07:00:32 PM: Wrote all preds for split 'val' to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run_for_logits
04/06 07:00:32 PM: Evaluating on: multirc, split: test
04/06 07:01:09 PM: 	Task multirc: batch 2
04/06 07:01:41 PM: 	Task multirc: batch 6
04/06 07:02:15 PM: 	Task multirc: batch 9
04/06 07:02:51 PM: 	Task multirc: batch 13
04/06 07:03:25 PM: 	Task multirc: batch 16
04/06 07:03:58 PM: 	Task multirc: batch 19
04/06 07:04:28 PM: 	Task multirc: batch 22
04/06 07:05:01 PM: 	Task multirc: batch 25
04/06 07:05:33 PM: 	Task multirc: batch 28
04/06 07:06:04 PM: 	Task multirc: batch 31
04/06 07:06:40 PM: 	Task multirc: batch 35
04/06 07:07:14 PM: 	Task multirc: batch 39
04/06 07:07:50 PM: 	Task multirc: batch 43
04/06 07:08:21 PM: 	Task multirc: batch 47
04/06 07:08:53 PM: 	Task multirc: batch 51
04/06 07:09:25 PM: 	Task multirc: batch 55
04/06 07:09:56 PM: 	Task multirc: batch 60
04/06 07:10:28 PM: 	Task multirc: batch 64
04/06 07:11:01 PM: 	Task multirc: batch 69
04/06 07:11:38 PM: 	Task multirc: batch 73
04/06 07:12:10 PM: 	Task multirc: batch 77
04/06 07:12:47 PM: 	Task multirc: batch 81
04/06 07:13:20 PM: 	Task multirc: batch 85
04/06 07:13:54 PM: 	Task multirc: batch 89
04/06 07:14:28 PM: 	Task multirc: batch 93
04/06 07:15:01 PM: 	Task multirc: batch 98
04/06 07:15:37 PM: 	Task multirc: batch 102
04/06 07:16:09 PM: 	Task multirc: batch 106
04/06 07:16:43 PM: 	Task multirc: batch 111
04/06 07:17:14 PM: 	Task multirc: batch 115
04/06 07:17:48 PM: 	Task multirc: batch 119
04/06 07:18:23 PM: 	Task multirc: batch 124
04/06 07:18:54 PM: 	Task multirc: batch 128
04/06 07:19:30 PM: 	Task multirc: batch 133
04/06 07:20:03 PM: 	Task multirc: batch 138
04/06 07:20:37 PM: 	Task multirc: batch 144
04/06 07:21:13 PM: 	Task multirc: batch 150
04/06 07:21:46 PM: 	Task multirc: batch 154
04/06 07:22:19 PM: 	Task multirc: batch 158
04/06 07:22:53 PM: 	Task multirc: batch 162
04/06 07:23:30 PM: 	Task multirc: batch 166
04/06 07:24:03 PM: 	Task multirc: batch 170
04/06 07:24:33 PM: 	Task multirc: batch 174
04/06 07:25:09 PM: 	Task multirc: batch 180
04/06 07:25:43 PM: 	Task multirc: batch 184
04/06 07:26:14 PM: 	Task multirc: batch 188
04/06 07:26:46 PM: 	Task multirc: batch 192
04/06 07:27:22 PM: 	Task multirc: batch 197
04/06 07:27:52 PM: 	Task multirc: batch 201
04/06 07:28:30 PM: 	Task multirc: batch 205
04/06 07:29:01 PM: 	Task multirc: batch 209
04/06 07:29:34 PM: 	Task multirc: batch 213
04/06 07:30:08 PM: 	Task multirc: batch 217
04/06 07:30:40 PM: 	Task multirc: batch 221
04/06 07:31:18 PM: 	Task multirc: batch 225
04/06 07:31:50 PM: 	Task multirc: batch 228
04/06 07:32:23 PM: 	Task multirc: batch 232
04/06 07:32:55 PM: 	Task multirc: batch 236
04/06 07:33:28 PM: 	Task multirc: batch 240
04/06 07:33:58 PM: 	Task multirc: batch 244
04/06 07:34:31 PM: 	Task multirc: batch 249
04/06 07:35:05 PM: 	Task multirc: batch 253
04/06 07:35:43 PM: 	Task multirc: batch 257
04/06 07:36:16 PM: 	Task multirc: batch 261
04/06 07:36:52 PM: 	Task multirc: batch 265
04/06 07:37:29 PM: 	Task multirc: batch 270
04/06 07:38:02 PM: 	Task multirc: batch 274
04/06 07:38:40 PM: 	Task multirc: batch 278
04/06 07:39:17 PM: 	Task multirc: batch 283
04/06 07:39:48 PM: 	Task multirc: batch 286
04/06 07:40:25 PM: 	Task multirc: batch 290
04/06 07:40:59 PM: 	Task multirc: batch 294
04/06 07:41:30 PM: 	Task multirc: batch 297
04/06 07:42:07 PM: 	Task multirc: batch 301
04/06 07:42:16 PM: Task 'multirc': sorting predictions by 'idx'
04/06 07:42:16 PM: Finished evaluating on: multirc
04/06 07:42:17 PM: Task 'multirc': Wrote predictions to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run_for_logits
04/06 07:42:17 PM: Wrote all preds for split 'test' to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run_for_logits
04/06 07:42:17 PM: Writing results for split 'val' to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/results.tsv
04/06 07:42:17 PM: micro_avg: 0.301, macro_avg: 0.301, multirc_ans_f1: 0.587, multirc_qst_f1: 0.560, multirc_em: 0.016, multirc_avg: 0.301
04/06 07:42:17 PM: Done!
