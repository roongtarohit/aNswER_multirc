03/25 02:43:51 PM: Git branch: HEAD
03/25 02:43:51 PM: Git SHA: f4a155efabb900c8b3da314cca47384795f97ab6
03/25 02:43:51 PM: Parsed args: 
{
  "batch_size": 4,
  "classifier": "log_reg",
  "classifier_hid_dim": 256,
  "d_proj": 256,
  "dropout": 0.1,
  "dropout_embs": 0.1,
  "exp_dir": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/",
  "exp_name": "bert_uncased_multirc_PT_reduced",
  "input_module": "bert-base-uncased",
  "local_log_path": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc/log.log",
  "lr": 0.0003,
  "lr_patience": 2,
  "max_epochs": 10,
  "max_seq_len": 128,
  "max_vals": 500,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "patience": 2,
  "remote_log_name": "bert_uncased_multirc_PT_reduced__bert_uncased_multirc",
  "run_dir": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc",
  "run_name": "bert_uncased_multirc",
  "s2s": {
    "attention": "none"
  },
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "multirc",
  "target_train_max_vals": 10000,
  "target_train_val_interval": 1,
  "transfer_paradigm": "finetune",
  "transformers_output_mode": "top",
  "val_interval": 100,
  "write_preds": "val,test",
  "write_strict_glue_format": 1
}
03/25 02:43:51 PM: Saved config to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc/params.conf
03/25 02:43:51 PM: Using random seed 1234
03/25 02:43:51 PM: Loading tasks...
03/25 02:43:51 PM: Writing pre-preprocessed tasks to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/
03/25 02:43:51 PM: 	Creating task multirc from scratch.
03/25 02:43:51 PM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
03/25 02:43:51 PM: 	Creating task sst from scratch.
03/25 02:43:52 PM: 	Loading Tokenizer bert-base-uncased
03/25 02:43:52 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/soujanya/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
03/25 02:44:04 PM: 	Finished loading SST data.
03/25 02:44:04 PM: 	Task 'sst': |train|=67349 |val|=872 |test|=1821
03/25 02:44:04 PM: 	Finished loading tasks: multirc sst.
03/25 02:44:04 PM: 	Building vocab from scratch.
03/25 02:44:04 PM: 	Counting units for task multirc.
03/25 02:44:11 PM: 	Counting units for task sst.
03/25 02:44:12 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/soujanya/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
03/25 02:44:12 PM: Added transformers vocab (bert-base-uncased): 30522 tokens
03/25 02:44:12 PM: 	Saved vocab to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/vocab
03/25 02:44:12 PM: Loading token dictionary from /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/vocab.
03/25 02:44:12 PM: 	Loaded vocab from /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/vocab
03/25 02:44:12 PM: 	Vocab namespace chars: size 78
03/25 02:44:12 PM: 	Vocab namespace bert_uncased: size 30524
03/25 02:44:12 PM: 	Vocab namespace tokens: size 17932
03/25 02:44:12 PM: 	Finished building vocab.
03/25 02:44:12 PM: 	Task multirc (train): Indexing from scratch.
03/25 02:44:29 PM: 	Task multirc (train): Saved 27243 instances to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__train_data
03/25 02:44:29 PM: 	Task multirc (val): Indexing from scratch.
03/25 02:44:32 PM: 	Task multirc (val): Saved 4848 instances to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__val_data
03/25 02:44:32 PM: 	Task multirc (test): Indexing from scratch.
03/25 02:44:38 PM: 	Task multirc (test): Saved 9693 instances to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__test_data
03/25 02:44:38 PM: 	Task sst (train): Indexing from scratch.
03/25 02:44:42 PM: 	Task sst (train): Saved 67349 instances to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__train_data
03/25 02:44:42 PM: 	Task sst (val): Indexing from scratch.
03/25 02:44:42 PM: 	Task sst (val): Saved 872 instances to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__val_data
03/25 02:44:42 PM: 	Task sst (test): Indexing from scratch.
03/25 02:44:42 PM: 	Task sst (test): Saved 1821 instances to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__test_data
03/25 02:44:42 PM: 	Finished indexing tasks
03/25 02:44:42 PM: 	Creating trimmed target-only version of multirc train.
03/25 02:44:42 PM: 	Creating trimmed pretraining-only version of sst train.
03/25 02:44:42 PM: 	  Training on sst
03/25 02:44:42 PM: 	  Evaluating on multirc
03/25 02:44:42 PM: 	Finished loading tasks in 50.650s
03/25 02:44:42 PM: 	 Tasks: ['multirc', 'sst']
03/25 02:44:42 PM: Building model...
03/25 02:44:42 PM: Using BERT model (bert-base-uncased).
03/25 02:44:43 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpl11wg1hu
03/25 02:44:43 PM: copying /tmp/tmpl11wg1hu to cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
03/25 02:44:43 PM: creating metadata file for /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
03/25 02:44:43 PM: removing temp file /tmp/tmpl11wg1hu
03/25 02:44:43 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
03/25 02:44:43 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

03/25 02:44:44 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp2gpe4dej
03/25 03:19:19 PM: copying /tmp/tmp2gpe4dej to cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
03/25 03:19:19 PM: creating metadata file for /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
03/25 03:19:19 PM: removing temp file /tmp/tmp2gpe4dej
03/25 03:19:19 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
03/25 03:19:21 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpb_3m1jl0
03/25 03:19:22 PM: copying /tmp/tmpb_3m1jl0 to cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
03/25 03:19:22 PM: creating metadata file for /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
03/25 03:19:22 PM: removing temp file /tmp/tmpb_3m1jl0
03/25 03:19:22 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
03/25 03:19:22 PM: Initializing parameters
03/25 03:19:22 PM: Done initializing parameters; the following parameters are using their default initialization from their code
03/25 03:19:22 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
03/25 03:19:22 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
03/25 03:19:22 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
03/25 03:19:22 PM:    _text_field_embedder.model.pooler.dense.bias
03/25 03:19:22 PM:    _text_field_embedder.model.pooler.dense.weight
03/25 03:19:22 PM: 	Task 'multirc' params: {
  "cls_type": "log_reg",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 256,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
03/25 03:19:22 PM: 	Task 'sst' params: {
  "cls_type": "log_reg",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 256,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "sst"
}
03/25 03:19:22 PM: Model specification:
03/25 03:19:22 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.1)
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (sst_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
03/25 03:19:22 PM: Model parameters:
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 23440896 with torch.Size([30522, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 03:19:22 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 03:19:22 PM: 	multirc_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/25 03:19:22 PM: 	multirc_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
03/25 03:19:22 PM: 	sst_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/25 03:19:22 PM: 	sst_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
03/25 03:19:22 PM: Total number of parameters: 109485316 (1.09485e+08)
03/25 03:19:22 PM: Number of trainable parameters: 109485316 (1.09485e+08)
03/25 03:19:22 PM: Finished building model in 2080.212s
03/25 03:19:22 PM: Will run the following steps for this experiment:
Training model on tasks: sst 
Re-training model for individual target tasks 
Evaluating model on tasks: multirc 

03/25 03:19:22 PM: Training...
03/25 03:19:22 PM: patience = 2
03/25 03:19:22 PM: val_interval = 100
03/25 03:19:22 PM: max_vals = 500
03/25 03:19:22 PM: cuda_device = -1
03/25 03:19:22 PM: grad_norm = 5.0
03/25 03:19:22 PM: grad_clipping = None
03/25 03:19:22 PM: lr_decay = 0.99
03/25 03:19:22 PM: min_lr = 1e-06
03/25 03:19:22 PM: keep_all_checkpoints = 0
03/25 03:19:22 PM: val_data_limit = 5000
03/25 03:19:22 PM: max_epochs = 10
03/25 03:19:22 PM: dec_val_scale = 250
03/25 03:19:22 PM: training_data_fraction = 1
03/25 03:19:22 PM: accumulation_steps = 1
03/25 03:19:22 PM: type = bert_adam
03/25 03:19:22 PM: parameter_groups = None
03/25 03:19:22 PM: Number of trainable parameters: 109485316
03/25 03:19:22 PM: infer_type_and_cast = True
03/25 03:19:22 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/25 03:19:22 PM: CURRENTLY DEFINED PARAMETERS: 
03/25 03:19:22 PM: lr = 0.0003
03/25 03:19:22 PM: t_total = 50000
03/25 03:19:22 PM: warmup = 0.1
03/25 03:19:22 PM: type = reduce_on_plateau
03/25 03:19:22 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/25 03:19:22 PM: CURRENTLY DEFINED PARAMETERS: 
03/25 03:19:22 PM: mode = max
03/25 03:19:22 PM: factor = 0.5
03/25 03:19:22 PM: patience = 2
03/25 03:19:22 PM: threshold = 0.0001
03/25 03:19:22 PM: threshold_mode = abs
03/25 03:19:22 PM: verbose = True
03/25 03:19:22 PM: load_model=1 but there is not checkpoint.                         Starting training without restoring from a checkpoint.
03/25 03:19:22 PM: Training examples per task, before any subsampling: {'sst': 67349}
03/25 03:19:22 PM: Beginning training with stopping criteria based on metric: sst_accuracy
03/25 03:19:34 PM: Update 4: task sst, steps since last val 4 (total steps = 4): accuracy: 0.6250, sst_loss: 0.6563
03/25 03:19:45 PM: Update 8: task sst, steps since last val 8 (total steps = 8): accuracy: 0.6250, sst_loss: 0.6456
03/25 03:19:55 PM: Update 12: task sst, steps since last val 12 (total steps = 12): accuracy: 0.5833, sst_loss: 0.6770
03/25 03:20:07 PM: Update 17: task sst, steps since last val 17 (total steps = 17): accuracy: 0.5588, sst_loss: 0.7035
03/25 03:20:19 PM: Update 22: task sst, steps since last val 22 (total steps = 22): accuracy: 0.5682, sst_loss: 0.6957
03/25 03:20:30 PM: Update 26: task sst, steps since last val 26 (total steps = 26): accuracy: 0.5673, sst_loss: 0.6994
03/25 03:20:42 PM: Update 31: task sst, steps since last val 31 (total steps = 31): accuracy: 0.5645, sst_loss: 0.7066
03/25 03:20:52 PM: Update 35: task sst, steps since last val 35 (total steps = 35): accuracy: 0.5643, sst_loss: 0.6953
03/25 03:21:04 PM: Update 40: task sst, steps since last val 40 (total steps = 40): accuracy: 0.5687, sst_loss: 0.6936
03/25 03:21:15 PM: Update 44: task sst, steps since last val 44 (total steps = 44): accuracy: 0.5795, sst_loss: 0.6844
03/25 03:21:27 PM: Update 49: task sst, steps since last val 49 (total steps = 49): accuracy: 0.5663, sst_loss: 0.6890
03/25 03:21:37 PM: Update 53: task sst, steps since last val 53 (total steps = 53): accuracy: 0.5660, sst_loss: 0.6859
03/25 03:21:47 PM: Update 57: task sst, steps since last val 57 (total steps = 57): accuracy: 0.5658, sst_loss: 0.6853
03/25 03:21:57 PM: Update 61: task sst, steps since last val 61 (total steps = 61): accuracy: 0.5656, sst_loss: 0.6841
03/25 03:22:08 PM: Update 65: task sst, steps since last val 65 (total steps = 65): accuracy: 0.5692, sst_loss: 0.6816
03/25 03:22:20 PM: Update 70: task sst, steps since last val 70 (total steps = 70): accuracy: 0.5857, sst_loss: 0.6753
03/25 03:22:30 PM: Update 74: task sst, steps since last val 74 (total steps = 74): accuracy: 0.5811, sst_loss: 0.6778
03/25 03:22:43 PM: Update 79: task sst, steps since last val 79 (total steps = 79): accuracy: 0.5949, sst_loss: 0.6692
03/25 03:22:54 PM: Update 83: task sst, steps since last val 83 (total steps = 83): accuracy: 0.6054, sst_loss: 0.6630
03/25 03:23:04 PM: Update 87: task sst, steps since last val 87 (total steps = 87): accuracy: 0.6149, sst_loss: 0.6566
03/25 03:23:16 PM: Update 92: task sst, steps since last val 92 (total steps = 92): accuracy: 0.6196, sst_loss: 0.6483
03/25 03:23:28 PM: Update 97: task sst, steps since last val 97 (total steps = 97): accuracy: 0.6263, sst_loss: 0.6466
03/25 03:23:36 PM: ***** Step 100 / Validation 1 *****
03/25 03:23:36 PM: sst: trained on 100 steps (100 batches) since val, 0.006 epochs
03/25 03:23:36 PM: Validating...
03/25 03:23:39 PM: Evaluate: task sst, batch 16 (218): accuracy: 0.8906, sst_loss: 0.2955
03/25 03:23:49 PM: Evaluate: task sst, batch 69 (218): accuracy: 0.8333, sst_loss: 0.4144
03/25 03:23:59 PM: Evaluate: task sst, batch 122 (218): accuracy: 0.8607, sst_loss: 0.3695
03/25 03:24:09 PM: Evaluate: task sst, batch 177 (218): accuracy: 0.8559, sst_loss: 0.3672
03/25 03:24:17 PM: Best result seen so far for sst.
03/25 03:24:17 PM: Best result seen so far for micro.
03/25 03:24:17 PM: Best result seen so far for macro.
03/25 03:24:17 PM: Updating LR scheduler:
03/25 03:24:17 PM: 	Best result seen so far for macro_avg: 0.856
03/25 03:24:17 PM: 	# validation passes without improvement: 0
03/25 03:24:17 PM: sst_loss: training: 0.636198 validation: 0.361010
03/25 03:24:17 PM: macro_avg: validation: 0.855505
03/25 03:24:17 PM: micro_avg: validation: 0.855505
03/25 03:24:17 PM: sst_accuracy: training: 0.635000 validation: 0.855505
03/25 03:24:17 PM: Global learning rate: 0.0003
03/25 03:24:17 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 03:24:20 PM: Update 101: task sst, steps since last val 1 (total steps = 101): accuracy: 1.0000, sst_loss: 0.1156
03/25 03:24:31 PM: Update 105: task sst, steps since last val 5 (total steps = 105): accuracy: 0.8500, sst_loss: 0.3768
03/25 03:24:41 PM: Update 109: task sst, steps since last val 9 (total steps = 109): accuracy: 0.8611, sst_loss: 0.4256
03/25 03:24:51 PM: Update 113: task sst, steps since last val 13 (total steps = 113): accuracy: 0.8462, sst_loss: 0.3898
03/25 03:25:03 PM: Update 118: task sst, steps since last val 18 (total steps = 118): accuracy: 0.8611, sst_loss: 0.3503
03/25 03:25:15 PM: Update 123: task sst, steps since last val 23 (total steps = 123): accuracy: 0.8370, sst_loss: 0.3979
03/25 03:25:27 PM: Update 128: task sst, steps since last val 28 (total steps = 128): accuracy: 0.8482, sst_loss: 0.3950
03/25 03:25:38 PM: Update 132: task sst, steps since last val 32 (total steps = 132): accuracy: 0.8125, sst_loss: 0.4982
03/25 03:25:50 PM: Update 137: task sst, steps since last val 37 (total steps = 137): accuracy: 0.8176, sst_loss: 0.4757
03/25 03:26:02 PM: Update 142: task sst, steps since last val 42 (total steps = 142): accuracy: 0.8155, sst_loss: 0.4693
03/25 03:26:14 PM: Update 147: task sst, steps since last val 47 (total steps = 147): accuracy: 0.8191, sst_loss: 0.4715
03/25 03:26:26 PM: Update 152: task sst, steps since last val 52 (total steps = 152): accuracy: 0.8029, sst_loss: 0.5192
03/25 03:26:39 PM: Update 157: task sst, steps since last val 57 (total steps = 157): accuracy: 0.7982, sst_loss: 0.5078
03/25 03:26:52 PM: Update 162: task sst, steps since last val 62 (total steps = 162): accuracy: 0.7984, sst_loss: 0.4945
03/25 03:27:02 PM: Update 166: task sst, steps since last val 66 (total steps = 166): accuracy: 0.8030, sst_loss: 0.4823
03/25 03:27:12 PM: Update 170: task sst, steps since last val 70 (total steps = 170): accuracy: 0.8107, sst_loss: 0.4645
03/25 03:27:22 PM: Update 174: task sst, steps since last val 74 (total steps = 174): accuracy: 0.8176, sst_loss: 0.4576
03/25 03:27:32 PM: Update 178: task sst, steps since last val 78 (total steps = 178): accuracy: 0.8141, sst_loss: 0.4754
03/25 03:27:42 PM: Update 182: task sst, steps since last val 82 (total steps = 182): accuracy: 0.8232, sst_loss: 0.4547
03/25 03:27:54 PM: Update 187: task sst, steps since last val 87 (total steps = 187): accuracy: 0.8247, sst_loss: 0.4557
03/25 03:28:07 PM: Update 192: task sst, steps since last val 92 (total steps = 192): accuracy: 0.8261, sst_loss: 0.4453
03/25 03:28:17 PM: Update 196: task sst, steps since last val 96 (total steps = 196): accuracy: 0.8333, sst_loss: 0.4282
03/25 03:28:27 PM: ***** Step 200 / Validation 2 *****
03/25 03:28:27 PM: sst: trained on 100 steps (100 batches) since val, 0.006 epochs
03/25 03:28:27 PM: Validating...
03/25 03:28:27 PM: Evaluate: task sst, batch 1 (218): accuracy: 0.7500, sst_loss: 0.9962
03/25 03:28:37 PM: Evaluate: task sst, batch 55 (218): accuracy: 0.8273, sst_loss: 0.5528
03/25 03:28:47 PM: Evaluate: task sst, batch 109 (218): accuracy: 0.8463, sst_loss: 0.5054
03/25 03:28:57 PM: Evaluate: task sst, batch 162 (218): accuracy: 0.8364, sst_loss: 0.5381
03/25 03:29:08 PM: Evaluate: task sst, batch 215 (218): accuracy: 0.8314, sst_loss: 0.5398
03/25 03:29:08 PM: Updating LR scheduler:
03/25 03:29:08 PM: 	Best result seen so far for macro_avg: 0.856
03/25 03:29:08 PM: 	# validation passes without improvement: 1
03/25 03:29:08 PM: sst_loss: training: 0.413270 validation: 0.544910
03/25 03:29:08 PM: macro_avg: validation: 0.830275
03/25 03:29:08 PM: micro_avg: validation: 0.830275
03/25 03:29:08 PM: sst_accuracy: training: 0.840000 validation: 0.830275
03/25 03:29:08 PM: Global learning rate: 0.0003
03/25 03:29:08 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 03:29:19 PM: Update 204: task sst, steps since last val 4 (total steps = 204): accuracy: 0.7500, sst_loss: 0.9309
03/25 03:29:29 PM: Update 208: task sst, steps since last val 8 (total steps = 208): accuracy: 0.8125, sst_loss: 0.5546
03/25 03:29:41 PM: Update 213: task sst, steps since last val 13 (total steps = 213): accuracy: 0.8654, sst_loss: 0.4624
03/25 03:29:54 PM: Update 218: task sst, steps since last val 18 (total steps = 218): accuracy: 0.8611, sst_loss: 0.4534
03/25 03:30:04 PM: Update 222: task sst, steps since last val 22 (total steps = 222): accuracy: 0.8409, sst_loss: 0.4978
03/25 03:30:16 PM: Update 227: task sst, steps since last val 27 (total steps = 227): accuracy: 0.8241, sst_loss: 0.5379
03/25 03:30:28 PM: Update 232: task sst, steps since last val 32 (total steps = 232): accuracy: 0.8359, sst_loss: 0.5106
03/25 03:30:39 PM: Update 236: task sst, steps since last val 36 (total steps = 236): accuracy: 0.8264, sst_loss: 0.5178
03/25 03:30:49 PM: Update 240: task sst, steps since last val 40 (total steps = 240): accuracy: 0.8125, sst_loss: 0.5306
03/25 03:31:02 PM: Update 245: task sst, steps since last val 45 (total steps = 245): accuracy: 0.8111, sst_loss: 0.5251
03/25 03:31:14 PM: Update 250: task sst, steps since last val 50 (total steps = 250): accuracy: 0.8250, sst_loss: 0.5036
03/25 03:31:26 PM: Update 255: task sst, steps since last val 55 (total steps = 255): accuracy: 0.8273, sst_loss: 0.4834
03/25 03:31:38 PM: Update 260: task sst, steps since last val 60 (total steps = 260): accuracy: 0.8292, sst_loss: 0.4753
03/25 03:31:51 PM: Update 265: task sst, steps since last val 65 (total steps = 265): accuracy: 0.8269, sst_loss: 0.4839
03/25 03:32:03 PM: Update 270: task sst, steps since last val 70 (total steps = 270): accuracy: 0.8179, sst_loss: 0.5008
03/25 03:32:13 PM: Update 274: task sst, steps since last val 74 (total steps = 274): accuracy: 0.8108, sst_loss: 0.5130
03/25 03:32:26 PM: Update 279: task sst, steps since last val 79 (total steps = 279): accuracy: 0.8038, sst_loss: 0.5133
03/25 03:32:38 PM: Update 284: task sst, steps since last val 84 (total steps = 284): accuracy: 0.7976, sst_loss: 0.5106
03/25 03:32:50 PM: Update 289: task sst, steps since last val 89 (total steps = 289): accuracy: 0.8006, sst_loss: 0.4985
03/25 03:33:03 PM: Update 294: task sst, steps since last val 94 (total steps = 294): accuracy: 0.8059, sst_loss: 0.4938
03/25 03:33:15 PM: Update 299: task sst, steps since last val 99 (total steps = 299): accuracy: 0.8106, sst_loss: 0.4831
03/25 03:33:17 PM: ***** Step 300 / Validation 3 *****
03/25 03:33:17 PM: sst: trained on 100 steps (100 batches) since val, 0.006 epochs
03/25 03:33:17 PM: Validating...
03/25 03:33:25 PM: Evaluate: task sst, batch 42 (218): accuracy: 0.8690, sst_loss: 0.3044
03/25 03:33:35 PM: Evaluate: task sst, batch 96 (218): accuracy: 0.8776, sst_loss: 0.3116
03/25 03:33:45 PM: Evaluate: task sst, batch 148 (218): accuracy: 0.8801, sst_loss: 0.3313
03/25 03:33:55 PM: Evaluate: task sst, batch 201 (218): accuracy: 0.8731, sst_loss: 0.3348
03/25 03:33:58 PM: Best result seen so far for sst.
03/25 03:33:58 PM: Best result seen so far for micro.
03/25 03:33:58 PM: Best result seen so far for macro.
03/25 03:33:58 PM: Updating LR scheduler:
03/25 03:33:58 PM: 	Best result seen so far for macro_avg: 0.872
03/25 03:33:58 PM: 	# validation passes without improvement: 0
03/25 03:33:58 PM: sst_loss: training: 0.482763 validation: 0.342193
03/25 03:33:58 PM: macro_avg: validation: 0.871560
03/25 03:33:58 PM: micro_avg: validation: 0.871560
03/25 03:33:58 PM: sst_accuracy: training: 0.810000 validation: 0.871560
03/25 03:33:58 PM: Global learning rate: 0.0003
03/25 03:33:58 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 03:34:07 PM: Update 303: task sst, steps since last val 3 (total steps = 303): accuracy: 0.8333, sst_loss: 0.4502
03/25 03:34:19 PM: Update 308: task sst, steps since last val 8 (total steps = 308): accuracy: 0.8438, sst_loss: 0.3945
03/25 03:34:32 PM: Update 313: task sst, steps since last val 13 (total steps = 313): accuracy: 0.8462, sst_loss: 0.4398
03/25 03:34:44 PM: Update 318: task sst, steps since last val 18 (total steps = 318): accuracy: 0.8333, sst_loss: 0.5399
03/25 03:34:56 PM: Update 323: task sst, steps since last val 23 (total steps = 323): accuracy: 0.8261, sst_loss: 0.5186
03/25 03:35:06 PM: Update 327: task sst, steps since last val 27 (total steps = 327): accuracy: 0.8333, sst_loss: 0.5045
03/25 03:35:18 PM: Update 332: task sst, steps since last val 32 (total steps = 332): accuracy: 0.8438, sst_loss: 0.4623
03/25 03:35:31 PM: Update 337: task sst, steps since last val 37 (total steps = 337): accuracy: 0.8311, sst_loss: 0.4920
03/25 03:35:43 PM: Update 342: task sst, steps since last val 42 (total steps = 342): accuracy: 0.8393, sst_loss: 0.4542
03/25 03:35:55 PM: Update 347: task sst, steps since last val 47 (total steps = 347): accuracy: 0.8511, sst_loss: 0.4335
03/25 03:36:07 PM: Update 352: task sst, steps since last val 52 (total steps = 352): accuracy: 0.8510, sst_loss: 0.4067
03/25 03:36:19 PM: Update 357: task sst, steps since last val 57 (total steps = 357): accuracy: 0.8596, sst_loss: 0.3858
03/25 03:36:32 PM: Update 362: task sst, steps since last val 62 (total steps = 362): accuracy: 0.8508, sst_loss: 0.4274
03/25 03:36:44 PM: Update 367: task sst, steps since last val 67 (total steps = 367): accuracy: 0.8545, sst_loss: 0.4238
03/25 03:36:56 PM: Update 372: task sst, steps since last val 72 (total steps = 372): accuracy: 0.8576, sst_loss: 0.4149
03/25 03:37:08 PM: Update 377: task sst, steps since last val 77 (total steps = 377): accuracy: 0.8604, sst_loss: 0.4047
03/25 03:37:20 PM: Update 382: task sst, steps since last val 82 (total steps = 382): accuracy: 0.8567, sst_loss: 0.4187
03/25 03:37:30 PM: Update 386: task sst, steps since last val 86 (total steps = 386): accuracy: 0.8488, sst_loss: 0.4404
03/25 03:37:42 PM: Update 391: task sst, steps since last val 91 (total steps = 391): accuracy: 0.8489, sst_loss: 0.4413
03/25 03:37:53 PM: Update 395: task sst, steps since last val 95 (total steps = 395): accuracy: 0.8474, sst_loss: 0.4413
03/25 03:38:05 PM: Update 400: task sst, steps since last val 100 (total steps = 400): accuracy: 0.8450, sst_loss: 0.4327
03/25 03:38:05 PM: ***** Step 400 / Validation 4 *****
03/25 03:38:05 PM: sst: trained on 100 steps (100 batches) since val, 0.006 epochs
03/25 03:38:05 PM: Validating...
03/25 03:38:15 PM: Evaluate: task sst, batch 55 (218): accuracy: 0.8636, sst_loss: 0.3620
03/25 03:38:25 PM: Evaluate: task sst, batch 109 (218): accuracy: 0.8784, sst_loss: 0.3288
03/25 03:38:35 PM: Evaluate: task sst, batch 162 (218): accuracy: 0.8735, sst_loss: 0.3268
03/25 03:38:45 PM: Evaluate: task sst, batch 214 (218): accuracy: 0.8727, sst_loss: 0.3247
03/25 03:38:46 PM: Updating LR scheduler:
03/25 03:38:46 PM: 	Best result seen so far for macro_avg: 0.872
03/25 03:38:46 PM: 	# validation passes without improvement: 1
03/25 03:38:46 PM: sst_loss: training: 0.432661 validation: 0.330255
03/25 03:38:46 PM: macro_avg: validation: 0.870413
03/25 03:38:46 PM: micro_avg: validation: 0.870413
03/25 03:38:46 PM: sst_accuracy: training: 0.845000 validation: 0.870413
03/25 03:38:46 PM: Global learning rate: 0.0003
03/25 03:38:46 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 03:38:57 PM: Update 404: task sst, steps since last val 4 (total steps = 404): accuracy: 0.7500, sst_loss: 0.7458
03/25 03:39:10 PM: Update 409: task sst, steps since last val 9 (total steps = 409): accuracy: 0.8056, sst_loss: 0.6161
03/25 03:39:22 PM: Update 414: task sst, steps since last val 14 (total steps = 414): accuracy: 0.8393, sst_loss: 0.4816
03/25 03:39:32 PM: Update 418: task sst, steps since last val 18 (total steps = 418): accuracy: 0.8472, sst_loss: 0.4578
03/25 03:39:44 PM: Update 423: task sst, steps since last val 23 (total steps = 423): accuracy: 0.8804, sst_loss: 0.3676
03/25 03:39:56 PM: Update 428: task sst, steps since last val 28 (total steps = 428): accuracy: 0.8929, sst_loss: 0.3554
03/25 03:40:07 PM: Update 432: task sst, steps since last val 32 (total steps = 432): accuracy: 0.8750, sst_loss: 0.4316
03/25 03:40:19 PM: Update 437: task sst, steps since last val 37 (total steps = 437): accuracy: 0.8716, sst_loss: 0.4257
03/25 03:40:31 PM: Update 442: task sst, steps since last val 42 (total steps = 442): accuracy: 0.8810, sst_loss: 0.4015
03/25 03:40:43 PM: Update 447: task sst, steps since last val 47 (total steps = 447): accuracy: 0.8830, sst_loss: 0.3900
03/25 03:40:53 PM: Update 451: task sst, steps since last val 51 (total steps = 451): accuracy: 0.8922, sst_loss: 0.3631
03/25 03:41:06 PM: Update 456: task sst, steps since last val 56 (total steps = 456): accuracy: 0.8884, sst_loss: 0.3908
03/25 03:41:18 PM: Update 461: task sst, steps since last val 61 (total steps = 461): accuracy: 0.8852, sst_loss: 0.4104
03/25 03:41:30 PM: Update 466: task sst, steps since last val 66 (total steps = 466): accuracy: 0.8864, sst_loss: 0.4146
03/25 03:41:42 PM: Update 471: task sst, steps since last val 71 (total steps = 471): accuracy: 0.8732, sst_loss: 0.4522
03/25 03:41:52 PM: Update 475: task sst, steps since last val 75 (total steps = 475): accuracy: 0.8700, sst_loss: 0.4563
03/25 03:42:02 PM: Update 479: task sst, steps since last val 79 (total steps = 479): accuracy: 0.8544, sst_loss: 0.4744
03/25 03:42:16 PM: Update 483: task sst, steps since last val 83 (total steps = 483): accuracy: 0.8554, sst_loss: 0.4686
03/25 03:42:29 PM: Update 488: task sst, steps since last val 88 (total steps = 488): accuracy: 0.8494, sst_loss: 0.4815
03/25 03:42:41 PM: Update 493: task sst, steps since last val 93 (total steps = 493): accuracy: 0.8522, sst_loss: 0.4675
03/25 03:42:53 PM: Update 498: task sst, steps since last val 98 (total steps = 498): accuracy: 0.8520, sst_loss: 0.4593
03/25 03:42:59 PM: ***** Step 500 / Validation 5 *****
03/25 03:42:59 PM: sst: trained on 100 steps (100 batches) since val, 0.006 epochs
03/25 03:42:59 PM: Validating...
03/25 03:43:04 PM: Evaluate: task sst, batch 28 (218): accuracy: 0.8929, sst_loss: 0.3021
03/25 03:43:14 PM: Evaluate: task sst, batch 82 (218): accuracy: 0.8384, sst_loss: 0.4369
03/25 03:43:24 PM: Evaluate: task sst, batch 134 (218): accuracy: 0.8526, sst_loss: 0.3887
03/25 03:43:34 PM: Evaluate: task sst, batch 189 (218): accuracy: 0.8558, sst_loss: 0.3850
03/25 03:43:40 PM: Updating LR scheduler:
03/25 03:43:40 PM: 	Best result seen so far for macro_avg: 0.872
03/25 03:43:40 PM: 	# validation passes without improvement: 2
03/25 03:43:40 PM: sst_loss: training: 0.457202 validation: 0.391495
03/25 03:43:40 PM: macro_avg: validation: 0.855505
03/25 03:43:40 PM: micro_avg: validation: 0.855505
03/25 03:43:40 PM: sst_accuracy: training: 0.852500 validation: 0.855505
03/25 03:43:40 PM: Global learning rate: 0.0003
03/25 03:43:40 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 03:43:46 PM: Update 502: task sst, steps since last val 2 (total steps = 502): accuracy: 1.0000, sst_loss: 0.1423
03/25 03:43:58 PM: Update 507: task sst, steps since last val 7 (total steps = 507): accuracy: 0.8571, sst_loss: 0.5094
03/25 03:44:10 PM: Update 512: task sst, steps since last val 12 (total steps = 512): accuracy: 0.8958, sst_loss: 0.3633
03/25 03:44:21 PM: Update 515: task sst, steps since last val 15 (total steps = 515): accuracy: 0.8833, sst_loss: 0.3282
03/25 03:44:32 PM: Update 519: task sst, steps since last val 19 (total steps = 519): accuracy: 0.8553, sst_loss: 0.4432
03/25 03:44:44 PM: Update 524: task sst, steps since last val 24 (total steps = 524): accuracy: 0.8542, sst_loss: 0.4148
03/25 03:44:54 PM: Update 528: task sst, steps since last val 28 (total steps = 528): accuracy: 0.8661, sst_loss: 0.3982
03/25 03:45:06 PM: Update 533: task sst, steps since last val 33 (total steps = 533): accuracy: 0.8712, sst_loss: 0.4185
03/25 03:45:18 PM: Update 538: task sst, steps since last val 38 (total steps = 538): accuracy: 0.8816, sst_loss: 0.3980
03/25 03:45:28 PM: Update 542: task sst, steps since last val 42 (total steps = 542): accuracy: 0.8690, sst_loss: 0.4693
03/25 03:45:40 PM: Update 547: task sst, steps since last val 47 (total steps = 547): accuracy: 0.8723, sst_loss: 0.4534
03/25 03:45:53 PM: Update 552: task sst, steps since last val 52 (total steps = 552): accuracy: 0.8654, sst_loss: 0.4822
03/25 03:46:05 PM: Update 557: task sst, steps since last val 57 (total steps = 557): accuracy: 0.8509, sst_loss: 0.5116
03/25 03:46:18 PM: Update 562: task sst, steps since last val 62 (total steps = 562): accuracy: 0.8508, sst_loss: 0.5016
03/25 03:46:28 PM: Update 566: task sst, steps since last val 66 (total steps = 566): accuracy: 0.8523, sst_loss: 0.4923
03/25 03:46:38 PM: Update 570: task sst, steps since last val 70 (total steps = 570): accuracy: 0.8500, sst_loss: 0.4823
03/25 03:46:48 PM: Update 574: task sst, steps since last val 74 (total steps = 574): accuracy: 0.8480, sst_loss: 0.4858
03/25 03:46:58 PM: Update 578: task sst, steps since last val 78 (total steps = 578): accuracy: 0.8462, sst_loss: 0.4858
03/25 03:47:08 PM: Update 582: task sst, steps since last val 82 (total steps = 582): accuracy: 0.8506, sst_loss: 0.4698
03/25 03:47:20 PM: Update 587: task sst, steps since last val 87 (total steps = 587): accuracy: 0.8448, sst_loss: 0.4911
03/25 03:47:32 PM: Update 592: task sst, steps since last val 92 (total steps = 592): accuracy: 0.8424, sst_loss: 0.5008
03/25 03:47:45 PM: Update 597: task sst, steps since last val 97 (total steps = 597): accuracy: 0.8479, sst_loss: 0.4810
03/25 03:47:52 PM: ***** Step 600 / Validation 6 *****
03/25 03:47:52 PM: sst: trained on 100 steps (100 batches) since val, 0.006 epochs
03/25 03:47:52 PM: Validating...
03/25 03:47:55 PM: Evaluate: task sst, batch 15 (218): accuracy: 0.8500, sst_loss: 0.5123
03/25 03:48:05 PM: Evaluate: task sst, batch 68 (218): accuracy: 0.8529, sst_loss: 0.4712
03/25 03:48:15 PM: Evaluate: task sst, batch 120 (218): accuracy: 0.8812, sst_loss: 0.3926
03/25 03:48:25 PM: Evaluate: task sst, batch 175 (218): accuracy: 0.8757, sst_loss: 0.4027
03/25 03:48:33 PM: Best result seen so far for sst.
03/25 03:48:33 PM: Best result seen so far for micro.
03/25 03:48:33 PM: Best result seen so far for macro.
03/25 03:48:33 PM: Updating LR scheduler:
03/25 03:48:33 PM: 	Best result seen so far for macro_avg: 0.876
03/25 03:48:33 PM: 	# validation passes without improvement: 0
03/25 03:48:33 PM: sst_loss: training: 0.483909 validation: 0.405630
03/25 03:48:33 PM: macro_avg: validation: 0.876147
03/25 03:48:33 PM: micro_avg: validation: 0.876147
03/25 03:48:33 PM: sst_accuracy: training: 0.847500 validation: 0.876147
03/25 03:48:33 PM: Global learning rate: 0.0003
03/25 03:48:33 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 03:48:37 PM: Update 601: task sst, steps since last val 1 (total steps = 601): accuracy: 1.0000, sst_loss: 0.0382
03/25 03:48:49 PM: Update 606: task sst, steps since last val 6 (total steps = 606): accuracy: 0.8333, sst_loss: 0.5095
03/25 03:49:01 PM: Update 611: task sst, steps since last val 11 (total steps = 611): accuracy: 0.8636, sst_loss: 0.4082
03/25 03:49:13 PM: Update 616: task sst, steps since last val 16 (total steps = 616): accuracy: 0.8594, sst_loss: 0.4113
03/25 03:49:25 PM: Update 621: task sst, steps since last val 21 (total steps = 621): accuracy: 0.8810, sst_loss: 0.3449
03/25 03:49:35 PM: Update 625: task sst, steps since last val 25 (total steps = 625): accuracy: 0.8600, sst_loss: 0.3988
03/25 03:49:47 PM: Update 630: task sst, steps since last val 30 (total steps = 630): accuracy: 0.8667, sst_loss: 0.3694
03/25 03:49:59 PM: Update 635: task sst, steps since last val 35 (total steps = 635): accuracy: 0.8714, sst_loss: 0.3633
03/25 03:50:12 PM: Update 640: task sst, steps since last val 40 (total steps = 640): accuracy: 0.8688, sst_loss: 0.3555
03/25 03:50:22 PM: Update 644: task sst, steps since last val 44 (total steps = 644): accuracy: 0.8693, sst_loss: 0.3666
03/25 03:50:34 PM: Update 649: task sst, steps since last val 49 (total steps = 649): accuracy: 0.8724, sst_loss: 0.3688
03/25 03:50:46 PM: Update 654: task sst, steps since last val 54 (total steps = 654): accuracy: 0.8750, sst_loss: 0.3743
03/25 03:51:00 PM: Update 658: task sst, steps since last val 58 (total steps = 658): accuracy: 0.8793, sst_loss: 0.3699
03/25 03:51:12 PM: Update 663: task sst, steps since last val 63 (total steps = 663): accuracy: 0.8810, sst_loss: 0.3605
03/25 03:51:25 PM: Update 668: task sst, steps since last val 68 (total steps = 668): accuracy: 0.8750, sst_loss: 0.3773
03/25 03:51:37 PM: Update 673: task sst, steps since last val 73 (total steps = 673): accuracy: 0.8733, sst_loss: 0.3864
03/25 03:51:49 PM: Update 678: task sst, steps since last val 78 (total steps = 678): accuracy: 0.8782, sst_loss: 0.3693
03/25 03:52:00 PM: Update 681: task sst, steps since last val 81 (total steps = 681): accuracy: 0.8765, sst_loss: 0.3761
03/25 03:52:12 PM: Update 686: task sst, steps since last val 86 (total steps = 686): accuracy: 0.8779, sst_loss: 0.3858
03/25 03:52:25 PM: Update 691: task sst, steps since last val 91 (total steps = 691): accuracy: 0.8791, sst_loss: 0.3815
03/25 03:52:35 PM: Update 695: task sst, steps since last val 95 (total steps = 695): accuracy: 0.8737, sst_loss: 0.3873
03/25 03:52:47 PM: Update 700: task sst, steps since last val 100 (total steps = 700): accuracy: 0.8775, sst_loss: 0.3889
03/25 03:52:47 PM: ***** Step 700 / Validation 7 *****
03/25 03:52:47 PM: sst: trained on 100 steps (100 batches) since val, 0.006 epochs
03/25 03:52:47 PM: Validating...
03/25 03:52:57 PM: Evaluate: task sst, batch 55 (218): accuracy: 0.8545, sst_loss: 0.4097
03/25 03:53:07 PM: Evaluate: task sst, batch 109 (218): accuracy: 0.8624, sst_loss: 0.4348
03/25 03:53:18 PM: Evaluate: task sst, batch 162 (218): accuracy: 0.8673, sst_loss: 0.4207
03/25 03:53:28 PM: Evaluate: task sst, batch 214 (218): accuracy: 0.8645, sst_loss: 0.4291
03/25 03:53:28 PM: Updating LR scheduler:
03/25 03:53:28 PM: 	Best result seen so far for macro_avg: 0.876
03/25 03:53:28 PM: 	# validation passes without improvement: 1
03/25 03:53:28 PM: sst_loss: training: 0.388893 validation: 0.429901
03/25 03:53:28 PM: macro_avg: validation: 0.862385
03/25 03:53:28 PM: micro_avg: validation: 0.862385
03/25 03:53:28 PM: sst_accuracy: training: 0.877500 validation: 0.862385
03/25 03:53:28 PM: Global learning rate: 0.0003
03/25 03:53:28 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 03:53:39 PM: Update 704: task sst, steps since last val 4 (total steps = 704): accuracy: 0.5625, sst_loss: 1.1419
03/25 03:53:49 PM: Update 708: task sst, steps since last val 8 (total steps = 708): accuracy: 0.7188, sst_loss: 0.7286
03/25 03:54:00 PM: Update 712: task sst, steps since last val 12 (total steps = 712): accuracy: 0.7708, sst_loss: 0.6073
03/25 03:54:10 PM: Update 716: task sst, steps since last val 16 (total steps = 716): accuracy: 0.7969, sst_loss: 0.5713
03/25 03:54:22 PM: Update 721: task sst, steps since last val 21 (total steps = 721): accuracy: 0.8333, sst_loss: 0.4609
03/25 03:54:38 PM: Update 726: task sst, steps since last val 26 (total steps = 726): accuracy: 0.8365, sst_loss: 0.4320
03/25 03:54:50 PM: Update 731: task sst, steps since last val 31 (total steps = 731): accuracy: 0.8548, sst_loss: 0.3897
03/25 03:55:02 PM: Update 736: task sst, steps since last val 36 (total steps = 736): accuracy: 0.8611, sst_loss: 0.3731
03/25 03:55:12 PM: Update 740: task sst, steps since last val 40 (total steps = 740): accuracy: 0.8688, sst_loss: 0.3538
03/25 03:55:25 PM: Update 745: task sst, steps since last val 45 (total steps = 745): accuracy: 0.8556, sst_loss: 0.3907
03/25 03:55:37 PM: Update 750: task sst, steps since last val 50 (total steps = 750): accuracy: 0.8550, sst_loss: 0.4048
03/25 03:55:49 PM: Update 755: task sst, steps since last val 55 (total steps = 755): accuracy: 0.8500, sst_loss: 0.4286
03/25 03:55:59 PM: Update 759: task sst, steps since last val 59 (total steps = 759): accuracy: 0.8432, sst_loss: 0.4643
03/25 03:56:12 PM: Update 764: task sst, steps since last val 64 (total steps = 764): accuracy: 0.8398, sst_loss: 0.4633
03/25 03:56:24 PM: Update 769: task sst, steps since last val 69 (total steps = 769): accuracy: 0.8442, sst_loss: 0.4514
03/25 03:56:34 PM: Update 773: task sst, steps since last val 73 (total steps = 773): accuracy: 0.8493, sst_loss: 0.4353
03/25 03:56:44 PM: Update 776: task sst, steps since last val 76 (total steps = 776): accuracy: 0.8487, sst_loss: 0.4300
03/25 03:56:57 PM: Update 781: task sst, steps since last val 81 (total steps = 781): accuracy: 0.8457, sst_loss: 0.4195
03/25 03:57:07 PM: Update 785: task sst, steps since last val 85 (total steps = 785): accuracy: 0.8471, sst_loss: 0.4193
03/25 03:57:19 PM: Update 790: task sst, steps since last val 90 (total steps = 790): accuracy: 0.8500, sst_loss: 0.4033
03/25 03:57:31 PM: Update 795: task sst, steps since last val 95 (total steps = 795): accuracy: 0.8579, sst_loss: 0.3845
03/25 03:57:43 PM: Update 800: task sst, steps since last val 100 (total steps = 800): accuracy: 0.8600, sst_loss: 0.3939
03/25 03:57:43 PM: ***** Step 800 / Validation 8 *****
03/25 03:57:43 PM: sst: trained on 100 steps (100 batches) since val, 0.006 epochs
03/25 03:57:43 PM: Validating...
03/25 03:57:54 PM: Evaluate: task sst, batch 55 (218): accuracy: 0.8773, sst_loss: 0.5483
03/25 03:58:04 PM: Evaluate: task sst, batch 109 (218): accuracy: 0.8784, sst_loss: 0.5318
03/25 03:58:14 PM: Evaluate: task sst, batch 162 (218): accuracy: 0.8858, sst_loss: 0.5004
03/25 03:58:24 PM: Evaluate: task sst, batch 214 (218): accuracy: 0.8738, sst_loss: 0.5436
03/25 03:58:25 PM: Updating LR scheduler:
03/25 03:58:25 PM: 	Best result seen so far for macro_avg: 0.876
03/25 03:58:25 PM: 	# validation passes without improvement: 2
03/25 03:58:25 PM: sst_loss: training: 0.393892 validation: 0.547979
03/25 03:58:25 PM: macro_avg: validation: 0.872706
03/25 03:58:25 PM: micro_avg: validation: 0.872706
03/25 03:58:25 PM: sst_accuracy: training: 0.860000 validation: 0.872706
03/25 03:58:25 PM: Global learning rate: 0.0003
03/25 03:58:25 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 03:58:35 PM: Update 804: task sst, steps since last val 4 (total steps = 804): accuracy: 0.9375, sst_loss: 0.2058
03/25 03:58:47 PM: Update 809: task sst, steps since last val 9 (total steps = 809): accuracy: 0.8889, sst_loss: 0.5020
03/25 03:58:59 PM: Update 814: task sst, steps since last val 14 (total steps = 814): accuracy: 0.8929, sst_loss: 0.4848
03/25 03:59:12 PM: Update 819: task sst, steps since last val 19 (total steps = 819): accuracy: 0.8947, sst_loss: 0.4587
03/25 03:59:22 PM: Update 823: task sst, steps since last val 23 (total steps = 823): accuracy: 0.8913, sst_loss: 0.4609
03/25 03:59:33 PM: Update 826: task sst, steps since last val 26 (total steps = 826): accuracy: 0.8942, sst_loss: 0.4393
03/25 03:59:45 PM: Update 831: task sst, steps since last val 31 (total steps = 831): accuracy: 0.8790, sst_loss: 0.4736
03/25 03:59:57 PM: Update 836: task sst, steps since last val 36 (total steps = 836): accuracy: 0.8750, sst_loss: 0.4895
03/25 04:00:09 PM: Update 841: task sst, steps since last val 41 (total steps = 841): accuracy: 0.8841, sst_loss: 0.4453
03/25 04:00:21 PM: Update 846: task sst, steps since last val 46 (total steps = 846): accuracy: 0.8696, sst_loss: 0.4548
03/25 04:00:33 PM: Update 851: task sst, steps since last val 51 (total steps = 851): accuracy: 0.8775, sst_loss: 0.4251
03/25 04:00:45 PM: Update 856: task sst, steps since last val 56 (total steps = 856): accuracy: 0.8839, sst_loss: 0.4085
03/25 04:00:56 PM: Update 860: task sst, steps since last val 60 (total steps = 860): accuracy: 0.8875, sst_loss: 0.3997
03/25 04:01:08 PM: Update 865: task sst, steps since last val 65 (total steps = 865): accuracy: 0.8692, sst_loss: 0.4475
03/25 04:01:19 PM: Update 868: task sst, steps since last val 68 (total steps = 868): accuracy: 0.8750, sst_loss: 0.4294
03/25 04:01:31 PM: Update 873: task sst, steps since last val 73 (total steps = 873): accuracy: 0.8801, sst_loss: 0.4181
03/25 04:01:43 PM: Update 878: task sst, steps since last val 78 (total steps = 878): accuracy: 0.8814, sst_loss: 0.4174
03/25 04:01:57 PM: Update 882: task sst, steps since last val 82 (total steps = 882): accuracy: 0.8811, sst_loss: 0.4139
03/25 04:02:07 PM: Update 886: task sst, steps since last val 86 (total steps = 886): accuracy: 0.8779, sst_loss: 0.4243
03/25 04:02:19 PM: Update 891: task sst, steps since last val 91 (total steps = 891): accuracy: 0.8764, sst_loss: 0.4163
03/25 04:02:31 PM: Update 896: task sst, steps since last val 96 (total steps = 896): accuracy: 0.8724, sst_loss: 0.4214
03/25 04:02:41 PM: ***** Step 900 / Validation 9 *****
03/25 04:02:41 PM: sst: trained on 100 steps (100 batches) since val, 0.006 epochs
03/25 04:02:41 PM: Validating...
03/25 04:02:41 PM: Evaluate: task sst, batch 2 (218): accuracy: 1.0000, sst_loss: 0.0456
03/25 04:02:51 PM: Evaluate: task sst, batch 55 (218): accuracy: 0.8545, sst_loss: 0.3766
03/25 04:03:02 PM: Evaluate: task sst, batch 108 (218): accuracy: 0.8634, sst_loss: 0.3727
03/25 04:03:12 PM: Evaluate: task sst, batch 161 (218): accuracy: 0.8742, sst_loss: 0.3566
03/25 04:03:22 PM: Evaluate: task sst, batch 212 (218): accuracy: 0.8620, sst_loss: 0.3700
03/25 04:03:23 PM: Updating LR scheduler:
03/25 04:03:23 PM: 	Best result seen so far for macro_avg: 0.876
03/25 04:03:23 PM: 	# validation passes without improvement: 0
03/25 04:03:23 PM: Ran out of early stopping patience. Stopping training.
03/25 04:03:23 PM: sst_loss: training: 0.417470 validation: 0.371125
03/25 04:03:23 PM: macro_avg: validation: 0.862385
03/25 04:03:23 PM: micro_avg: validation: 0.862385
03/25 04:03:23 PM: sst_accuracy: training: 0.872500 validation: 0.862385
03/25 04:03:23 PM: Global learning rate: 0.00015
03/25 04:03:23 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 04:03:24 PM: Stopped training after 9 validation checks
03/25 04:03:24 PM: Trained sst for 900 steps or 0.053 epochs
03/25 04:03:24 PM: ***** VALIDATION RESULTS *****
03/25 04:03:24 PM: sst_accuracy (for best val pass 6): sst_loss: 0.40563, macro_avg: 0.87615, micro_avg: 0.87615, sst_accuracy: 0.87615
03/25 04:03:24 PM: micro_avg (for best val pass 6): sst_loss: 0.40563, macro_avg: 0.87615, micro_avg: 0.87615, sst_accuracy: 0.87615
03/25 04:03:24 PM: macro_avg (for best val pass 6): sst_loss: 0.40563, macro_avg: 0.87615, micro_avg: 0.87615, sst_accuracy: 0.87615
03/25 04:03:24 PM: Not loading task-specific parameters for task: multirc
03/25 04:03:24 PM: Loaded model state from /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc/model_state_pretrain_val_6.best.th
03/25 04:03:24 PM: patience = 2
03/25 04:03:24 PM: val_interval = 1
03/25 04:03:24 PM: max_vals = 10000
03/25 04:03:24 PM: cuda_device = -1
03/25 04:03:24 PM: grad_norm = 5.0
03/25 04:03:24 PM: grad_clipping = None
03/25 04:03:24 PM: lr_decay = 0.99
03/25 04:03:24 PM: min_lr = 1e-06
03/25 04:03:24 PM: keep_all_checkpoints = 0
03/25 04:03:24 PM: val_data_limit = 5000
03/25 04:03:24 PM: max_epochs = 10
03/25 04:03:24 PM: dec_val_scale = 250
03/25 04:03:24 PM: training_data_fraction = 1
03/25 04:03:24 PM: accumulation_steps = 1
03/25 04:03:24 PM: type = bert_adam
03/25 04:03:24 PM: parameter_groups = None
03/25 04:03:24 PM: Number of trainable parameters: 109485316
03/25 04:03:24 PM: infer_type_and_cast = True
03/25 04:03:24 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/25 04:03:24 PM: CURRENTLY DEFINED PARAMETERS: 
03/25 04:03:24 PM: lr = 0.0003
03/25 04:03:24 PM: t_total = 10000
03/25 04:03:24 PM: warmup = 0.1
03/25 04:03:24 PM: type = reduce_on_plateau
03/25 04:03:24 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/25 04:03:24 PM: CURRENTLY DEFINED PARAMETERS: 
03/25 04:03:24 PM: mode = max
03/25 04:03:24 PM: factor = 0.5
03/25 04:03:24 PM: patience = 2
03/25 04:03:24 PM: threshold = 0.0001
03/25 04:03:24 PM: threshold_mode = abs
03/25 04:03:24 PM: verbose = True
03/25 04:03:24 PM: Starting training without restoring from a checkpoint.
03/25 04:03:24 PM: Training examples per task, before any subsampling: {'multirc': 27243}
03/25 04:03:24 PM: Beginning training with stopping criteria based on metric: multirc_avg
03/25 04:03:34 PM: ***** Step 1 / Validation 1 *****
03/25 04:03:34 PM: multirc: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 04:03:34 PM: Validating...
03/25 04:03:35 PM: Evaluate: task multirc, batch 1 (1212): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.7105
03/25 04:03:45 PM: Evaluate: task multirc, batch 14 (1212): ans_f1: 0.4923, qst_f1: 0.3929, em: 0.0000, avg: 0.2462, multirc_loss: 0.7115
03/25 04:03:56 PM: Evaluate: task multirc, batch 27 (1212): ans_f1: 0.5401, qst_f1: 0.4732, em: 0.0417, avg: 0.2909, multirc_loss: 0.7134
03/25 04:04:06 PM: Evaluate: task multirc, batch 40 (1212): ans_f1: 0.5279, qst_f1: 0.4660, em: 0.0270, avg: 0.2775, multirc_loss: 0.7090
03/25 04:04:17 PM: Evaluate: task multirc, batch 53 (1212): ans_f1: 0.5462, qst_f1: 0.4859, em: 0.0400, avg: 0.2931, multirc_loss: 0.7073
03/25 04:04:27 PM: Evaluate: task multirc, batch 66 (1212): ans_f1: 0.5740, qst_f1: 0.5364, em: 0.0317, avg: 0.3029, multirc_loss: 0.7068
03/25 04:04:38 PM: Evaluate: task multirc, batch 79 (1212): ans_f1: 0.5913, qst_f1: 0.5496, em: 0.0278, avg: 0.3096, multirc_loss: 0.7066
03/25 04:04:48 PM: Evaluate: task multirc, batch 93 (1212): ans_f1: 0.5907, qst_f1: 0.5258, em: 0.0241, avg: 0.3074, multirc_loss: 0.7029
03/25 04:04:59 PM: Evaluate: task multirc, batch 107 (1212): ans_f1: 0.5529, qst_f1: 0.4581, em: 0.0208, avg: 0.2869, multirc_loss: 0.7043
03/25 04:05:10 PM: Evaluate: task multirc, batch 121 (1212): ans_f1: 0.5331, qst_f1: 0.4394, em: 0.0190, avg: 0.2761, multirc_loss: 0.7044
03/25 04:05:20 PM: Evaluate: task multirc, batch 134 (1212): ans_f1: 0.5344, qst_f1: 0.4323, em: 0.0174, avg: 0.2759, multirc_loss: 0.7035
03/25 04:05:31 PM: Evaluate: task multirc, batch 147 (1212): ans_f1: 0.5324, qst_f1: 0.4435, em: 0.0159, avg: 0.2741, multirc_loss: 0.7064
03/25 04:05:41 PM: Evaluate: task multirc, batch 159 (1212): ans_f1: 0.5315, qst_f1: 0.4499, em: 0.0150, avg: 0.2733, multirc_loss: 0.7068
03/25 04:05:52 PM: Evaluate: task multirc, batch 173 (1212): ans_f1: 0.5364, qst_f1: 0.4536, em: 0.0141, avg: 0.2752, multirc_loss: 0.7070
03/25 04:06:02 PM: Evaluate: task multirc, batch 186 (1212): ans_f1: 0.5353, qst_f1: 0.4567, em: 0.0134, avg: 0.2743, multirc_loss: 0.7076
03/25 04:06:12 PM: Evaluate: task multirc, batch 199 (1212): ans_f1: 0.5268, qst_f1: 0.4515, em: 0.0127, avg: 0.2697, multirc_loss: 0.7090
03/25 04:06:23 PM: Evaluate: task multirc, batch 213 (1212): ans_f1: 0.5226, qst_f1: 0.4468, em: 0.0121, avg: 0.2674, multirc_loss: 0.7096
03/25 04:06:33 PM: Evaluate: task multirc, batch 226 (1212): ans_f1: 0.5137, qst_f1: 0.4362, em: 0.0173, avg: 0.2655, multirc_loss: 0.7082
03/25 04:06:44 PM: Evaluate: task multirc, batch 240 (1212): ans_f1: 0.5155, qst_f1: 0.4405, em: 0.0108, avg: 0.2632, multirc_loss: 0.7081
03/25 04:06:54 PM: Evaluate: task multirc, batch 253 (1212): ans_f1: 0.5142, qst_f1: 0.4435, em: 0.0102, avg: 0.2622, multirc_loss: 0.7086
03/25 04:07:05 PM: Evaluate: task multirc, batch 267 (1212): ans_f1: 0.5114, qst_f1: 0.4434, em: 0.0096, avg: 0.2605, multirc_loss: 0.7105
03/25 04:07:15 PM: Evaluate: task multirc, batch 281 (1212): ans_f1: 0.5090, qst_f1: 0.4455, em: 0.0092, avg: 0.2591, multirc_loss: 0.7120
03/25 04:07:25 PM: Evaluate: task multirc, batch 295 (1212): ans_f1: 0.5081, qst_f1: 0.4457, em: 0.0088, avg: 0.2585, multirc_loss: 0.7136
03/25 04:07:36 PM: Evaluate: task multirc, batch 309 (1212): ans_f1: 0.5114, qst_f1: 0.4485, em: 0.0085, avg: 0.2599, multirc_loss: 0.7133
03/25 04:07:47 PM: Evaluate: task multirc, batch 323 (1212): ans_f1: 0.5159, qst_f1: 0.4509, em: 0.0082, avg: 0.2621, multirc_loss: 0.7137
03/25 04:07:58 PM: Evaluate: task multirc, batch 337 (1212): ans_f1: 0.5182, qst_f1: 0.4561, em: 0.0079, avg: 0.2630, multirc_loss: 0.7139
03/25 04:08:08 PM: Evaluate: task multirc, batch 351 (1212): ans_f1: 0.5215, qst_f1: 0.4584, em: 0.0075, avg: 0.2645, multirc_loss: 0.7134
03/25 04:08:19 PM: Evaluate: task multirc, batch 365 (1212): ans_f1: 0.5282, qst_f1: 0.4652, em: 0.0072, avg: 0.2677, multirc_loss: 0.7128
03/25 04:08:29 PM: Evaluate: task multirc, batch 378 (1212): ans_f1: 0.5328, qst_f1: 0.4706, em: 0.0103, avg: 0.2716, multirc_loss: 0.7122
03/25 04:08:40 PM: Evaluate: task multirc, batch 391 (1212): ans_f1: 0.5344, qst_f1: 0.4718, em: 0.0067, avg: 0.2705, multirc_loss: 0.7124
03/25 04:08:50 PM: Evaluate: task multirc, batch 404 (1212): ans_f1: 0.5301, qst_f1: 0.4671, em: 0.0064, avg: 0.2682, multirc_loss: 0.7128
03/25 04:09:01 PM: Evaluate: task multirc, batch 418 (1212): ans_f1: 0.5264, qst_f1: 0.4655, em: 0.0062, avg: 0.2663, multirc_loss: 0.7153
03/25 04:09:11 PM: Evaluate: task multirc, batch 431 (1212): ans_f1: 0.5242, qst_f1: 0.4596, em: 0.0090, avg: 0.2666, multirc_loss: 0.7138
03/25 04:09:21 PM: Evaluate: task multirc, batch 444 (1212): ans_f1: 0.5209, qst_f1: 0.4547, em: 0.0087, avg: 0.2648, multirc_loss: 0.7138
03/25 04:09:32 PM: Evaluate: task multirc, batch 457 (1212): ans_f1: 0.5213, qst_f1: 0.4564, em: 0.0085, avg: 0.2649, multirc_loss: 0.7145
03/25 04:09:42 PM: Evaluate: task multirc, batch 470 (1212): ans_f1: 0.5258, qst_f1: 0.4647, em: 0.0082, avg: 0.2670, multirc_loss: 0.7144
03/25 04:09:52 PM: Evaluate: task multirc, batch 483 (1212): ans_f1: 0.5227, qst_f1: 0.4554, em: 0.0079, avg: 0.2653, multirc_loss: 0.7131
03/25 04:10:03 PM: Evaluate: task multirc, batch 497 (1212): ans_f1: 0.5197, qst_f1: 0.4477, em: 0.0103, avg: 0.2650, multirc_loss: 0.7118
03/25 04:10:14 PM: Evaluate: task multirc, batch 511 (1212): ans_f1: 0.5166, qst_f1: 0.4432, em: 0.0100, avg: 0.2633, multirc_loss: 0.7115
03/25 04:10:25 PM: Evaluate: task multirc, batch 525 (1212): ans_f1: 0.5235, qst_f1: 0.4532, em: 0.0096, avg: 0.2665, multirc_loss: 0.7111
03/25 04:10:35 PM: Evaluate: task multirc, batch 538 (1212): ans_f1: 0.5242, qst_f1: 0.4550, em: 0.0117, avg: 0.2680, multirc_loss: 0.7115
03/25 04:10:46 PM: Evaluate: task multirc, batch 551 (1212): ans_f1: 0.5258, qst_f1: 0.4570, em: 0.0092, avg: 0.2675, multirc_loss: 0.7118
03/25 04:10:57 PM: Evaluate: task multirc, batch 564 (1212): ans_f1: 0.5266, qst_f1: 0.4575, em: 0.0090, avg: 0.2678, multirc_loss: 0.7121
03/25 04:11:08 PM: Evaluate: task multirc, batch 578 (1212): ans_f1: 0.5262, qst_f1: 0.4537, em: 0.0088, avg: 0.2675, multirc_loss: 0.7114
03/25 04:11:19 PM: Evaluate: task multirc, batch 592 (1212): ans_f1: 0.5227, qst_f1: 0.4502, em: 0.0086, avg: 0.2657, multirc_loss: 0.7112
03/25 04:11:29 PM: Evaluate: task multirc, batch 605 (1212): ans_f1: 0.5220, qst_f1: 0.4480, em: 0.0084, avg: 0.2652, multirc_loss: 0.7110
03/25 04:11:40 PM: Evaluate: task multirc, batch 619 (1212): ans_f1: 0.5186, qst_f1: 0.4416, em: 0.0083, avg: 0.2635, multirc_loss: 0.7104
03/25 04:11:51 PM: Evaluate: task multirc, batch 633 (1212): ans_f1: 0.5182, qst_f1: 0.4371, em: 0.0081, avg: 0.2631, multirc_loss: 0.7099
03/25 04:12:01 PM: Evaluate: task multirc, batch 647 (1212): ans_f1: 0.5212, qst_f1: 0.4371, em: 0.0079, avg: 0.2645, multirc_loss: 0.7093
03/25 04:12:12 PM: Evaluate: task multirc, batch 661 (1212): ans_f1: 0.5242, qst_f1: 0.4413, em: 0.0077, avg: 0.2660, multirc_loss: 0.7091
03/25 04:12:23 PM: Evaluate: task multirc, batch 675 (1212): ans_f1: 0.5257, qst_f1: 0.4441, em: 0.0075, avg: 0.2666, multirc_loss: 0.7094
03/25 04:12:33 PM: Evaluate: task multirc, batch 687 (1212): ans_f1: 0.5247, qst_f1: 0.4442, em: 0.0074, avg: 0.2661, multirc_loss: 0.7100
03/25 04:12:44 PM: Evaluate: task multirc, batch 700 (1212): ans_f1: 0.5276, qst_f1: 0.4484, em: 0.0091, avg: 0.2684, multirc_loss: 0.7100
03/25 04:12:54 PM: Evaluate: task multirc, batch 712 (1212): ans_f1: 0.5341, qst_f1: 0.4544, em: 0.0072, avg: 0.2706, multirc_loss: 0.7093
03/25 04:13:05 PM: Evaluate: task multirc, batch 725 (1212): ans_f1: 0.5346, qst_f1: 0.4553, em: 0.0071, avg: 0.2709, multirc_loss: 0.7088
03/25 04:13:15 PM: Evaluate: task multirc, batch 738 (1212): ans_f1: 0.5312, qst_f1: 0.4491, em: 0.0070, avg: 0.2691, multirc_loss: 0.7085
03/25 04:13:26 PM: Evaluate: task multirc, batch 752 (1212): ans_f1: 0.5307, qst_f1: 0.4475, em: 0.0068, avg: 0.2688, multirc_loss: 0.7084
03/25 04:13:37 PM: Evaluate: task multirc, batch 765 (1212): ans_f1: 0.5320, qst_f1: 0.4496, em: 0.0067, avg: 0.2694, multirc_loss: 0.7084
03/25 04:13:48 PM: Evaluate: task multirc, batch 779 (1212): ans_f1: 0.5355, qst_f1: 0.4537, em: 0.0082, avg: 0.2719, multirc_loss: 0.7081
03/25 04:13:58 PM: Evaluate: task multirc, batch 792 (1212): ans_f1: 0.5367, qst_f1: 0.4548, em: 0.0096, avg: 0.2732, multirc_loss: 0.7078
03/25 04:14:09 PM: Evaluate: task multirc, batch 806 (1212): ans_f1: 0.5385, qst_f1: 0.4564, em: 0.0094, avg: 0.2740, multirc_loss: 0.7072
03/25 04:14:20 PM: Evaluate: task multirc, batch 820 (1212): ans_f1: 0.5365, qst_f1: 0.4494, em: 0.0077, avg: 0.2721, multirc_loss: 0.7069
03/25 04:14:31 PM: Evaluate: task multirc, batch 834 (1212): ans_f1: 0.5365, qst_f1: 0.4487, em: 0.0091, avg: 0.2728, multirc_loss: 0.7066
03/25 04:14:42 PM: Evaluate: task multirc, batch 848 (1212): ans_f1: 0.5363, qst_f1: 0.4495, em: 0.0089, avg: 0.2726, multirc_loss: 0.7066
03/25 04:14:53 PM: Evaluate: task multirc, batch 862 (1212): ans_f1: 0.5368, qst_f1: 0.4498, em: 0.0087, avg: 0.2727, multirc_loss: 0.7065
03/25 04:15:04 PM: Evaluate: task multirc, batch 876 (1212): ans_f1: 0.5362, qst_f1: 0.4493, em: 0.0085, avg: 0.2724, multirc_loss: 0.7066
03/25 04:15:15 PM: Evaluate: task multirc, batch 890 (1212): ans_f1: 0.5328, qst_f1: 0.4434, em: 0.0084, avg: 0.2706, multirc_loss: 0.7065
03/25 04:15:26 PM: Evaluate: task multirc, batch 904 (1212): ans_f1: 0.5330, qst_f1: 0.4448, em: 0.0082, avg: 0.2706, multirc_loss: 0.7065
03/25 04:15:37 PM: Evaluate: task multirc, batch 917 (1212): ans_f1: 0.5320, qst_f1: 0.4451, em: 0.0081, avg: 0.2701, multirc_loss: 0.7071
03/25 04:15:48 PM: Evaluate: task multirc, batch 931 (1212): ans_f1: 0.5323, qst_f1: 0.4472, em: 0.0079, avg: 0.2701, multirc_loss: 0.7071
03/25 04:15:58 PM: Evaluate: task multirc, batch 945 (1212): ans_f1: 0.5315, qst_f1: 0.4442, em: 0.0092, avg: 0.2703, multirc_loss: 0.7068
03/25 04:16:09 PM: Evaluate: task multirc, batch 957 (1212): ans_f1: 0.5294, qst_f1: 0.4410, em: 0.0091, avg: 0.2692, multirc_loss: 0.7065
03/25 04:16:20 PM: Evaluate: task multirc, batch 971 (1212): ans_f1: 0.5283, qst_f1: 0.4403, em: 0.0090, avg: 0.2686, multirc_loss: 0.7066
03/25 04:16:31 PM: Evaluate: task multirc, batch 985 (1212): ans_f1: 0.5278, qst_f1: 0.4408, em: 0.0101, avg: 0.2690, multirc_loss: 0.7070
03/25 04:16:42 PM: Evaluate: task multirc, batch 998 (1212): ans_f1: 0.5283, qst_f1: 0.4413, em: 0.0101, avg: 0.2692, multirc_loss: 0.7068
03/25 04:16:53 PM: Evaluate: task multirc, batch 1012 (1212): ans_f1: 0.5278, qst_f1: 0.4407, em: 0.0099, avg: 0.2689, multirc_loss: 0.7067
03/25 04:17:04 PM: Evaluate: task multirc, batch 1025 (1212): ans_f1: 0.5260, qst_f1: 0.4382, em: 0.0098, avg: 0.2679, multirc_loss: 0.7067
03/25 04:17:14 PM: Evaluate: task multirc, batch 1038 (1212): ans_f1: 0.5259, qst_f1: 0.4379, em: 0.0109, avg: 0.2684, multirc_loss: 0.7067
03/25 04:17:25 PM: Evaluate: task multirc, batch 1052 (1212): ans_f1: 0.5261, qst_f1: 0.4374, em: 0.0108, avg: 0.2684, multirc_loss: 0.7065
03/25 04:17:36 PM: Evaluate: task multirc, batch 1065 (1212): ans_f1: 0.5264, qst_f1: 0.4369, em: 0.0106, avg: 0.2685, multirc_loss: 0.7064
03/25 04:17:47 PM: Evaluate: task multirc, batch 1078 (1212): ans_f1: 0.5271, qst_f1: 0.4373, em: 0.0105, avg: 0.2688, multirc_loss: 0.7064
03/25 04:17:58 PM: Evaluate: task multirc, batch 1091 (1212): ans_f1: 0.5287, qst_f1: 0.4395, em: 0.0104, avg: 0.2695, multirc_loss: 0.7064
03/25 04:18:09 PM: Evaluate: task multirc, batch 1104 (1212): ans_f1: 0.5271, qst_f1: 0.4393, em: 0.0114, avg: 0.2693, multirc_loss: 0.7067
03/25 04:18:20 PM: Evaluate: task multirc, batch 1118 (1212): ans_f1: 0.5280, qst_f1: 0.4400, em: 0.0102, avg: 0.2691, multirc_loss: 0.7066
03/25 04:18:31 PM: Evaluate: task multirc, batch 1132 (1212): ans_f1: 0.5251, qst_f1: 0.4386, em: 0.0101, avg: 0.2676, multirc_loss: 0.7067
03/25 04:18:42 PM: Evaluate: task multirc, batch 1146 (1212): ans_f1: 0.5243, qst_f1: 0.4373, em: 0.0111, avg: 0.2677, multirc_loss: 0.7069
03/25 04:18:53 PM: Evaluate: task multirc, batch 1160 (1212): ans_f1: 0.5227, qst_f1: 0.4346, em: 0.0110, avg: 0.2668, multirc_loss: 0.7067
03/25 04:19:04 PM: Evaluate: task multirc, batch 1173 (1212): ans_f1: 0.5201, qst_f1: 0.4290, em: 0.0108, avg: 0.2655, multirc_loss: 0.7065
03/25 04:19:15 PM: Evaluate: task multirc, batch 1187 (1212): ans_f1: 0.5199, qst_f1: 0.4285, em: 0.0118, avg: 0.2658, multirc_loss: 0.7061
03/25 04:19:26 PM: Evaluate: task multirc, batch 1200 (1212): ans_f1: 0.5213, qst_f1: 0.4295, em: 0.0117, avg: 0.2665, multirc_loss: 0.7060
03/25 04:19:35 PM: Best result seen so far for multirc.
03/25 04:19:35 PM: Best result seen so far for micro.
03/25 04:19:35 PM: Best result seen so far for macro.
03/25 04:19:35 PM: Updating LR scheduler:
03/25 04:19:35 PM: 	Best result seen so far for macro_avg: 0.267
03/25 04:19:35 PM: 	# validation passes without improvement: 0
03/25 04:19:35 PM: multirc_loss: training: 0.596266 validation: 0.705774
03/25 04:19:35 PM: macro_avg: validation: 0.267341
03/25 04:19:35 PM: micro_avg: validation: 0.267341
03/25 04:19:35 PM: multirc_ans_f1: training: 0.857143 validation: 0.523139
03/25 04:19:35 PM: multirc_qst_f1: training: 0.750000 validation: 0.431273
03/25 04:19:35 PM: multirc_em: training: 0.750000 validation: 0.011542
03/25 04:19:35 PM: multirc_avg: training: 0.803571 validation: 0.267341
03/25 04:19:35 PM: Global learning rate: 0.0003
03/25 04:19:35 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 04:19:42 PM: Update 2: task multirc, steps since last val 1 (total steps = 2): ans_f1: 0.6667, qst_f1: 0.5000, em: 0.5000, avg: 0.5833, multirc_loss: 0.6228
03/25 04:19:42 PM: ***** Step 2 / Validation 2 *****
03/25 04:19:42 PM: multirc: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 04:19:42 PM: Validating...
03/25 04:19:52 PM: Evaluate: task multirc, batch 13 (1212): ans_f1: 0.5246, qst_f1: 0.4649, em: 0.0000, avg: 0.2623, multirc_loss: 0.7113
03/25 04:20:03 PM: Evaluate: task multirc, batch 26 (1212): ans_f1: 0.5564, qst_f1: 0.4894, em: 0.0435, avg: 0.2999, multirc_loss: 0.7160
03/25 04:20:14 PM: Evaluate: task multirc, batch 39 (1212): ans_f1: 0.5628, qst_f1: 0.5208, em: 0.0278, avg: 0.2953, multirc_loss: 0.7108
03/25 04:20:24 PM: Evaluate: task multirc, batch 52 (1212): ans_f1: 0.5735, qst_f1: 0.5412, em: 0.0417, avg: 0.3076, multirc_loss: 0.7087
03/25 04:20:35 PM: Evaluate: task multirc, batch 65 (1212): ans_f1: 0.5983, qst_f1: 0.5786, em: 0.0323, avg: 0.3153, multirc_loss: 0.7070
03/25 04:20:45 PM: Evaluate: task multirc, batch 78 (1212): ans_f1: 0.6075, qst_f1: 0.5829, em: 0.0282, avg: 0.3178, multirc_loss: 0.7075
03/25 04:20:55 PM: Evaluate: task multirc, batch 92 (1212): ans_f1: 0.6082, qst_f1: 0.5625, em: 0.0244, avg: 0.3163, multirc_loss: 0.7042
03/25 04:21:06 PM: Evaluate: task multirc, batch 106 (1212): ans_f1: 0.5714, qst_f1: 0.4897, em: 0.0211, avg: 0.2962, multirc_loss: 0.7046
03/25 04:21:16 PM: Evaluate: task multirc, batch 120 (1212): ans_f1: 0.5461, qst_f1: 0.4623, em: 0.0190, avg: 0.2826, multirc_loss: 0.7050
03/25 04:21:27 PM: Evaluate: task multirc, batch 133 (1212): ans_f1: 0.5469, qst_f1: 0.4660, em: 0.0175, avg: 0.2822, multirc_loss: 0.7043
03/25 04:21:38 PM: Evaluate: task multirc, batch 146 (1212): ans_f1: 0.5436, qst_f1: 0.4723, em: 0.0161, avg: 0.2799, multirc_loss: 0.7076
03/25 04:21:48 PM: Evaluate: task multirc, batch 158 (1212): ans_f1: 0.5419, qst_f1: 0.4781, em: 0.0226, avg: 0.2822, multirc_loss: 0.7083
03/25 04:21:58 PM: Evaluate: task multirc, batch 171 (1212): ans_f1: 0.5465, qst_f1: 0.4815, em: 0.0143, avg: 0.2804, multirc_loss: 0.7083
03/25 04:22:09 PM: Evaluate: task multirc, batch 185 (1212): ans_f1: 0.5480, qst_f1: 0.4837, em: 0.0134, avg: 0.2807, multirc_loss: 0.7089
03/25 04:22:19 PM: Evaluate: task multirc, batch 198 (1212): ans_f1: 0.5399, qst_f1: 0.4800, em: 0.0128, avg: 0.2763, multirc_loss: 0.7105
03/25 04:22:29 PM: Evaluate: task multirc, batch 211 (1212): ans_f1: 0.5369, qst_f1: 0.4774, em: 0.0123, avg: 0.2746, multirc_loss: 0.7114
03/25 04:22:40 PM: Evaluate: task multirc, batch 225 (1212): ans_f1: 0.5294, qst_f1: 0.4634, em: 0.0116, avg: 0.2705, multirc_loss: 0.7101
03/25 04:22:51 PM: Evaluate: task multirc, batch 239 (1212): ans_f1: 0.5348, qst_f1: 0.4734, em: 0.0109, avg: 0.2728, multirc_loss: 0.7094
03/25 04:23:01 PM: Evaluate: task multirc, batch 252 (1212): ans_f1: 0.5325, qst_f1: 0.4740, em: 0.0103, avg: 0.2714, multirc_loss: 0.7104
03/25 04:23:12 PM: Evaluate: task multirc, batch 266 (1212): ans_f1: 0.5291, qst_f1: 0.4727, em: 0.0096, avg: 0.2694, multirc_loss: 0.7122
03/25 04:23:23 PM: Evaluate: task multirc, batch 280 (1212): ans_f1: 0.5247, qst_f1: 0.4722, em: 0.0093, avg: 0.2670, multirc_loss: 0.7142
03/25 04:23:33 PM: Evaluate: task multirc, batch 294 (1212): ans_f1: 0.5217, qst_f1: 0.4702, em: 0.0088, avg: 0.2653, multirc_loss: 0.7161
03/25 04:23:43 PM: Evaluate: task multirc, batch 307 (1212): ans_f1: 0.5253, qst_f1: 0.4734, em: 0.0085, avg: 0.2669, multirc_loss: 0.7156
03/25 04:23:54 PM: Evaluate: task multirc, batch 321 (1212): ans_f1: 0.5290, qst_f1: 0.4750, em: 0.0082, avg: 0.2686, multirc_loss: 0.7160
03/25 04:24:05 PM: Evaluate: task multirc, batch 335 (1212): ans_f1: 0.5306, qst_f1: 0.4798, em: 0.0079, avg: 0.2693, multirc_loss: 0.7162
03/25 04:24:15 PM: Evaluate: task multirc, batch 349 (1212): ans_f1: 0.5344, qst_f1: 0.4814, em: 0.0076, avg: 0.2710, multirc_loss: 0.7159
03/25 04:24:26 PM: Evaluate: task multirc, batch 363 (1212): ans_f1: 0.5420, qst_f1: 0.4912, em: 0.0073, avg: 0.2746, multirc_loss: 0.7151
03/25 04:24:36 PM: Evaluate: task multirc, batch 376 (1212): ans_f1: 0.5456, qst_f1: 0.4946, em: 0.0069, avg: 0.2763, multirc_loss: 0.7147
03/25 04:24:47 PM: Evaluate: task multirc, batch 389 (1212): ans_f1: 0.5482, qst_f1: 0.4976, em: 0.0067, avg: 0.2775, multirc_loss: 0.7145
03/25 04:24:57 PM: Evaluate: task multirc, batch 402 (1212): ans_f1: 0.5443, qst_f1: 0.4928, em: 0.0064, avg: 0.2754, multirc_loss: 0.7151
03/25 04:25:08 PM: Evaluate: task multirc, batch 416 (1212): ans_f1: 0.5382, qst_f1: 0.4879, em: 0.0062, avg: 0.2722, multirc_loss: 0.7181
03/25 04:25:19 PM: Evaluate: task multirc, batch 430 (1212): ans_f1: 0.5363, qst_f1: 0.4820, em: 0.0090, avg: 0.2726, multirc_loss: 0.7169
03/25 04:25:29 PM: Evaluate: task multirc, batch 443 (1212): ans_f1: 0.5329, qst_f1: 0.4776, em: 0.0088, avg: 0.2708, multirc_loss: 0.7165
03/25 04:25:40 PM: Evaluate: task multirc, batch 457 (1212): ans_f1: 0.5326, qst_f1: 0.4775, em: 0.0085, avg: 0.2706, multirc_loss: 0.7173
03/25 04:25:51 PM: Evaluate: task multirc, batch 470 (1212): ans_f1: 0.5364, qst_f1: 0.4848, em: 0.0082, avg: 0.2723, multirc_loss: 0.7172
03/25 04:26:01 PM: Evaluate: task multirc, batch 483 (1212): ans_f1: 0.5331, qst_f1: 0.4750, em: 0.0079, avg: 0.2705, multirc_loss: 0.7160
03/25 04:26:12 PM: Evaluate: task multirc, batch 497 (1212): ans_f1: 0.5302, qst_f1: 0.4662, em: 0.0077, avg: 0.2690, multirc_loss: 0.7145
03/25 04:26:23 PM: Evaluate: task multirc, batch 511 (1212): ans_f1: 0.5278, qst_f1: 0.4631, em: 0.0100, avg: 0.2689, multirc_loss: 0.7142
03/25 04:26:34 PM: Evaluate: task multirc, batch 525 (1212): ans_f1: 0.5341, qst_f1: 0.4724, em: 0.0096, avg: 0.2718, multirc_loss: 0.7137
03/25 04:26:44 PM: Evaluate: task multirc, batch 538 (1212): ans_f1: 0.5343, qst_f1: 0.4737, em: 0.0117, avg: 0.2730, multirc_loss: 0.7142
03/25 04:26:55 PM: Evaluate: task multirc, batch 551 (1212): ans_f1: 0.5360, qst_f1: 0.4757, em: 0.0092, avg: 0.2726, multirc_loss: 0.7145
03/25 04:27:06 PM: Evaluate: task multirc, batch 564 (1212): ans_f1: 0.5365, qst_f1: 0.4758, em: 0.0090, avg: 0.2727, multirc_loss: 0.7147
03/25 04:27:17 PM: Evaluate: task multirc, batch 578 (1212): ans_f1: 0.5357, qst_f1: 0.4717, em: 0.0088, avg: 0.2722, multirc_loss: 0.7140
03/25 04:27:27 PM: Evaluate: task multirc, batch 591 (1212): ans_f1: 0.5345, qst_f1: 0.4707, em: 0.0086, avg: 0.2716, multirc_loss: 0.7138
03/25 04:27:37 PM: Evaluate: task multirc, batch 604 (1212): ans_f1: 0.5324, qst_f1: 0.4678, em: 0.0084, avg: 0.2704, multirc_loss: 0.7137
03/25 04:27:48 PM: Evaluate: task multirc, batch 618 (1212): ans_f1: 0.5296, qst_f1: 0.4628, em: 0.0104, avg: 0.2700, multirc_loss: 0.7131
03/25 04:27:59 PM: Evaluate: task multirc, batch 632 (1212): ans_f1: 0.5299, qst_f1: 0.4599, em: 0.0081, avg: 0.2690, multirc_loss: 0.7126
03/25 04:28:10 PM: Evaluate: task multirc, batch 646 (1212): ans_f1: 0.5322, qst_f1: 0.4596, em: 0.0079, avg: 0.2701, multirc_loss: 0.7120
03/25 04:28:20 PM: Evaluate: task multirc, batch 660 (1212): ans_f1: 0.5354, qst_f1: 0.4631, em: 0.0077, avg: 0.2716, multirc_loss: 0.7117
03/25 04:28:31 PM: Evaluate: task multirc, batch 674 (1212): ans_f1: 0.5366, qst_f1: 0.4660, em: 0.0075, avg: 0.2720, multirc_loss: 0.7121
03/25 04:28:42 PM: Evaluate: task multirc, batch 686 (1212): ans_f1: 0.5358, qst_f1: 0.4662, em: 0.0074, avg: 0.2716, multirc_loss: 0.7126
03/25 04:28:52 PM: Evaluate: task multirc, batch 699 (1212): ans_f1: 0.5371, qst_f1: 0.4688, em: 0.0073, avg: 0.2722, multirc_loss: 0.7129
03/25 04:29:03 PM: Evaluate: task multirc, batch 711 (1212): ans_f1: 0.5445, qst_f1: 0.4760, em: 0.0090, avg: 0.2767, multirc_loss: 0.7118
03/25 04:29:13 PM: Evaluate: task multirc, batch 724 (1212): ans_f1: 0.5443, qst_f1: 0.4764, em: 0.0089, avg: 0.2766, multirc_loss: 0.7114
03/25 04:29:24 PM: Evaluate: task multirc, batch 737 (1212): ans_f1: 0.5417, qst_f1: 0.4706, em: 0.0070, avg: 0.2743, multirc_loss: 0.7111
03/25 04:29:35 PM: Evaluate: task multirc, batch 751 (1212): ans_f1: 0.5408, qst_f1: 0.4681, em: 0.0068, avg: 0.2738, multirc_loss: 0.7110
03/25 04:29:45 PM: Evaluate: task multirc, batch 764 (1212): ans_f1: 0.5422, qst_f1: 0.4711, em: 0.0067, avg: 0.2745, multirc_loss: 0.7109
03/25 04:29:56 PM: Evaluate: task multirc, batch 777 (1212): ans_f1: 0.5456, qst_f1: 0.4751, em: 0.0083, avg: 0.2769, multirc_loss: 0.7107
03/25 04:30:06 PM: Evaluate: task multirc, batch 790 (1212): ans_f1: 0.5470, qst_f1: 0.4770, em: 0.0081, avg: 0.2775, multirc_loss: 0.7103
03/25 04:30:17 PM: Evaluate: task multirc, batch 803 (1212): ans_f1: 0.5471, qst_f1: 0.4764, em: 0.0095, avg: 0.2783, multirc_loss: 0.7099
03/25 04:30:27 PM: Evaluate: task multirc, batch 817 (1212): ans_f1: 0.5485, qst_f1: 0.4753, em: 0.0108, avg: 0.2797, multirc_loss: 0.7093
03/25 04:30:38 PM: Evaluate: task multirc, batch 831 (1212): ans_f1: 0.5491, qst_f1: 0.4741, em: 0.0152, avg: 0.2821, multirc_loss: 0.7089
03/25 04:30:49 PM: Evaluate: task multirc, batch 845 (1212): ans_f1: 0.5491, qst_f1: 0.4754, em: 0.0149, avg: 0.2820, multirc_loss: 0.7090
03/25 04:31:00 PM: Evaluate: task multirc, batch 859 (1212): ans_f1: 0.5496, qst_f1: 0.4768, em: 0.0161, avg: 0.2828, multirc_loss: 0.7088
03/25 04:31:11 PM: Evaluate: task multirc, batch 873 (1212): ans_f1: 0.5491, qst_f1: 0.4742, em: 0.0142, avg: 0.2817, multirc_loss: 0.7089
03/25 04:31:22 PM: Evaluate: task multirc, batch 886 (1212): ans_f1: 0.5468, qst_f1: 0.4706, em: 0.0140, avg: 0.2804, multirc_loss: 0.7088
03/25 04:31:32 PM: Evaluate: task multirc, batch 899 (1212): ans_f1: 0.5468, qst_f1: 0.4716, em: 0.0152, avg: 0.2810, multirc_loss: 0.7086
03/25 04:31:43 PM: Evaluate: task multirc, batch 913 (1212): ans_f1: 0.5457, qst_f1: 0.4716, em: 0.0136, avg: 0.2797, multirc_loss: 0.7091
03/25 04:31:54 PM: Evaluate: task multirc, batch 927 (1212): ans_f1: 0.5466, qst_f1: 0.4743, em: 0.0133, avg: 0.2800, multirc_loss: 0.7093
03/25 04:32:05 PM: Evaluate: task multirc, batch 941 (1212): ans_f1: 0.5449, qst_f1: 0.4717, em: 0.0145, avg: 0.2797, multirc_loss: 0.7092
03/25 04:32:16 PM: Evaluate: task multirc, batch 954 (1212): ans_f1: 0.5435, qst_f1: 0.4692, em: 0.0156, avg: 0.2796, multirc_loss: 0.7089
03/25 04:32:27 PM: Evaluate: task multirc, batch 967 (1212): ans_f1: 0.5413, qst_f1: 0.4663, em: 0.0141, avg: 0.2777, multirc_loss: 0.7087
03/25 04:32:37 PM: Evaluate: task multirc, batch 980 (1212): ans_f1: 0.5406, qst_f1: 0.4667, em: 0.0140, avg: 0.2773, multirc_loss: 0.7094
03/25 04:32:48 PM: Evaluate: task multirc, batch 993 (1212): ans_f1: 0.5406, qst_f1: 0.4667, em: 0.0139, avg: 0.2772, multirc_loss: 0.7092
03/25 04:32:58 PM: Evaluate: task multirc, batch 1006 (1212): ans_f1: 0.5403, qst_f1: 0.4667, em: 0.0137, avg: 0.2770, multirc_loss: 0.7091
03/25 04:33:09 PM: Evaluate: task multirc, batch 1019 (1212): ans_f1: 0.5395, qst_f1: 0.4656, em: 0.0136, avg: 0.2766, multirc_loss: 0.7091
03/25 04:33:19 PM: Evaluate: task multirc, batch 1032 (1212): ans_f1: 0.5368, qst_f1: 0.4622, em: 0.0134, avg: 0.2751, multirc_loss: 0.7091
03/25 04:33:30 PM: Evaluate: task multirc, batch 1045 (1212): ans_f1: 0.5387, qst_f1: 0.4636, em: 0.0145, avg: 0.2766, multirc_loss: 0.7088
03/25 04:33:41 PM: Evaluate: task multirc, batch 1058 (1212): ans_f1: 0.5382, qst_f1: 0.4629, em: 0.0143, avg: 0.2763, multirc_loss: 0.7088
03/25 04:33:51 PM: Evaluate: task multirc, batch 1071 (1212): ans_f1: 0.5393, qst_f1: 0.4641, em: 0.0152, avg: 0.2772, multirc_loss: 0.7086
03/25 04:34:02 PM: Evaluate: task multirc, batch 1084 (1212): ans_f1: 0.5401, qst_f1: 0.4655, em: 0.0150, avg: 0.2776, multirc_loss: 0.7087
03/25 04:34:13 PM: Evaluate: task multirc, batch 1096 (1212): ans_f1: 0.5407, qst_f1: 0.4663, em: 0.0149, avg: 0.2778, multirc_loss: 0.7088
03/25 04:34:24 PM: Evaluate: task multirc, batch 1110 (1212): ans_f1: 0.5399, qst_f1: 0.4664, em: 0.0148, avg: 0.2774, multirc_loss: 0.7091
03/25 04:34:34 PM: Evaluate: task multirc, batch 1123 (1212): ans_f1: 0.5392, qst_f1: 0.4659, em: 0.0147, avg: 0.2770, multirc_loss: 0.7090
03/25 04:34:45 PM: Evaluate: task multirc, batch 1137 (1212): ans_f1: 0.5374, qst_f1: 0.4649, em: 0.0145, avg: 0.2760, multirc_loss: 0.7094
03/25 04:34:56 PM: Evaluate: task multirc, batch 1151 (1212): ans_f1: 0.5361, qst_f1: 0.4630, em: 0.0144, avg: 0.2752, multirc_loss: 0.7093
03/25 04:35:08 PM: Evaluate: task multirc, batch 1165 (1212): ans_f1: 0.5338, qst_f1: 0.4585, em: 0.0142, avg: 0.2740, multirc_loss: 0.7091
03/25 04:35:19 PM: Evaluate: task multirc, batch 1179 (1212): ans_f1: 0.5322, qst_f1: 0.4547, em: 0.0151, avg: 0.2737, multirc_loss: 0.7088
03/25 04:35:30 PM: Evaluate: task multirc, batch 1193 (1212): ans_f1: 0.5319, qst_f1: 0.4533, em: 0.0149, avg: 0.2734, multirc_loss: 0.7085
03/25 04:35:41 PM: Evaluate: task multirc, batch 1206 (1212): ans_f1: 0.5350, qst_f1: 0.4567, em: 0.0148, avg: 0.2749, multirc_loss: 0.7082
03/25 04:35:46 PM: Best result seen so far for multirc.
03/25 04:35:46 PM: Best result seen so far for micro.
03/25 04:35:46 PM: Best result seen so far for macro.
03/25 04:35:46 PM: Updating LR scheduler:
03/25 04:35:46 PM: 	Best result seen so far for macro_avg: 0.275
03/25 04:35:46 PM: 	# validation passes without improvement: 0
03/25 04:35:46 PM: multirc_loss: training: 0.622848 validation: 0.708139
03/25 04:35:46 PM: macro_avg: validation: 0.274976
03/25 04:35:46 PM: micro_avg: validation: 0.274976
03/25 04:35:46 PM: multirc_ans_f1: training: 0.666667 validation: 0.535261
03/25 04:35:46 PM: multirc_qst_f1: training: 0.500000 validation: 0.456418
03/25 04:35:46 PM: multirc_em: training: 0.500000 validation: 0.014690
03/25 04:35:46 PM: multirc_avg: training: 0.583333 validation: 0.274976
03/25 04:35:46 PM: Global learning rate: 0.0003
03/25 04:35:46 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 04:35:51 PM: Update 3: task multirc, steps since last val 1 (total steps = 3): ans_f1: 1.0000, qst_f1: 0.7500, em: 1.0000, avg: 1.0000, multirc_loss: 0.6279
03/25 04:35:51 PM: ***** Step 3 / Validation 3 *****
03/25 04:35:51 PM: multirc: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 04:35:51 PM: Validating...
03/25 04:36:02 PM: Evaluate: task multirc, batch 13 (1212): ans_f1: 0.5312, qst_f1: 0.5104, em: 0.0000, avg: 0.2656, multirc_loss: 0.7166
03/25 04:36:12 PM: Evaluate: task multirc, batch 26 (1212): ans_f1: 0.5588, qst_f1: 0.5112, em: 0.0435, avg: 0.3012, multirc_loss: 0.7213
03/25 04:36:23 PM: Evaluate: task multirc, batch 39 (1212): ans_f1: 0.5742, qst_f1: 0.5540, em: 0.0278, avg: 0.3010, multirc_loss: 0.7137
03/25 04:36:33 PM: Evaluate: task multirc, batch 52 (1212): ans_f1: 0.5895, qst_f1: 0.5742, em: 0.0417, avg: 0.3156, multirc_loss: 0.7113
03/25 04:36:44 PM: Evaluate: task multirc, batch 65 (1212): ans_f1: 0.6099, qst_f1: 0.6042, em: 0.0323, avg: 0.3211, multirc_loss: 0.7095
03/25 04:36:54 PM: Evaluate: task multirc, batch 78 (1212): ans_f1: 0.6168, qst_f1: 0.6052, em: 0.0282, avg: 0.3225, multirc_loss: 0.7106
03/25 04:37:04 PM: Evaluate: task multirc, batch 92 (1212): ans_f1: 0.6218, qst_f1: 0.5880, em: 0.0366, avg: 0.3292, multirc_loss: 0.7070
03/25 04:37:15 PM: Evaluate: task multirc, batch 106 (1212): ans_f1: 0.5893, qst_f1: 0.5166, em: 0.0316, avg: 0.3104, multirc_loss: 0.7060
03/25 04:37:25 PM: Evaluate: task multirc, batch 120 (1212): ans_f1: 0.5710, qst_f1: 0.4958, em: 0.0286, avg: 0.2998, multirc_loss: 0.7061
03/25 04:37:36 PM: Evaluate: task multirc, batch 133 (1212): ans_f1: 0.5697, qst_f1: 0.5004, em: 0.0263, avg: 0.2980, multirc_loss: 0.7059
03/25 04:37:47 PM: Evaluate: task multirc, batch 146 (1212): ans_f1: 0.5663, qst_f1: 0.5064, em: 0.0242, avg: 0.2953, multirc_loss: 0.7100
03/25 04:37:57 PM: Evaluate: task multirc, batch 157 (1212): ans_f1: 0.5620, qst_f1: 0.5073, em: 0.0227, avg: 0.2924, multirc_loss: 0.7120
03/25 04:38:07 PM: Evaluate: task multirc, batch 170 (1212): ans_f1: 0.5681, qst_f1: 0.5148, em: 0.0214, avg: 0.2948, multirc_loss: 0.7112
03/25 04:38:18 PM: Evaluate: task multirc, batch 184 (1212): ans_f1: 0.5678, qst_f1: 0.5148, em: 0.0203, avg: 0.2940, multirc_loss: 0.7120
03/25 04:38:29 PM: Evaluate: task multirc, batch 197 (1212): ans_f1: 0.5604, qst_f1: 0.5119, em: 0.0192, avg: 0.2898, multirc_loss: 0.7145
03/25 04:38:39 PM: Evaluate: task multirc, batch 210 (1212): ans_f1: 0.5574, qst_f1: 0.5084, em: 0.0184, avg: 0.2879, multirc_loss: 0.7159
03/25 04:38:49 PM: Evaluate: task multirc, batch 224 (1212): ans_f1: 0.5563, qst_f1: 0.5024, em: 0.0174, avg: 0.2869, multirc_loss: 0.7141
03/25 04:39:00 PM: Evaluate: task multirc, batch 237 (1212): ans_f1: 0.5674, qst_f1: 0.5139, em: 0.0220, avg: 0.2947, multirc_loss: 0.7130
03/25 04:39:10 PM: Evaluate: task multirc, batch 250 (1212): ans_f1: 0.5634, qst_f1: 0.5118, em: 0.0155, avg: 0.2894, multirc_loss: 0.7142
03/25 04:39:20 PM: Evaluate: task multirc, batch 263 (1212): ans_f1: 0.5598, qst_f1: 0.5104, em: 0.0146, avg: 0.2872, multirc_loss: 0.7162
03/25 04:39:31 PM: Evaluate: task multirc, batch 277 (1212): ans_f1: 0.5509, qst_f1: 0.5043, em: 0.0140, avg: 0.2825, multirc_loss: 0.7194
03/25 04:39:41 PM: Evaluate: task multirc, batch 291 (1212): ans_f1: 0.5507, qst_f1: 0.5053, em: 0.0134, avg: 0.2820, multirc_loss: 0.7211
03/25 04:39:52 PM: Evaluate: task multirc, batch 305 (1212): ans_f1: 0.5504, qst_f1: 0.5056, em: 0.0128, avg: 0.2816, multirc_loss: 0.7211
03/25 04:40:03 PM: Evaluate: task multirc, batch 319 (1212): ans_f1: 0.5528, qst_f1: 0.5080, em: 0.0124, avg: 0.2826, multirc_loss: 0.7215
03/25 04:40:13 PM: Evaluate: task multirc, batch 333 (1212): ans_f1: 0.5541, qst_f1: 0.5082, em: 0.0120, avg: 0.2830, multirc_loss: 0.7216
03/25 04:40:24 PM: Evaluate: task multirc, batch 347 (1212): ans_f1: 0.5573, qst_f1: 0.5142, em: 0.0152, avg: 0.2862, multirc_loss: 0.7215
03/25 04:40:35 PM: Evaluate: task multirc, batch 361 (1212): ans_f1: 0.5621, qst_f1: 0.5164, em: 0.0110, avg: 0.2865, multirc_loss: 0.7207
03/25 04:40:46 PM: Evaluate: task multirc, batch 375 (1212): ans_f1: 0.5661, qst_f1: 0.5244, em: 0.0139, avg: 0.2900, multirc_loss: 0.7200
03/25 04:40:56 PM: Evaluate: task multirc, batch 388 (1212): ans_f1: 0.5685, qst_f1: 0.5259, em: 0.0101, avg: 0.2893, multirc_loss: 0.7198
03/25 04:41:06 PM: Evaluate: task multirc, batch 401 (1212): ans_f1: 0.5642, qst_f1: 0.5195, em: 0.0096, avg: 0.2869, multirc_loss: 0.7207
03/25 04:41:17 PM: Evaluate: task multirc, batch 415 (1212): ans_f1: 0.5582, qst_f1: 0.5137, em: 0.0093, avg: 0.2838, multirc_loss: 0.7237
03/25 04:41:28 PM: Evaluate: task multirc, batch 429 (1212): ans_f1: 0.5557, qst_f1: 0.5092, em: 0.0120, avg: 0.2839, multirc_loss: 0.7234
03/25 04:41:39 PM: Evaluate: task multirc, batch 442 (1212): ans_f1: 0.5511, qst_f1: 0.5015, em: 0.0117, avg: 0.2814, multirc_loss: 0.7229
03/25 04:41:49 PM: Evaluate: task multirc, batch 456 (1212): ans_f1: 0.5509, qst_f1: 0.5038, em: 0.0113, avg: 0.2811, multirc_loss: 0.7237
03/25 04:42:00 PM: Evaluate: task multirc, batch 469 (1212): ans_f1: 0.5535, qst_f1: 0.5091, em: 0.0109, avg: 0.2822, multirc_loss: 0.7237
03/25 04:42:10 PM: Evaluate: task multirc, batch 482 (1212): ans_f1: 0.5503, qst_f1: 0.4994, em: 0.0106, avg: 0.2805, multirc_loss: 0.7225
03/25 04:42:21 PM: Evaluate: task multirc, batch 496 (1212): ans_f1: 0.5479, qst_f1: 0.4902, em: 0.0103, avg: 0.2791, multirc_loss: 0.7210
03/25 04:42:32 PM: Evaluate: task multirc, batch 510 (1212): ans_f1: 0.5457, qst_f1: 0.4869, em: 0.0125, avg: 0.2791, multirc_loss: 0.7205
03/25 04:42:43 PM: Evaluate: task multirc, batch 524 (1212): ans_f1: 0.5512, qst_f1: 0.4952, em: 0.0096, avg: 0.2804, multirc_loss: 0.7197
03/25 04:42:53 PM: Evaluate: task multirc, batch 537 (1212): ans_f1: 0.5510, qst_f1: 0.4956, em: 0.0117, avg: 0.2813, multirc_loss: 0.7202
03/25 04:43:04 PM: Evaluate: task multirc, batch 550 (1212): ans_f1: 0.5526, qst_f1: 0.4968, em: 0.0092, avg: 0.2809, multirc_loss: 0.7204
03/25 04:43:15 PM: Evaluate: task multirc, batch 563 (1212): ans_f1: 0.5527, qst_f1: 0.4975, em: 0.0090, avg: 0.2809, multirc_loss: 0.7208
03/25 04:43:25 PM: Evaluate: task multirc, batch 576 (1212): ans_f1: 0.5508, qst_f1: 0.4942, em: 0.0110, avg: 0.2809, multirc_loss: 0.7202
03/25 04:43:36 PM: Evaluate: task multirc, batch 590 (1212): ans_f1: 0.5510, qst_f1: 0.4930, em: 0.0086, avg: 0.2798, multirc_loss: 0.7198
03/25 04:43:46 PM: Evaluate: task multirc, batch 603 (1212): ans_f1: 0.5475, qst_f1: 0.4907, em: 0.0084, avg: 0.2780, multirc_loss: 0.7198
03/25 04:43:57 PM: Evaluate: task multirc, batch 617 (1212): ans_f1: 0.5446, qst_f1: 0.4863, em: 0.0083, avg: 0.2765, multirc_loss: 0.7193
03/25 04:44:08 PM: Evaluate: task multirc, batch 631 (1212): ans_f1: 0.5450, qst_f1: 0.4859, em: 0.0081, avg: 0.2765, multirc_loss: 0.7188
03/25 04:44:19 PM: Evaluate: task multirc, batch 645 (1212): ans_f1: 0.5467, qst_f1: 0.4879, em: 0.0079, avg: 0.2773, multirc_loss: 0.7182
03/25 04:44:29 PM: Evaluate: task multirc, batch 659 (1212): ans_f1: 0.5496, qst_f1: 0.4917, em: 0.0097, avg: 0.2796, multirc_loss: 0.7178
03/25 04:44:40 PM: Evaluate: task multirc, batch 673 (1212): ans_f1: 0.5508, qst_f1: 0.4935, em: 0.0075, avg: 0.2792, multirc_loss: 0.7181
03/25 04:44:51 PM: Evaluate: task multirc, batch 685 (1212): ans_f1: 0.5503, qst_f1: 0.4939, em: 0.0074, avg: 0.2789, multirc_loss: 0.7187
03/25 04:45:01 PM: Evaluate: task multirc, batch 698 (1212): ans_f1: 0.5500, qst_f1: 0.4951, em: 0.0073, avg: 0.2787, multirc_loss: 0.7192
03/25 04:45:12 PM: Evaluate: task multirc, batch 711 (1212): ans_f1: 0.5573, qst_f1: 0.5021, em: 0.0090, avg: 0.2832, multirc_loss: 0.7178
03/25 04:45:23 PM: Evaluate: task multirc, batch 724 (1212): ans_f1: 0.5594, qst_f1: 0.5031, em: 0.0089, avg: 0.2841, multirc_loss: 0.7172
03/25 04:45:33 PM: Evaluate: task multirc, batch 737 (1212): ans_f1: 0.5570, qst_f1: 0.4982, em: 0.0070, avg: 0.2820, multirc_loss: 0.7169
03/25 04:45:44 PM: Evaluate: task multirc, batch 751 (1212): ans_f1: 0.5554, qst_f1: 0.4961, em: 0.0068, avg: 0.2811, multirc_loss: 0.7169
03/25 04:45:55 PM: Evaluate: task multirc, batch 764 (1212): ans_f1: 0.5565, qst_f1: 0.4986, em: 0.0067, avg: 0.2816, multirc_loss: 0.7168
03/25 04:46:06 PM: Evaluate: task multirc, batch 777 (1212): ans_f1: 0.5594, qst_f1: 0.5021, em: 0.0083, avg: 0.2838, multirc_loss: 0.7165
03/25 04:46:16 PM: Evaluate: task multirc, batch 790 (1212): ans_f1: 0.5609, qst_f1: 0.5044, em: 0.0081, avg: 0.2845, multirc_loss: 0.7161
03/25 04:46:27 PM: Evaluate: task multirc, batch 803 (1212): ans_f1: 0.5607, qst_f1: 0.5032, em: 0.0095, avg: 0.2851, multirc_loss: 0.7156
03/25 04:46:38 PM: Evaluate: task multirc, batch 817 (1212): ans_f1: 0.5645, qst_f1: 0.5073, em: 0.0139, avg: 0.2892, multirc_loss: 0.7147
03/25 04:46:48 PM: Evaluate: task multirc, batch 831 (1212): ans_f1: 0.5644, qst_f1: 0.5057, em: 0.0167, avg: 0.2905, multirc_loss: 0.7143
03/25 04:46:59 PM: Evaluate: task multirc, batch 845 (1212): ans_f1: 0.5637, qst_f1: 0.5060, em: 0.0164, avg: 0.2901, multirc_loss: 0.7145
03/25 04:47:10 PM: Evaluate: task multirc, batch 859 (1212): ans_f1: 0.5645, qst_f1: 0.5082, em: 0.0190, avg: 0.2918, multirc_loss: 0.7144
03/25 04:47:21 PM: Evaluate: task multirc, batch 873 (1212): ans_f1: 0.5639, qst_f1: 0.5059, em: 0.0171, avg: 0.2905, multirc_loss: 0.7144
03/25 04:47:32 PM: Evaluate: task multirc, batch 886 (1212): ans_f1: 0.5636, qst_f1: 0.5053, em: 0.0168, avg: 0.2902, multirc_loss: 0.7141
03/25 04:47:42 PM: Evaluate: task multirc, batch 899 (1212): ans_f1: 0.5645, qst_f1: 0.5072, em: 0.0180, avg: 0.2912, multirc_loss: 0.7137
03/25 04:47:53 PM: Evaluate: task multirc, batch 913 (1212): ans_f1: 0.5633, qst_f1: 0.5066, em: 0.0163, avg: 0.2898, multirc_loss: 0.7144
03/25 04:48:04 PM: Evaluate: task multirc, batch 927 (1212): ans_f1: 0.5637, qst_f1: 0.5086, em: 0.0160, avg: 0.2898, multirc_loss: 0.7146
03/25 04:48:15 PM: Evaluate: task multirc, batch 941 (1212): ans_f1: 0.5629, qst_f1: 0.5067, em: 0.0171, avg: 0.2900, multirc_loss: 0.7145
03/25 04:48:26 PM: Evaluate: task multirc, batch 954 (1212): ans_f1: 0.5623, qst_f1: 0.5052, em: 0.0182, avg: 0.2902, multirc_loss: 0.7142
03/25 04:48:37 PM: Evaluate: task multirc, batch 967 (1212): ans_f1: 0.5602, qst_f1: 0.5027, em: 0.0167, avg: 0.2884, multirc_loss: 0.7140
03/25 04:48:47 PM: Evaluate: task multirc, batch 980 (1212): ans_f1: 0.5593, qst_f1: 0.5027, em: 0.0165, avg: 0.2879, multirc_loss: 0.7148
03/25 04:48:57 PM: Evaluate: task multirc, batch 992 (1212): ans_f1: 0.5589, qst_f1: 0.5021, em: 0.0164, avg: 0.2877, multirc_loss: 0.7147
03/25 04:49:08 PM: Evaluate: task multirc, batch 1005 (1212): ans_f1: 0.5591, qst_f1: 0.5028, em: 0.0163, avg: 0.2877, multirc_loss: 0.7146
03/25 04:49:19 PM: Evaluate: task multirc, batch 1019 (1212): ans_f1: 0.5580, qst_f1: 0.5014, em: 0.0161, avg: 0.2870, multirc_loss: 0.7146
03/25 04:49:30 PM: Evaluate: task multirc, batch 1032 (1212): ans_f1: 0.5560, qst_f1: 0.4999, em: 0.0159, avg: 0.2860, multirc_loss: 0.7146
03/25 04:49:41 PM: Evaluate: task multirc, batch 1045 (1212): ans_f1: 0.5574, qst_f1: 0.5012, em: 0.0157, avg: 0.2866, multirc_loss: 0.7144
03/25 04:49:51 PM: Evaluate: task multirc, batch 1058 (1212): ans_f1: 0.5574, qst_f1: 0.5015, em: 0.0155, avg: 0.2865, multirc_loss: 0.7143
03/25 04:50:02 PM: Evaluate: task multirc, batch 1071 (1212): ans_f1: 0.5582, qst_f1: 0.5023, em: 0.0176, avg: 0.2879, multirc_loss: 0.7141
03/25 04:50:13 PM: Evaluate: task multirc, batch 1084 (1212): ans_f1: 0.5590, qst_f1: 0.5034, em: 0.0174, avg: 0.2882, multirc_loss: 0.7142
03/25 04:50:23 PM: Evaluate: task multirc, batch 1096 (1212): ans_f1: 0.5596, qst_f1: 0.5041, em: 0.0172, avg: 0.2884, multirc_loss: 0.7143
03/25 04:50:34 PM: Evaluate: task multirc, batch 1110 (1212): ans_f1: 0.5586, qst_f1: 0.5040, em: 0.0171, avg: 0.2878, multirc_loss: 0.7146
03/25 04:50:45 PM: Evaluate: task multirc, batch 1124 (1212): ans_f1: 0.5581, qst_f1: 0.5036, em: 0.0169, avg: 0.2875, multirc_loss: 0.7146
03/25 04:50:56 PM: Evaluate: task multirc, batch 1138 (1212): ans_f1: 0.5566, qst_f1: 0.5033, em: 0.0168, avg: 0.2867, multirc_loss: 0.7150
03/25 04:51:07 PM: Evaluate: task multirc, batch 1152 (1212): ans_f1: 0.5552, qst_f1: 0.5016, em: 0.0177, avg: 0.2864, multirc_loss: 0.7150
03/25 04:51:19 PM: Evaluate: task multirc, batch 1166 (1212): ans_f1: 0.5534, qst_f1: 0.4983, em: 0.0174, avg: 0.2854, multirc_loss: 0.7148
03/25 04:51:30 PM: Evaluate: task multirc, batch 1180 (1212): ans_f1: 0.5517, qst_f1: 0.4938, em: 0.0183, avg: 0.2850, multirc_loss: 0.7145
03/25 04:51:41 PM: Evaluate: task multirc, batch 1194 (1212): ans_f1: 0.5518, qst_f1: 0.4938, em: 0.0181, avg: 0.2850, multirc_loss: 0.7142
03/25 04:51:51 PM: Evaluate: task multirc, batch 1207 (1212): ans_f1: 0.5538, qst_f1: 0.4952, em: 0.0179, avg: 0.2859, multirc_loss: 0.7139
03/25 04:51:56 PM: Best result seen so far for multirc.
03/25 04:51:56 PM: Best result seen so far for micro.
03/25 04:51:56 PM: Best result seen so far for macro.
03/25 04:51:56 PM: Updating LR scheduler:
03/25 04:51:56 PM: 	Best result seen so far for macro_avg: 0.286
03/25 04:51:56 PM: 	# validation passes without improvement: 0
03/25 04:51:56 PM: multirc_loss: training: 0.627867 validation: 0.713710
03/25 04:51:56 PM: macro_avg: validation: 0.286311
03/25 04:51:56 PM: micro_avg: validation: 0.286311
03/25 04:51:56 PM: multirc_ans_f1: training: 1.000000 validation: 0.554783
03/25 04:51:56 PM: multirc_qst_f1: training: 0.750000 validation: 0.495970
03/25 04:51:56 PM: multirc_em: training: 1.000000 validation: 0.017838
03/25 04:51:56 PM: multirc_avg: training: 1.000000 validation: 0.286311
03/25 04:51:56 PM: Global learning rate: 0.0003
03/25 04:51:56 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 04:52:02 PM: Update 4: task multirc, steps since last val 1 (total steps = 4): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.2500, avg: 0.1250, multirc_loss: 0.7990
03/25 04:52:02 PM: ***** Step 4 / Validation 4 *****
03/25 04:52:02 PM: multirc: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 04:52:02 PM: Validating...
03/25 04:52:12 PM: Evaluate: task multirc, batch 13 (1212): ans_f1: 0.5915, qst_f1: 0.5952, em: 0.0000, avg: 0.2958, multirc_loss: 0.7206
03/25 04:52:23 PM: Evaluate: task multirc, batch 26 (1212): ans_f1: 0.5931, qst_f1: 0.5648, em: 0.0000, avg: 0.2966, multirc_loss: 0.7248
03/25 04:52:33 PM: Evaluate: task multirc, batch 39 (1212): ans_f1: 0.6188, qst_f1: 0.6011, em: 0.0000, avg: 0.3094, multirc_loss: 0.7153
03/25 04:52:44 PM: Evaluate: task multirc, batch 52 (1212): ans_f1: 0.6312, qst_f1: 0.6178, em: 0.0208, avg: 0.3260, multirc_loss: 0.7128
03/25 04:52:54 PM: Evaluate: task multirc, batch 65 (1212): ans_f1: 0.6421, qst_f1: 0.6380, em: 0.0161, avg: 0.3291, multirc_loss: 0.7111
03/25 04:53:04 PM: Evaluate: task multirc, batch 78 (1212): ans_f1: 0.6433, qst_f1: 0.6348, em: 0.0141, avg: 0.3287, multirc_loss: 0.7126
03/25 04:53:15 PM: Evaluate: task multirc, batch 92 (1212): ans_f1: 0.6424, qst_f1: 0.6111, em: 0.0122, avg: 0.3273, multirc_loss: 0.7089
03/25 04:53:25 PM: Evaluate: task multirc, batch 106 (1212): ans_f1: 0.6138, qst_f1: 0.5446, em: 0.0105, avg: 0.3121, multirc_loss: 0.7068
03/25 04:53:36 PM: Evaluate: task multirc, batch 120 (1212): ans_f1: 0.6038, qst_f1: 0.5293, em: 0.0095, avg: 0.3066, multirc_loss: 0.7066
03/25 04:53:46 PM: Evaluate: task multirc, batch 133 (1212): ans_f1: 0.5963, qst_f1: 0.5292, em: 0.0088, avg: 0.3025, multirc_loss: 0.7071
03/25 04:53:57 PM: Evaluate: task multirc, batch 146 (1212): ans_f1: 0.5907, qst_f1: 0.5328, em: 0.0081, avg: 0.2994, multirc_loss: 0.7119
03/25 04:54:07 PM: Evaluate: task multirc, batch 158 (1212): ans_f1: 0.5871, qst_f1: 0.5367, em: 0.0150, avg: 0.3011, multirc_loss: 0.7137
03/25 04:54:18 PM: Evaluate: task multirc, batch 172 (1212): ans_f1: 0.5898, qst_f1: 0.5392, em: 0.0071, avg: 0.2984, multirc_loss: 0.7137
03/25 04:54:28 PM: Evaluate: task multirc, batch 186 (1212): ans_f1: 0.5863, qst_f1: 0.5376, em: 0.0067, avg: 0.2965, multirc_loss: 0.7155
03/25 04:54:39 PM: Evaluate: task multirc, batch 199 (1212): ans_f1: 0.5789, qst_f1: 0.5324, em: 0.0064, avg: 0.2927, multirc_loss: 0.7183
03/25 04:54:49 PM: Evaluate: task multirc, batch 213 (1212): ans_f1: 0.5757, qst_f1: 0.5280, em: 0.0061, avg: 0.2909, multirc_loss: 0.7191
03/25 04:55:00 PM: Evaluate: task multirc, batch 226 (1212): ans_f1: 0.5805, qst_f1: 0.5290, em: 0.0058, avg: 0.2931, multirc_loss: 0.7172
03/25 04:55:10 PM: Evaluate: task multirc, batch 240 (1212): ans_f1: 0.5883, qst_f1: 0.5379, em: 0.0054, avg: 0.2969, multirc_loss: 0.7164
03/25 04:55:21 PM: Evaluate: task multirc, batch 253 (1212): ans_f1: 0.5835, qst_f1: 0.5349, em: 0.0051, avg: 0.2943, multirc_loss: 0.7179
03/25 04:55:31 PM: Evaluate: task multirc, batch 267 (1212): ans_f1: 0.5774, qst_f1: 0.5290, em: 0.0048, avg: 0.2911, multirc_loss: 0.7209
03/25 04:55:42 PM: Evaluate: task multirc, batch 281 (1212): ans_f1: 0.5720, qst_f1: 0.5280, em: 0.0046, avg: 0.2883, multirc_loss: 0.7233
03/25 04:55:52 PM: Evaluate: task multirc, batch 295 (1212): ans_f1: 0.5689, qst_f1: 0.5264, em: 0.0044, avg: 0.2866, multirc_loss: 0.7256
03/25 04:56:02 PM: Evaluate: task multirc, batch 308 (1212): ans_f1: 0.5700, qst_f1: 0.5291, em: 0.0085, avg: 0.2893, multirc_loss: 0.7251
03/25 04:56:13 PM: Evaluate: task multirc, batch 322 (1212): ans_f1: 0.5706, qst_f1: 0.5281, em: 0.0041, avg: 0.2874, multirc_loss: 0.7258
03/25 04:56:24 PM: Evaluate: task multirc, batch 336 (1212): ans_f1: 0.5703, qst_f1: 0.5297, em: 0.0079, avg: 0.2891, multirc_loss: 0.7264
03/25 04:56:35 PM: Evaluate: task multirc, batch 350 (1212): ans_f1: 0.5723, qst_f1: 0.5306, em: 0.0038, avg: 0.2881, multirc_loss: 0.7258
03/25 04:56:45 PM: Evaluate: task multirc, batch 364 (1212): ans_f1: 0.5771, qst_f1: 0.5343, em: 0.0036, avg: 0.2903, multirc_loss: 0.7249
03/25 04:56:55 PM: Evaluate: task multirc, batch 377 (1212): ans_f1: 0.5791, qst_f1: 0.5385, em: 0.0035, avg: 0.2913, multirc_loss: 0.7244
03/25 04:57:06 PM: Evaluate: task multirc, batch 390 (1212): ans_f1: 0.5803, qst_f1: 0.5388, em: 0.0033, avg: 0.2918, multirc_loss: 0.7244
03/25 04:57:17 PM: Evaluate: task multirc, batch 404 (1212): ans_f1: 0.5753, qst_f1: 0.5352, em: 0.0064, avg: 0.2909, multirc_loss: 0.7256
03/25 04:57:27 PM: Evaluate: task multirc, batch 418 (1212): ans_f1: 0.5705, qst_f1: 0.5311, em: 0.0062, avg: 0.2883, multirc_loss: 0.7289
03/25 04:57:38 PM: Evaluate: task multirc, batch 431 (1212): ans_f1: 0.5682, qst_f1: 0.5248, em: 0.0090, avg: 0.2886, multirc_loss: 0.7279
03/25 04:57:48 PM: Evaluate: task multirc, batch 444 (1212): ans_f1: 0.5644, qst_f1: 0.5193, em: 0.0087, avg: 0.2866, multirc_loss: 0.7281
03/25 04:57:58 PM: Evaluate: task multirc, batch 457 (1212): ans_f1: 0.5635, qst_f1: 0.5190, em: 0.0085, avg: 0.2860, multirc_loss: 0.7290
03/25 04:58:09 PM: Evaluate: task multirc, batch 470 (1212): ans_f1: 0.5662, qst_f1: 0.5247, em: 0.0082, avg: 0.2872, multirc_loss: 0.7288
03/25 04:58:19 PM: Evaluate: task multirc, batch 483 (1212): ans_f1: 0.5623, qst_f1: 0.5163, em: 0.0079, avg: 0.2851, multirc_loss: 0.7276
03/25 04:58:30 PM: Evaluate: task multirc, batch 497 (1212): ans_f1: 0.5617, qst_f1: 0.5101, em: 0.0077, avg: 0.2847, multirc_loss: 0.7259
03/25 04:58:41 PM: Evaluate: task multirc, batch 511 (1212): ans_f1: 0.5612, qst_f1: 0.5097, em: 0.0075, avg: 0.2843, multirc_loss: 0.7253
03/25 04:58:51 PM: Evaluate: task multirc, batch 523 (1212): ans_f1: 0.5656, qst_f1: 0.5169, em: 0.0072, avg: 0.2864, multirc_loss: 0.7246
03/25 04:59:01 PM: Evaluate: task multirc, batch 535 (1212): ans_f1: 0.5646, qst_f1: 0.5147, em: 0.0071, avg: 0.2858, multirc_loss: 0.7252
03/25 04:59:12 PM: Evaluate: task multirc, batch 548 (1212): ans_f1: 0.5673, qst_f1: 0.5196, em: 0.0069, avg: 0.2871, multirc_loss: 0.7248
03/25 04:59:23 PM: Evaluate: task multirc, batch 561 (1212): ans_f1: 0.5656, qst_f1: 0.5188, em: 0.0068, avg: 0.2862, multirc_loss: 0.7259
03/25 04:59:33 PM: Evaluate: task multirc, batch 574 (1212): ans_f1: 0.5662, qst_f1: 0.5152, em: 0.0066, avg: 0.2864, multirc_loss: 0.7252
03/25 04:59:44 PM: Evaluate: task multirc, batch 588 (1212): ans_f1: 0.5667, qst_f1: 0.5168, em: 0.0087, avg: 0.2877, multirc_loss: 0.7247
03/25 04:59:55 PM: Evaluate: task multirc, batch 602 (1212): ans_f1: 0.5634, qst_f1: 0.5134, em: 0.0063, avg: 0.2849, multirc_loss: 0.7249
03/25 05:00:06 PM: Evaluate: task multirc, batch 616 (1212): ans_f1: 0.5618, qst_f1: 0.5127, em: 0.0062, avg: 0.2840, multirc_loss: 0.7244
03/25 05:00:17 PM: Evaluate: task multirc, batch 630 (1212): ans_f1: 0.5616, qst_f1: 0.5126, em: 0.0061, avg: 0.2838, multirc_loss: 0.7239
03/25 05:00:28 PM: Evaluate: task multirc, batch 644 (1212): ans_f1: 0.5628, qst_f1: 0.5133, em: 0.0060, avg: 0.2844, multirc_loss: 0.7232
03/25 05:00:38 PM: Evaluate: task multirc, batch 658 (1212): ans_f1: 0.5645, qst_f1: 0.5162, em: 0.0058, avg: 0.2852, multirc_loss: 0.7229
03/25 05:00:49 PM: Evaluate: task multirc, batch 672 (1212): ans_f1: 0.5661, qst_f1: 0.5185, em: 0.0057, avg: 0.2859, multirc_loss: 0.7229
03/25 05:00:59 PM: Evaluate: task multirc, batch 684 (1212): ans_f1: 0.5650, qst_f1: 0.5181, em: 0.0056, avg: 0.2853, multirc_loss: 0.7238
03/25 05:01:10 PM: Evaluate: task multirc, batch 697 (1212): ans_f1: 0.5644, qst_f1: 0.5190, em: 0.0055, avg: 0.2849, multirc_loss: 0.7243
03/25 05:01:21 PM: Evaluate: task multirc, batch 710 (1212): ans_f1: 0.5707, qst_f1: 0.5251, em: 0.0072, avg: 0.2890, multirc_loss: 0.7228
03/25 05:01:31 PM: Evaluate: task multirc, batch 723 (1212): ans_f1: 0.5729, qst_f1: 0.5257, em: 0.0053, avg: 0.2891, multirc_loss: 0.7221
03/25 05:01:42 PM: Evaluate: task multirc, batch 736 (1212): ans_f1: 0.5719, qst_f1: 0.5237, em: 0.0052, avg: 0.2886, multirc_loss: 0.7218
03/25 05:01:53 PM: Evaluate: task multirc, batch 750 (1212): ans_f1: 0.5704, qst_f1: 0.5223, em: 0.0051, avg: 0.2878, multirc_loss: 0.7217
03/25 05:02:03 PM: Evaluate: task multirc, batch 763 (1212): ans_f1: 0.5709, qst_f1: 0.5233, em: 0.0051, avg: 0.2880, multirc_loss: 0.7217
03/25 05:02:14 PM: Evaluate: task multirc, batch 776 (1212): ans_f1: 0.5730, qst_f1: 0.5260, em: 0.0066, avg: 0.2898, multirc_loss: 0.7214
03/25 05:02:24 PM: Evaluate: task multirc, batch 789 (1212): ans_f1: 0.5750, qst_f1: 0.5284, em: 0.0065, avg: 0.2907, multirc_loss: 0.7208
03/25 05:02:35 PM: Evaluate: task multirc, batch 802 (1212): ans_f1: 0.5739, qst_f1: 0.5258, em: 0.0063, avg: 0.2901, multirc_loss: 0.7203
03/25 05:02:46 PM: Evaluate: task multirc, batch 816 (1212): ans_f1: 0.5794, qst_f1: 0.5337, em: 0.0124, avg: 0.2959, multirc_loss: 0.7193
03/25 05:02:56 PM: Evaluate: task multirc, batch 830 (1212): ans_f1: 0.5810, qst_f1: 0.5361, em: 0.0167, avg: 0.2988, multirc_loss: 0.7187
03/25 05:03:07 PM: Evaluate: task multirc, batch 844 (1212): ans_f1: 0.5801, qst_f1: 0.5359, em: 0.0164, avg: 0.2983, multirc_loss: 0.7190
03/25 05:03:18 PM: Evaluate: task multirc, batch 858 (1212): ans_f1: 0.5799, qst_f1: 0.5364, em: 0.0176, avg: 0.2988, multirc_loss: 0.7189
03/25 05:03:29 PM: Evaluate: task multirc, batch 872 (1212): ans_f1: 0.5815, qst_f1: 0.5385, em: 0.0171, avg: 0.2993, multirc_loss: 0.7187
03/25 05:03:40 PM: Evaluate: task multirc, batch 886 (1212): ans_f1: 0.5831, qst_f1: 0.5394, em: 0.0168, avg: 0.2999, multirc_loss: 0.7184
03/25 05:03:51 PM: Evaluate: task multirc, batch 900 (1212): ans_f1: 0.5853, qst_f1: 0.5421, em: 0.0166, avg: 0.3009, multirc_loss: 0.7180
03/25 05:04:03 PM: Evaluate: task multirc, batch 914 (1212): ans_f1: 0.5837, qst_f1: 0.5420, em: 0.0163, avg: 0.3000, multirc_loss: 0.7188
03/25 05:04:13 PM: Evaluate: task multirc, batch 928 (1212): ans_f1: 0.5836, qst_f1: 0.5423, em: 0.0159, avg: 0.2997, multirc_loss: 0.7191
03/25 05:04:24 PM: Evaluate: task multirc, batch 942 (1212): ans_f1: 0.5819, qst_f1: 0.5408, em: 0.0157, avg: 0.2988, multirc_loss: 0.7189
03/25 05:04:35 PM: Evaluate: task multirc, batch 955 (1212): ans_f1: 0.5820, qst_f1: 0.5416, em: 0.0169, avg: 0.2994, multirc_loss: 0.7186
03/25 05:04:46 PM: Evaluate: task multirc, batch 969 (1212): ans_f1: 0.5809, qst_f1: 0.5405, em: 0.0167, avg: 0.2988, multirc_loss: 0.7187
03/25 05:04:56 PM: Evaluate: task multirc, batch 982 (1212): ans_f1: 0.5803, qst_f1: 0.5399, em: 0.0165, avg: 0.2984, multirc_loss: 0.7193
03/25 05:05:07 PM: Evaluate: task multirc, batch 995 (1212): ans_f1: 0.5796, qst_f1: 0.5400, em: 0.0164, avg: 0.2980, multirc_loss: 0.7192
03/25 05:05:18 PM: Evaluate: task multirc, batch 1009 (1212): ans_f1: 0.5789, qst_f1: 0.5390, em: 0.0162, avg: 0.2976, multirc_loss: 0.7191
03/25 05:05:29 PM: Evaluate: task multirc, batch 1022 (1212): ans_f1: 0.5779, qst_f1: 0.5381, em: 0.0160, avg: 0.2970, multirc_loss: 0.7193
03/25 05:05:39 PM: Evaluate: task multirc, batch 1035 (1212): ans_f1: 0.5779, qst_f1: 0.5388, em: 0.0158, avg: 0.2968, multirc_loss: 0.7192
03/25 05:05:50 PM: Evaluate: task multirc, batch 1048 (1212): ans_f1: 0.5780, qst_f1: 0.5386, em: 0.0156, avg: 0.2968, multirc_loss: 0.7190
03/25 05:06:01 PM: Evaluate: task multirc, batch 1061 (1212): ans_f1: 0.5787, qst_f1: 0.5399, em: 0.0154, avg: 0.2971, multirc_loss: 0.7188
03/25 05:06:11 PM: Evaluate: task multirc, batch 1074 (1212): ans_f1: 0.5789, qst_f1: 0.5399, em: 0.0163, avg: 0.2976, multirc_loss: 0.7187
03/25 05:06:22 PM: Evaluate: task multirc, batch 1087 (1212): ans_f1: 0.5796, qst_f1: 0.5404, em: 0.0150, avg: 0.2973, multirc_loss: 0.7187
03/25 05:06:33 PM: Evaluate: task multirc, batch 1099 (1212): ans_f1: 0.5790, qst_f1: 0.5399, em: 0.0149, avg: 0.2969, multirc_loss: 0.7192
03/25 05:06:43 PM: Evaluate: task multirc, batch 1113 (1212): ans_f1: 0.5783, qst_f1: 0.5399, em: 0.0148, avg: 0.2965, multirc_loss: 0.7193
03/25 05:06:55 PM: Evaluate: task multirc, batch 1127 (1212): ans_f1: 0.5771, qst_f1: 0.5387, em: 0.0146, avg: 0.2959, multirc_loss: 0.7194
03/25 05:07:06 PM: Evaluate: task multirc, batch 1141 (1212): ans_f1: 0.5755, qst_f1: 0.5379, em: 0.0145, avg: 0.2950, multirc_loss: 0.7199
03/25 05:07:17 PM: Evaluate: task multirc, batch 1155 (1212): ans_f1: 0.5740, qst_f1: 0.5360, em: 0.0143, avg: 0.2942, multirc_loss: 0.7198
03/25 05:07:27 PM: Evaluate: task multirc, batch 1168 (1212): ans_f1: 0.5725, qst_f1: 0.5335, em: 0.0152, avg: 0.2938, multirc_loss: 0.7196
03/25 05:07:38 PM: Evaluate: task multirc, batch 1182 (1212): ans_f1: 0.5715, qst_f1: 0.5291, em: 0.0150, avg: 0.2933, multirc_loss: 0.7192
03/25 05:07:49 PM: Evaluate: task multirc, batch 1196 (1212): ans_f1: 0.5716, qst_f1: 0.5293, em: 0.0138, avg: 0.2927, multirc_loss: 0.7190
03/25 05:08:00 PM: Evaluate: task multirc, batch 1209 (1212): ans_f1: 0.5737, qst_f1: 0.5312, em: 0.0137, avg: 0.2937, multirc_loss: 0.7185
03/25 05:08:03 PM: Best result seen so far for multirc.
03/25 05:08:03 PM: Best result seen so far for micro.
03/25 05:08:03 PM: Best result seen so far for macro.
03/25 05:08:03 PM: Updating LR scheduler:
03/25 05:08:03 PM: 	Best result seen so far for macro_avg: 0.294
03/25 05:08:03 PM: 	# validation passes without improvement: 0
03/25 05:08:03 PM: multirc_loss: training: 0.799050 validation: 0.718442
03/25 05:08:03 PM: macro_avg: validation: 0.293814
03/25 05:08:03 PM: micro_avg: validation: 0.293814
03/25 05:08:03 PM: multirc_ans_f1: training: 0.000000 validation: 0.573987
03/25 05:08:03 PM: multirc_qst_f1: training: 0.000000 validation: 0.531363
03/25 05:08:03 PM: multirc_em: training: 0.250000 validation: 0.013641
03/25 05:08:03 PM: multirc_avg: training: 0.125000 validation: 0.293814
03/25 05:08:03 PM: Global learning rate: 0.0003
03/25 05:08:03 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 05:08:08 PM: ***** Step 5 / Validation 5 *****
03/25 05:08:08 PM: multirc: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 05:08:08 PM: Validating...
03/25 05:08:11 PM: Evaluate: task multirc, batch 3 (1212): ans_f1: 0.5333, qst_f1: 0.4444, em: 0.0000, avg: 0.2667, multirc_loss: 0.7299
03/25 05:08:21 PM: Evaluate: task multirc, batch 16 (1212): ans_f1: 0.5843, qst_f1: 0.5840, em: 0.0000, avg: 0.2921, multirc_loss: 0.7236
03/25 05:08:31 PM: Evaluate: task multirc, batch 29 (1212): ans_f1: 0.5976, qst_f1: 0.5956, em: 0.0385, avg: 0.3180, multirc_loss: 0.7207
03/25 05:08:42 PM: Evaluate: task multirc, batch 42 (1212): ans_f1: 0.6198, qst_f1: 0.5921, em: 0.0256, avg: 0.3227, multirc_loss: 0.7151
03/25 05:08:52 PM: Evaluate: task multirc, batch 55 (1212): ans_f1: 0.6398, qst_f1: 0.6320, em: 0.0385, avg: 0.3391, multirc_loss: 0.7090
03/25 05:09:03 PM: Evaluate: task multirc, batch 68 (1212): ans_f1: 0.6348, qst_f1: 0.6318, em: 0.0154, avg: 0.3251, multirc_loss: 0.7137
03/25 05:09:13 PM: Evaluate: task multirc, batch 82 (1212): ans_f1: 0.6488, qst_f1: 0.6420, em: 0.0270, avg: 0.3379, multirc_loss: 0.7105
03/25 05:09:24 PM: Evaluate: task multirc, batch 96 (1212): ans_f1: 0.6328, qst_f1: 0.5821, em: 0.0116, avg: 0.3222, multirc_loss: 0.7077
03/25 05:09:34 PM: Evaluate: task multirc, batch 110 (1212): ans_f1: 0.6159, qst_f1: 0.5493, em: 0.0102, avg: 0.3131, multirc_loss: 0.7060
03/25 05:09:45 PM: Evaluate: task multirc, batch 123 (1212): ans_f1: 0.6142, qst_f1: 0.5485, em: 0.0094, avg: 0.3118, multirc_loss: 0.7051
03/25 05:09:55 PM: Evaluate: task multirc, batch 136 (1212): ans_f1: 0.6041, qst_f1: 0.5433, em: 0.0085, avg: 0.3063, multirc_loss: 0.7066
03/25 05:10:06 PM: Evaluate: task multirc, batch 149 (1212): ans_f1: 0.5945, qst_f1: 0.5430, em: 0.0078, avg: 0.3011, multirc_loss: 0.7118
03/25 05:10:17 PM: Evaluate: task multirc, batch 162 (1212): ans_f1: 0.5896, qst_f1: 0.5421, em: 0.0074, avg: 0.2985, multirc_loss: 0.7140
03/25 05:10:27 PM: Evaluate: task multirc, batch 176 (1212): ans_f1: 0.5920, qst_f1: 0.5456, em: 0.0070, avg: 0.2995, multirc_loss: 0.7141
03/25 05:10:37 PM: Evaluate: task multirc, batch 189 (1212): ans_f1: 0.5880, qst_f1: 0.5446, em: 0.0066, avg: 0.2973, multirc_loss: 0.7159
03/25 05:10:48 PM: Evaluate: task multirc, batch 202 (1212): ans_f1: 0.5820, qst_f1: 0.5418, em: 0.0063, avg: 0.2941, multirc_loss: 0.7187
03/25 05:10:58 PM: Evaluate: task multirc, batch 216 (1212): ans_f1: 0.5786, qst_f1: 0.5352, em: 0.0060, avg: 0.2923, multirc_loss: 0.7187
03/25 05:11:09 PM: Evaluate: task multirc, batch 229 (1212): ans_f1: 0.5878, qst_f1: 0.5382, em: 0.0057, avg: 0.2968, multirc_loss: 0.7168
03/25 05:11:19 PM: Evaluate: task multirc, batch 243 (1212): ans_f1: 0.5885, qst_f1: 0.5414, em: 0.0053, avg: 0.2969, multirc_loss: 0.7171
03/25 05:11:30 PM: Evaluate: task multirc, batch 256 (1212): ans_f1: 0.5858, qst_f1: 0.5413, em: 0.0050, avg: 0.2954, multirc_loss: 0.7183
03/25 05:11:40 PM: Evaluate: task multirc, batch 269 (1212): ans_f1: 0.5784, qst_f1: 0.5351, em: 0.0048, avg: 0.2916, multirc_loss: 0.7215
03/25 05:11:50 PM: Evaluate: task multirc, batch 282 (1212): ans_f1: 0.5754, qst_f1: 0.5346, em: 0.0092, avg: 0.2923, multirc_loss: 0.7232
03/25 05:12:01 PM: Evaluate: task multirc, batch 296 (1212): ans_f1: 0.5712, qst_f1: 0.5317, em: 0.0044, avg: 0.2878, multirc_loss: 0.7258
03/25 05:12:11 PM: Evaluate: task multirc, batch 309 (1212): ans_f1: 0.5706, qst_f1: 0.5308, em: 0.0042, avg: 0.2874, multirc_loss: 0.7259
03/25 05:12:21 PM: Evaluate: task multirc, batch 323 (1212): ans_f1: 0.5719, qst_f1: 0.5305, em: 0.0041, avg: 0.2880, multirc_loss: 0.7264
03/25 05:12:32 PM: Evaluate: task multirc, batch 337 (1212): ans_f1: 0.5716, qst_f1: 0.5325, em: 0.0039, avg: 0.2878, multirc_loss: 0.7267
03/25 05:12:43 PM: Evaluate: task multirc, batch 351 (1212): ans_f1: 0.5750, qst_f1: 0.5360, em: 0.0038, avg: 0.2894, multirc_loss: 0.7259
03/25 05:12:53 PM: Evaluate: task multirc, batch 365 (1212): ans_f1: 0.5789, qst_f1: 0.5385, em: 0.0036, avg: 0.2913, multirc_loss: 0.7251
03/25 05:13:04 PM: Evaluate: task multirc, batch 378 (1212): ans_f1: 0.5816, qst_f1: 0.5443, em: 0.0069, avg: 0.2942, multirc_loss: 0.7245
03/25 05:13:14 PM: Evaluate: task multirc, batch 391 (1212): ans_f1: 0.5813, qst_f1: 0.5433, em: 0.0033, avg: 0.2923, multirc_loss: 0.7249
03/25 05:13:24 PM: Evaluate: task multirc, batch 404 (1212): ans_f1: 0.5768, qst_f1: 0.5386, em: 0.0064, avg: 0.2916, multirc_loss: 0.7260
03/25 05:13:35 PM: Evaluate: task multirc, batch 418 (1212): ans_f1: 0.5719, qst_f1: 0.5344, em: 0.0062, avg: 0.2890, multirc_loss: 0.7293
03/25 05:13:45 PM: Evaluate: task multirc, batch 431 (1212): ans_f1: 0.5696, qst_f1: 0.5280, em: 0.0090, avg: 0.2893, multirc_loss: 0.7283
03/25 05:13:56 PM: Evaluate: task multirc, batch 444 (1212): ans_f1: 0.5657, qst_f1: 0.5224, em: 0.0087, avg: 0.2872, multirc_loss: 0.7285
03/25 05:14:06 PM: Evaluate: task multirc, batch 457 (1212): ans_f1: 0.5648, qst_f1: 0.5220, em: 0.0085, avg: 0.2867, multirc_loss: 0.7294
03/25 05:14:17 PM: Evaluate: task multirc, batch 470 (1212): ans_f1: 0.5674, qst_f1: 0.5276, em: 0.0082, avg: 0.2878, multirc_loss: 0.7292
03/25 05:14:27 PM: Evaluate: task multirc, batch 483 (1212): ans_f1: 0.5633, qst_f1: 0.5191, em: 0.0079, avg: 0.2856, multirc_loss: 0.7279
03/25 05:14:38 PM: Evaluate: task multirc, batch 497 (1212): ans_f1: 0.5632, qst_f1: 0.5133, em: 0.0077, avg: 0.2855, multirc_loss: 0.7263
03/25 05:14:49 PM: Evaluate: task multirc, batch 511 (1212): ans_f1: 0.5634, qst_f1: 0.5137, em: 0.0075, avg: 0.2854, multirc_loss: 0.7256
03/25 05:15:00 PM: Evaluate: task multirc, batch 525 (1212): ans_f1: 0.5681, qst_f1: 0.5211, em: 0.0072, avg: 0.2876, multirc_loss: 0.7248
03/25 05:15:10 PM: Evaluate: task multirc, batch 538 (1212): ans_f1: 0.5673, qst_f1: 0.5212, em: 0.0094, avg: 0.2883, multirc_loss: 0.7254
03/25 05:15:21 PM: Evaluate: task multirc, batch 551 (1212): ans_f1: 0.5680, qst_f1: 0.5222, em: 0.0069, avg: 0.2874, multirc_loss: 0.7257
03/25 05:15:32 PM: Evaluate: task multirc, batch 564 (1212): ans_f1: 0.5677, qst_f1: 0.5213, em: 0.0067, avg: 0.2872, multirc_loss: 0.7261
03/25 05:15:43 PM: Evaluate: task multirc, batch 578 (1212): ans_f1: 0.5689, qst_f1: 0.5184, em: 0.0066, avg: 0.2877, multirc_loss: 0.7253
03/25 05:15:54 PM: Evaluate: task multirc, batch 592 (1212): ans_f1: 0.5670, qst_f1: 0.5176, em: 0.0065, avg: 0.2867, multirc_loss: 0.7254
03/25 05:16:04 PM: Evaluate: task multirc, batch 605 (1212): ans_f1: 0.5654, qst_f1: 0.5175, em: 0.0063, avg: 0.2858, multirc_loss: 0.7253
03/25 05:16:15 PM: Evaluate: task multirc, batch 618 (1212): ans_f1: 0.5644, qst_f1: 0.5178, em: 0.0083, avg: 0.2864, multirc_loss: 0.7247
03/25 05:16:26 PM: Evaluate: task multirc, batch 632 (1212): ans_f1: 0.5649, qst_f1: 0.5171, em: 0.0081, avg: 0.2865, multirc_loss: 0.7242
03/25 05:16:36 PM: Evaluate: task multirc, batch 646 (1212): ans_f1: 0.5658, qst_f1: 0.5178, em: 0.0059, avg: 0.2858, multirc_loss: 0.7236
03/25 05:16:47 PM: Evaluate: task multirc, batch 660 (1212): ans_f1: 0.5678, qst_f1: 0.5207, em: 0.0058, avg: 0.2868, multirc_loss: 0.7232
03/25 05:16:58 PM: Evaluate: task multirc, batch 674 (1212): ans_f1: 0.5681, qst_f1: 0.5221, em: 0.0056, avg: 0.2869, multirc_loss: 0.7236
03/25 05:17:09 PM: Evaluate: task multirc, batch 686 (1212): ans_f1: 0.5669, qst_f1: 0.5217, em: 0.0056, avg: 0.2862, multirc_loss: 0.7244
03/25 05:17:20 PM: Evaluate: task multirc, batch 698 (1212): ans_f1: 0.5669, qst_f1: 0.5231, em: 0.0055, avg: 0.2862, multirc_loss: 0.7248
03/25 05:17:31 PM: Evaluate: task multirc, batch 711 (1212): ans_f1: 0.5736, qst_f1: 0.5295, em: 0.0072, avg: 0.2904, multirc_loss: 0.7231
03/25 05:17:42 PM: Evaluate: task multirc, batch 725 (1212): ans_f1: 0.5756, qst_f1: 0.5297, em: 0.0053, avg: 0.2905, multirc_loss: 0.7224
03/25 05:17:52 PM: Evaluate: task multirc, batch 738 (1212): ans_f1: 0.5736, qst_f1: 0.5270, em: 0.0052, avg: 0.2894, multirc_loss: 0.7222
03/25 05:18:03 PM: Evaluate: task multirc, batch 752 (1212): ans_f1: 0.5726, qst_f1: 0.5262, em: 0.0051, avg: 0.2889, multirc_loss: 0.7222
03/25 05:18:14 PM: Evaluate: task multirc, batch 765 (1212): ans_f1: 0.5730, qst_f1: 0.5271, em: 0.0051, avg: 0.2890, multirc_loss: 0.7222
03/25 05:18:25 PM: Evaluate: task multirc, batch 778 (1212): ans_f1: 0.5758, qst_f1: 0.5300, em: 0.0066, avg: 0.2912, multirc_loss: 0.7217
03/25 05:18:36 PM: Evaluate: task multirc, batch 791 (1212): ans_f1: 0.5767, qst_f1: 0.5312, em: 0.0064, avg: 0.2916, multirc_loss: 0.7211
03/25 05:18:47 PM: Evaluate: task multirc, batch 804 (1212): ans_f1: 0.5772, qst_f1: 0.5320, em: 0.0063, avg: 0.2918, multirc_loss: 0.7207
03/25 05:18:57 PM: Evaluate: task multirc, batch 817 (1212): ans_f1: 0.5824, qst_f1: 0.5383, em: 0.0123, avg: 0.2974, multirc_loss: 0.7196
03/25 05:19:08 PM: Evaluate: task multirc, batch 831 (1212): ans_f1: 0.5834, qst_f1: 0.5402, em: 0.0167, avg: 0.3001, multirc_loss: 0.7192
03/25 05:19:19 PM: Evaluate: task multirc, batch 845 (1212): ans_f1: 0.5825, qst_f1: 0.5400, em: 0.0164, avg: 0.2994, multirc_loss: 0.7194
03/25 05:19:29 PM: Evaluate: task multirc, batch 858 (1212): ans_f1: 0.5824, qst_f1: 0.5401, em: 0.0161, avg: 0.2992, multirc_loss: 0.7193
03/25 05:19:40 PM: Evaluate: task multirc, batch 871 (1212): ans_f1: 0.5839, qst_f1: 0.5422, em: 0.0157, avg: 0.2998, multirc_loss: 0.7190
03/25 05:19:50 PM: Evaluate: task multirc, batch 884 (1212): ans_f1: 0.5849, qst_f1: 0.5432, em: 0.0155, avg: 0.3002, multirc_loss: 0.7189
03/25 05:20:01 PM: Evaluate: task multirc, batch 897 (1212): ans_f1: 0.5870, qst_f1: 0.5456, em: 0.0152, avg: 0.3011, multirc_loss: 0.7185
03/25 05:20:11 PM: Evaluate: task multirc, batch 910 (1212): ans_f1: 0.5865, qst_f1: 0.5459, em: 0.0150, avg: 0.3008, multirc_loss: 0.7189
03/25 05:20:22 PM: Evaluate: task multirc, batch 923 (1212): ans_f1: 0.5855, qst_f1: 0.5455, em: 0.0147, avg: 0.3001, multirc_loss: 0.7195
03/25 05:20:33 PM: Evaluate: task multirc, batch 937 (1212): ans_f1: 0.5844, qst_f1: 0.5446, em: 0.0145, avg: 0.2994, multirc_loss: 0.7195
03/25 05:20:43 PM: Evaluate: task multirc, batch 950 (1212): ans_f1: 0.5835, qst_f1: 0.5440, em: 0.0143, avg: 0.2989, multirc_loss: 0.7192
03/25 05:20:54 PM: Evaluate: task multirc, batch 963 (1212): ans_f1: 0.5834, qst_f1: 0.5430, em: 0.0155, avg: 0.2994, multirc_loss: 0.7188
03/25 05:21:05 PM: Evaluate: task multirc, batch 977 (1212): ans_f1: 0.5817, qst_f1: 0.5431, em: 0.0153, avg: 0.2985, multirc_loss: 0.7199
03/25 05:21:16 PM: Evaluate: task multirc, batch 990 (1212): ans_f1: 0.5819, qst_f1: 0.5429, em: 0.0151, avg: 0.2985, multirc_loss: 0.7196
03/25 05:21:27 PM: Evaluate: task multirc, batch 1003 (1212): ans_f1: 0.5824, qst_f1: 0.5438, em: 0.0150, avg: 0.2987, multirc_loss: 0.7195
03/25 05:21:38 PM: Evaluate: task multirc, batch 1017 (1212): ans_f1: 0.5809, qst_f1: 0.5427, em: 0.0149, avg: 0.2979, multirc_loss: 0.7197
03/25 05:21:49 PM: Evaluate: task multirc, batch 1030 (1212): ans_f1: 0.5795, qst_f1: 0.5418, em: 0.0147, avg: 0.2971, multirc_loss: 0.7198
03/25 05:21:59 PM: Evaluate: task multirc, batch 1043 (1212): ans_f1: 0.5807, qst_f1: 0.5432, em: 0.0145, avg: 0.2976, multirc_loss: 0.7195
03/25 05:22:10 PM: Evaluate: task multirc, batch 1056 (1212): ans_f1: 0.5805, qst_f1: 0.5434, em: 0.0155, avg: 0.2980, multirc_loss: 0.7194
03/25 05:22:21 PM: Evaluate: task multirc, batch 1069 (1212): ans_f1: 0.5804, qst_f1: 0.5427, em: 0.0141, avg: 0.2973, multirc_loss: 0.7192
03/25 05:22:32 PM: Evaluate: task multirc, batch 1082 (1212): ans_f1: 0.5811, qst_f1: 0.5431, em: 0.0139, avg: 0.2975, multirc_loss: 0.7193
03/25 05:22:43 PM: Evaluate: task multirc, batch 1094 (1212): ans_f1: 0.5819, qst_f1: 0.5444, em: 0.0138, avg: 0.2978, multirc_loss: 0.7193
03/25 05:22:53 PM: Evaluate: task multirc, batch 1108 (1212): ans_f1: 0.5800, qst_f1: 0.5433, em: 0.0137, avg: 0.2968, multirc_loss: 0.7199
03/25 05:23:04 PM: Evaluate: task multirc, batch 1121 (1212): ans_f1: 0.5806, qst_f1: 0.5440, em: 0.0136, avg: 0.2971, multirc_loss: 0.7197
03/25 05:23:14 PM: Evaluate: task multirc, batch 1134 (1212): ans_f1: 0.5782, qst_f1: 0.5421, em: 0.0135, avg: 0.2958, multirc_loss: 0.7202
03/25 05:23:26 PM: Evaluate: task multirc, batch 1148 (1212): ans_f1: 0.5769, qst_f1: 0.5408, em: 0.0133, avg: 0.2951, multirc_loss: 0.7204
03/25 05:23:37 PM: Evaluate: task multirc, batch 1162 (1212): ans_f1: 0.5758, qst_f1: 0.5389, em: 0.0131, avg: 0.2945, multirc_loss: 0.7202
03/25 05:23:48 PM: Evaluate: task multirc, batch 1175 (1212): ans_f1: 0.5743, qst_f1: 0.5346, em: 0.0130, avg: 0.2937, multirc_loss: 0.7199
03/25 05:23:58 PM: Evaluate: task multirc, batch 1188 (1212): ans_f1: 0.5740, qst_f1: 0.5338, em: 0.0128, avg: 0.2934, multirc_loss: 0.7196
03/25 05:24:09 PM: Evaluate: task multirc, batch 1201 (1212): ans_f1: 0.5749, qst_f1: 0.5348, em: 0.0127, avg: 0.2938, multirc_loss: 0.7194
03/25 05:24:18 PM: Best result seen so far for multirc.
03/25 05:24:18 PM: Best result seen so far for micro.
03/25 05:24:18 PM: Best result seen so far for macro.
03/25 05:24:18 PM: Updating LR scheduler:
03/25 05:24:18 PM: 	Best result seen so far for macro_avg: 0.295
03/25 05:24:18 PM: 	# validation passes without improvement: 0
03/25 05:24:18 PM: multirc_loss: training: 0.747366 validation: 0.718930
03/25 05:24:18 PM: macro_avg: validation: 0.294619
03/25 05:24:18 PM: micro_avg: validation: 0.294619
03/25 05:24:18 PM: multirc_ans_f1: training: 0.000000 validation: 0.576647
03/25 05:24:18 PM: multirc_qst_f1: training: 0.000000 validation: 0.535784
03/25 05:24:18 PM: multirc_em: training: 0.250000 validation: 0.012592
03/25 05:24:18 PM: multirc_avg: training: 0.125000 validation: 0.294619
03/25 05:24:18 PM: Global learning rate: 0.0003
03/25 05:24:18 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 05:24:24 PM: Update 6: task multirc, steps since last val 1 (total steps = 6): ans_f1: 0.6667, qst_f1: 0.5000, em: 0.5000, avg: 0.5833, multirc_loss: 0.6456
03/25 05:24:24 PM: ***** Step 6 / Validation 6 *****
03/25 05:24:24 PM: multirc: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 05:24:24 PM: Validating...
03/25 05:24:34 PM: Evaluate: task multirc, batch 13 (1212): ans_f1: 0.6133, qst_f1: 0.6072, em: 0.0000, avg: 0.3067, multirc_loss: 0.7244
03/25 05:24:45 PM: Evaluate: task multirc, batch 26 (1212): ans_f1: 0.6040, qst_f1: 0.5705, em: 0.0000, avg: 0.3020, multirc_loss: 0.7281
03/25 05:24:55 PM: Evaluate: task multirc, batch 39 (1212): ans_f1: 0.6376, qst_f1: 0.6198, em: 0.0000, avg: 0.3188, multirc_loss: 0.7166
03/25 05:25:05 PM: Evaluate: task multirc, batch 52 (1212): ans_f1: 0.6450, qst_f1: 0.6318, em: 0.0208, avg: 0.3329, multirc_loss: 0.7139
03/25 05:25:16 PM: Evaluate: task multirc, batch 65 (1212): ans_f1: 0.6528, qst_f1: 0.6488, em: 0.0161, avg: 0.3345, multirc_loss: 0.7123
03/25 05:25:26 PM: Evaluate: task multirc, batch 78 (1212): ans_f1: 0.6523, qst_f1: 0.6442, em: 0.0141, avg: 0.3332, multirc_loss: 0.7142
03/25 05:25:37 PM: Evaluate: task multirc, batch 92 (1212): ans_f1: 0.6542, qst_f1: 0.6385, em: 0.0244, avg: 0.3393, multirc_loss: 0.7104
03/25 05:25:47 PM: Evaluate: task multirc, batch 106 (1212): ans_f1: 0.6498, qst_f1: 0.6104, em: 0.0316, avg: 0.3407, multirc_loss: 0.7071
03/25 05:25:58 PM: Evaluate: task multirc, batch 120 (1212): ans_f1: 0.6437, qst_f1: 0.5977, em: 0.0286, avg: 0.3361, multirc_loss: 0.7067
03/25 05:26:08 PM: Evaluate: task multirc, batch 133 (1212): ans_f1: 0.6340, qst_f1: 0.5957, em: 0.0263, avg: 0.3302, multirc_loss: 0.7079
03/25 05:26:19 PM: Evaluate: task multirc, batch 146 (1212): ans_f1: 0.6253, qst_f1: 0.5940, em: 0.0242, avg: 0.3248, multirc_loss: 0.7134
03/25 05:26:29 PM: Evaluate: task multirc, batch 158 (1212): ans_f1: 0.6193, qst_f1: 0.5937, em: 0.0301, avg: 0.3247, multirc_loss: 0.7159
03/25 05:26:40 PM: Evaluate: task multirc, batch 172 (1212): ans_f1: 0.6191, qst_f1: 0.5930, em: 0.0213, avg: 0.3202, multirc_loss: 0.7160
03/25 05:26:50 PM: Evaluate: task multirc, batch 186 (1212): ans_f1: 0.6136, qst_f1: 0.5885, em: 0.0201, avg: 0.3169, multirc_loss: 0.7182
03/25 05:27:01 PM: Evaluate: task multirc, batch 199 (1212): ans_f1: 0.6047, qst_f1: 0.5807, em: 0.0191, avg: 0.3119, multirc_loss: 0.7214
03/25 05:27:11 PM: Evaluate: task multirc, batch 213 (1212): ans_f1: 0.6012, qst_f1: 0.5800, em: 0.0242, avg: 0.3127, multirc_loss: 0.7223
03/25 05:27:22 PM: Evaluate: task multirc, batch 226 (1212): ans_f1: 0.6053, qst_f1: 0.5751, em: 0.0173, avg: 0.3113, multirc_loss: 0.7202
03/25 05:27:32 PM: Evaluate: task multirc, batch 240 (1212): ans_f1: 0.6113, qst_f1: 0.5811, em: 0.0162, avg: 0.3138, multirc_loss: 0.7193
03/25 05:27:43 PM: Evaluate: task multirc, batch 253 (1212): ans_f1: 0.6055, qst_f1: 0.5756, em: 0.0153, avg: 0.3104, multirc_loss: 0.7212
03/25 05:27:53 PM: Evaluate: task multirc, batch 267 (1212): ans_f1: 0.5985, qst_f1: 0.5672, em: 0.0144, avg: 0.3064, multirc_loss: 0.7246
03/25 05:28:04 PM: Evaluate: task multirc, batch 281 (1212): ans_f1: 0.5922, qst_f1: 0.5648, em: 0.0138, avg: 0.3030, multirc_loss: 0.7275
03/25 05:28:14 PM: Evaluate: task multirc, batch 295 (1212): ans_f1: 0.5882, qst_f1: 0.5614, em: 0.0132, avg: 0.3007, multirc_loss: 0.7300
03/25 05:28:25 PM: Evaluate: task multirc, batch 309 (1212): ans_f1: 0.5871, qst_f1: 0.5601, em: 0.0127, avg: 0.2999, multirc_loss: 0.7300
03/25 05:28:35 PM: Evaluate: task multirc, batch 322 (1212): ans_f1: 0.5882, qst_f1: 0.5610, em: 0.0123, avg: 0.3003, multirc_loss: 0.7303
03/25 05:28:46 PM: Evaluate: task multirc, batch 336 (1212): ans_f1: 0.5872, qst_f1: 0.5611, em: 0.0157, avg: 0.3015, multirc_loss: 0.7308
03/25 05:28:57 PM: Evaluate: task multirc, batch 350 (1212): ans_f1: 0.5885, qst_f1: 0.5608, em: 0.0114, avg: 0.2999, multirc_loss: 0.7301
03/25 05:29:08 PM: Evaluate: task multirc, batch 364 (1212): ans_f1: 0.5924, qst_f1: 0.5631, em: 0.0108, avg: 0.3016, multirc_loss: 0.7291
03/25 05:29:18 PM: Evaluate: task multirc, batch 377 (1212): ans_f1: 0.5939, qst_f1: 0.5661, em: 0.0104, avg: 0.3021, multirc_loss: 0.7286
03/25 05:29:29 PM: Evaluate: task multirc, batch 390 (1212): ans_f1: 0.5945, qst_f1: 0.5655, em: 0.0100, avg: 0.3023, multirc_loss: 0.7286
03/25 05:29:39 PM: Evaluate: task multirc, batch 403 (1212): ans_f1: 0.5897, qst_f1: 0.5598, em: 0.0128, avg: 0.3012, multirc_loss: 0.7300
03/25 05:29:50 PM: Evaluate: task multirc, batch 417 (1212): ans_f1: 0.5833, qst_f1: 0.5547, em: 0.0123, avg: 0.2978, multirc_loss: 0.7338
03/25 05:30:00 PM: Evaluate: task multirc, batch 430 (1212): ans_f1: 0.5819, qst_f1: 0.5485, em: 0.0120, avg: 0.2970, multirc_loss: 0.7329
03/25 05:30:11 PM: Evaluate: task multirc, batch 442 (1212): ans_f1: 0.5792, qst_f1: 0.5434, em: 0.0117, avg: 0.2955, multirc_loss: 0.7327
03/25 05:30:22 PM: Evaluate: task multirc, batch 455 (1212): ans_f1: 0.5785, qst_f1: 0.5448, em: 0.0114, avg: 0.2949, multirc_loss: 0.7335
03/25 05:30:33 PM: Evaluate: task multirc, batch 467 (1212): ans_f1: 0.5788, qst_f1: 0.5466, em: 0.0110, avg: 0.2949, multirc_loss: 0.7339
03/25 05:30:43 PM: Evaluate: task multirc, batch 479 (1212): ans_f1: 0.5787, qst_f1: 0.5439, em: 0.0107, avg: 0.2947, multirc_loss: 0.7324
03/25 05:30:54 PM: Evaluate: task multirc, batch 491 (1212): ans_f1: 0.5752, qst_f1: 0.5332, em: 0.0104, avg: 0.2928, multirc_loss: 0.7312
03/25 05:31:04 PM: Evaluate: task multirc, batch 503 (1212): ans_f1: 0.5783, qst_f1: 0.5373, em: 0.0127, avg: 0.2955, multirc_loss: 0.7305
03/25 05:31:15 PM: Evaluate: task multirc, batch 515 (1212): ans_f1: 0.5811, qst_f1: 0.5412, em: 0.0099, avg: 0.2955, multirc_loss: 0.7294
03/25 05:31:26 PM: Evaluate: task multirc, batch 527 (1212): ans_f1: 0.5838, qst_f1: 0.5454, em: 0.0096, avg: 0.2967, multirc_loss: 0.7289
03/25 05:31:36 PM: Evaluate: task multirc, batch 539 (1212): ans_f1: 0.5825, qst_f1: 0.5449, em: 0.0117, avg: 0.2971, multirc_loss: 0.7295
03/25 05:31:48 PM: Evaluate: task multirc, batch 552 (1212): ans_f1: 0.5823, qst_f1: 0.5447, em: 0.0091, avg: 0.2957, multirc_loss: 0.7300
03/25 05:31:59 PM: Evaluate: task multirc, batch 565 (1212): ans_f1: 0.5818, qst_f1: 0.5424, em: 0.0089, avg: 0.2954, multirc_loss: 0.7304
03/25 05:32:09 PM: Evaluate: task multirc, batch 578 (1212): ans_f1: 0.5827, qst_f1: 0.5401, em: 0.0088, avg: 0.2958, multirc_loss: 0.7296
03/25 05:32:20 PM: Evaluate: task multirc, batch 591 (1212): ans_f1: 0.5814, qst_f1: 0.5403, em: 0.0086, avg: 0.2950, multirc_loss: 0.7297
03/25 05:32:30 PM: Evaluate: task multirc, batch 604 (1212): ans_f1: 0.5795, qst_f1: 0.5394, em: 0.0084, avg: 0.2940, multirc_loss: 0.7298
03/25 05:32:41 PM: Evaluate: task multirc, batch 617 (1212): ans_f1: 0.5789, qst_f1: 0.5386, em: 0.0083, avg: 0.2936, multirc_loss: 0.7293
03/25 05:32:51 PM: Evaluate: task multirc, batch 630 (1212): ans_f1: 0.5784, qst_f1: 0.5386, em: 0.0081, avg: 0.2933, multirc_loss: 0.7289
03/25 05:33:02 PM: Evaluate: task multirc, batch 644 (1212): ans_f1: 0.5793, qst_f1: 0.5387, em: 0.0079, avg: 0.2936, multirc_loss: 0.7281
03/25 05:33:13 PM: Evaluate: task multirc, batch 656 (1212): ans_f1: 0.5798, qst_f1: 0.5394, em: 0.0078, avg: 0.2938, multirc_loss: 0.7279
03/25 05:33:24 PM: Evaluate: task multirc, batch 665 (1212): ans_f1: 0.5816, qst_f1: 0.5420, em: 0.0076, avg: 0.2946, multirc_loss: 0.7277
03/25 05:33:35 PM: Evaluate: task multirc, batch 678 (1212): ans_f1: 0.5810, qst_f1: 0.5422, em: 0.0075, avg: 0.2943, multirc_loss: 0.7283
03/25 05:33:46 PM: Evaluate: task multirc, batch 691 (1212): ans_f1: 0.5786, qst_f1: 0.5408, em: 0.0074, avg: 0.2930, multirc_loss: 0.7293
03/25 05:33:57 PM: Evaluate: task multirc, batch 704 (1212): ans_f1: 0.5827, qst_f1: 0.5454, em: 0.0091, avg: 0.2959, multirc_loss: 0.7284
03/25 05:34:08 PM: Evaluate: task multirc, batch 717 (1212): ans_f1: 0.5866, qst_f1: 0.5478, em: 0.0071, avg: 0.2969, multirc_loss: 0.7272
03/25 05:34:18 PM: Evaluate: task multirc, batch 730 (1212): ans_f1: 0.5872, qst_f1: 0.5481, em: 0.0070, avg: 0.2971, multirc_loss: 0.7266
03/25 05:34:29 PM: Evaluate: task multirc, batch 743 (1212): ans_f1: 0.5855, qst_f1: 0.5461, em: 0.0069, avg: 0.2962, multirc_loss: 0.7266
03/25 05:34:39 PM: Evaluate: task multirc, batch 756 (1212): ans_f1: 0.5853, qst_f1: 0.5469, em: 0.0068, avg: 0.2961, multirc_loss: 0.7265
03/25 05:34:50 PM: Evaluate: task multirc, batch 769 (1212): ans_f1: 0.5865, qst_f1: 0.5482, em: 0.0067, avg: 0.2966, multirc_loss: 0.7262
03/25 05:35:01 PM: Evaluate: task multirc, batch 782 (1212): ans_f1: 0.5887, qst_f1: 0.5518, em: 0.0082, avg: 0.2984, multirc_loss: 0.7257
03/25 05:35:12 PM: Evaluate: task multirc, batch 795 (1212): ans_f1: 0.5882, qst_f1: 0.5498, em: 0.0080, avg: 0.2981, multirc_loss: 0.7252
03/25 05:35:23 PM: Evaluate: task multirc, batch 809 (1212): ans_f1: 0.5915, qst_f1: 0.5531, em: 0.0094, avg: 0.3005, multirc_loss: 0.7240
03/25 05:35:33 PM: Evaluate: task multirc, batch 823 (1212): ans_f1: 0.5949, qst_f1: 0.5585, em: 0.0153, avg: 0.3051, multirc_loss: 0.7233
03/25 05:35:44 PM: Evaluate: task multirc, batch 836 (1212): ans_f1: 0.5939, qst_f1: 0.5581, em: 0.0166, avg: 0.3052, multirc_loss: 0.7232
03/25 05:35:55 PM: Evaluate: task multirc, batch 850 (1212): ans_f1: 0.5931, qst_f1: 0.5576, em: 0.0163, avg: 0.3047, multirc_loss: 0.7234
03/25 05:36:06 PM: Evaluate: task multirc, batch 863 (1212): ans_f1: 0.5934, qst_f1: 0.5587, em: 0.0159, avg: 0.3047, multirc_loss: 0.7233
03/25 05:36:17 PM: Evaluate: task multirc, batch 877 (1212): ans_f1: 0.5945, qst_f1: 0.5597, em: 0.0156, avg: 0.3051, multirc_loss: 0.7231
03/25 05:36:28 PM: Evaluate: task multirc, batch 891 (1212): ans_f1: 0.5969, qst_f1: 0.5630, em: 0.0167, avg: 0.3068, multirc_loss: 0.7225
03/25 05:36:39 PM: Evaluate: task multirc, batch 904 (1212): ans_f1: 0.5978, qst_f1: 0.5640, em: 0.0151, avg: 0.3064, multirc_loss: 0.7224
03/25 05:36:49 PM: Evaluate: task multirc, batch 917 (1212): ans_f1: 0.5960, qst_f1: 0.5627, em: 0.0149, avg: 0.3055, multirc_loss: 0.7233
03/25 05:37:00 PM: Evaluate: task multirc, batch 931 (1212): ans_f1: 0.5954, qst_f1: 0.5626, em: 0.0146, avg: 0.3050, multirc_loss: 0.7235
03/25 05:37:10 PM: Evaluate: task multirc, batch 945 (1212): ans_f1: 0.5942, qst_f1: 0.5619, em: 0.0144, avg: 0.3043, multirc_loss: 0.7233
03/25 05:37:21 PM: Evaluate: task multirc, batch 957 (1212): ans_f1: 0.5950, qst_f1: 0.5625, em: 0.0142, avg: 0.3046, multirc_loss: 0.7229
03/25 05:37:32 PM: Evaluate: task multirc, batch 971 (1212): ans_f1: 0.5947, qst_f1: 0.5630, em: 0.0141, avg: 0.3044, multirc_loss: 0.7230
03/25 05:37:43 PM: Evaluate: task multirc, batch 984 (1212): ans_f1: 0.5939, qst_f1: 0.5625, em: 0.0139, avg: 0.3039, multirc_loss: 0.7237
03/25 05:37:54 PM: Evaluate: task multirc, batch 997 (1212): ans_f1: 0.5945, qst_f1: 0.5629, em: 0.0138, avg: 0.3041, multirc_loss: 0.7236
03/25 05:38:05 PM: Evaluate: task multirc, batch 1011 (1212): ans_f1: 0.5928, qst_f1: 0.5614, em: 0.0137, avg: 0.3032, multirc_loss: 0.7239
03/25 05:38:16 PM: Evaluate: task multirc, batch 1024 (1212): ans_f1: 0.5923, qst_f1: 0.5612, em: 0.0135, avg: 0.3029, multirc_loss: 0.7239
03/25 05:38:26 PM: Evaluate: task multirc, batch 1037 (1212): ans_f1: 0.5926, qst_f1: 0.5620, em: 0.0134, avg: 0.3030, multirc_loss: 0.7239
03/25 05:38:37 PM: Evaluate: task multirc, batch 1050 (1212): ans_f1: 0.5919, qst_f1: 0.5609, em: 0.0132, avg: 0.3026, multirc_loss: 0.7238
03/25 05:38:47 PM: Evaluate: task multirc, batch 1063 (1212): ans_f1: 0.5924, qst_f1: 0.5618, em: 0.0130, avg: 0.3027, multirc_loss: 0.7236
03/25 05:38:58 PM: Evaluate: task multirc, batch 1076 (1212): ans_f1: 0.5929, qst_f1: 0.5623, em: 0.0128, avg: 0.3029, multirc_loss: 0.7234
03/25 05:39:09 PM: Evaluate: task multirc, batch 1089 (1212): ans_f1: 0.5939, qst_f1: 0.5635, em: 0.0138, avg: 0.3039, multirc_loss: 0.7234
03/25 05:39:20 PM: Evaluate: task multirc, batch 1102 (1212): ans_f1: 0.5925, qst_f1: 0.5621, em: 0.0126, avg: 0.3025, multirc_loss: 0.7241
03/25 05:39:31 PM: Evaluate: task multirc, batch 1116 (1212): ans_f1: 0.5926, qst_f1: 0.5626, em: 0.0125, avg: 0.3025, multirc_loss: 0.7240
03/25 05:39:42 PM: Evaluate: task multirc, batch 1130 (1212): ans_f1: 0.5902, qst_f1: 0.5611, em: 0.0124, avg: 0.3013, multirc_loss: 0.7244
03/25 05:39:53 PM: Evaluate: task multirc, batch 1144 (1212): ans_f1: 0.5886, qst_f1: 0.5599, em: 0.0122, avg: 0.3004, multirc_loss: 0.7248
03/25 05:40:04 PM: Evaluate: task multirc, batch 1157 (1212): ans_f1: 0.5873, qst_f1: 0.5583, em: 0.0121, avg: 0.2997, multirc_loss: 0.7248
03/25 05:40:15 PM: Evaluate: task multirc, batch 1170 (1212): ans_f1: 0.5861, qst_f1: 0.5561, em: 0.0119, avg: 0.2990, multirc_loss: 0.7245
03/25 05:40:26 PM: Evaluate: task multirc, batch 1184 (1212): ans_f1: 0.5853, qst_f1: 0.5530, em: 0.0118, avg: 0.2985, multirc_loss: 0.7242
03/25 05:40:36 PM: Evaluate: task multirc, batch 1197 (1212): ans_f1: 0.5858, qst_f1: 0.5544, em: 0.0128, avg: 0.2993, multirc_loss: 0.7239
03/25 05:40:47 PM: Evaluate: task multirc, batch 1211 (1212): ans_f1: 0.5880, qst_f1: 0.5558, em: 0.0116, avg: 0.2998, multirc_loss: 0.7233
03/25 05:40:49 PM: Best result seen so far for multirc.
03/25 05:40:49 PM: Best result seen so far for micro.
03/25 05:40:49 PM: Best result seen so far for macro.
03/25 05:40:49 PM: Updating LR scheduler:
03/25 05:40:49 PM: 	Best result seen so far for macro_avg: 0.300
03/25 05:40:49 PM: 	# validation passes without improvement: 0
03/25 05:40:49 PM: multirc_loss: training: 0.645580 validation: 0.723272
03/25 05:40:49 PM: macro_avg: validation: 0.299611
03/25 05:40:49 PM: micro_avg: validation: 0.299611
03/25 05:40:49 PM: multirc_ans_f1: training: 0.666667 validation: 0.587679
03/25 05:40:49 PM: multirc_qst_f1: training: 0.500000 validation: 0.555208
03/25 05:40:49 PM: multirc_em: training: 0.500000 validation: 0.011542
03/25 05:40:49 PM: multirc_avg: training: 0.583333 validation: 0.299611
03/25 05:40:49 PM: Global learning rate: 0.0003
03/25 05:40:49 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 05:40:54 PM: ***** Step 7 / Validation 7 *****
03/25 05:40:54 PM: multirc: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 05:40:54 PM: Validating...
03/25 05:40:58 PM: Evaluate: task multirc, batch 5 (1212): ans_f1: 0.6207, qst_f1: 0.5913, em: 0.0000, avg: 0.3103, multirc_loss: 0.7130
03/25 05:41:09 PM: Evaluate: task multirc, batch 18 (1212): ans_f1: 0.6019, qst_f1: 0.5615, em: 0.0000, avg: 0.3010, multirc_loss: 0.7350
03/25 05:41:19 PM: Evaluate: task multirc, batch 31 (1212): ans_f1: 0.6595, qst_f1: 0.6631, em: 0.0357, avg: 0.3476, multirc_loss: 0.7202
03/25 05:41:30 PM: Evaluate: task multirc, batch 44 (1212): ans_f1: 0.6462, qst_f1: 0.6337, em: 0.0244, avg: 0.3353, multirc_loss: 0.7185
03/25 05:41:40 PM: Evaluate: task multirc, batch 57 (1212): ans_f1: 0.6628, qst_f1: 0.6580, em: 0.0182, avg: 0.3405, multirc_loss: 0.7112
03/25 05:41:50 PM: Evaluate: task multirc, batch 70 (1212): ans_f1: 0.6473, qst_f1: 0.6468, em: 0.0152, avg: 0.3312, multirc_loss: 0.7200
03/25 05:42:01 PM: Evaluate: task multirc, batch 84 (1212): ans_f1: 0.6614, qst_f1: 0.6478, em: 0.0132, avg: 0.3373, multirc_loss: 0.7151
03/25 05:42:12 PM: Evaluate: task multirc, batch 98 (1212): ans_f1: 0.6559, qst_f1: 0.6347, em: 0.0227, avg: 0.3393, multirc_loss: 0.7106
03/25 05:42:22 PM: Evaluate: task multirc, batch 112 (1212): ans_f1: 0.6635, qst_f1: 0.6330, em: 0.0100, avg: 0.3368, multirc_loss: 0.7068
03/25 05:42:32 PM: Evaluate: task multirc, batch 125 (1212): ans_f1: 0.6573, qst_f1: 0.6278, em: 0.0093, avg: 0.3333, multirc_loss: 0.7068
03/25 05:42:42 PM: Evaluate: task multirc, batch 137 (1212): ans_f1: 0.6452, qst_f1: 0.6180, em: 0.0085, avg: 0.3268, multirc_loss: 0.7096
03/25 05:42:53 PM: Evaluate: task multirc, batch 150 (1212): ans_f1: 0.6359, qst_f1: 0.6140, em: 0.0078, avg: 0.3219, multirc_loss: 0.7149
03/25 05:43:04 PM: Evaluate: task multirc, batch 163 (1212): ans_f1: 0.6281, qst_f1: 0.6106, em: 0.0074, avg: 0.3177, multirc_loss: 0.7178
03/25 05:43:14 PM: Evaluate: task multirc, batch 177 (1212): ans_f1: 0.6231, qst_f1: 0.6035, em: 0.0069, avg: 0.3150, multirc_loss: 0.7196
03/25 05:43:24 PM: Evaluate: task multirc, batch 190 (1212): ans_f1: 0.6173, qst_f1: 0.5988, em: 0.0066, avg: 0.3119, multirc_loss: 0.7216
03/25 05:43:35 PM: Evaluate: task multirc, batch 203 (1212): ans_f1: 0.6121, qst_f1: 0.5981, em: 0.0063, avg: 0.3092, multirc_loss: 0.7240
03/25 05:43:45 PM: Evaluate: task multirc, batch 217 (1212): ans_f1: 0.6093, qst_f1: 0.5877, em: 0.0060, avg: 0.3076, multirc_loss: 0.7241
03/25 05:43:56 PM: Evaluate: task multirc, batch 230 (1212): ans_f1: 0.6169, qst_f1: 0.5958, em: 0.0057, avg: 0.3113, multirc_loss: 0.7218
03/25 05:44:06 PM: Evaluate: task multirc, batch 244 (1212): ans_f1: 0.6149, qst_f1: 0.5933, em: 0.0053, avg: 0.3101, multirc_loss: 0.7227
03/25 05:44:17 PM: Evaluate: task multirc, batch 257 (1212): ans_f1: 0.6110, qst_f1: 0.5886, em: 0.0050, avg: 0.3080, multirc_loss: 0.7245
03/25 05:44:27 PM: Evaluate: task multirc, batch 271 (1212): ans_f1: 0.6021, qst_f1: 0.5812, em: 0.0047, avg: 0.3034, multirc_loss: 0.7287
03/25 05:44:38 PM: Evaluate: task multirc, batch 285 (1212): ans_f1: 0.5967, qst_f1: 0.5745, em: 0.0045, avg: 0.3006, multirc_loss: 0.7322
03/25 05:44:48 PM: Evaluate: task multirc, batch 299 (1212): ans_f1: 0.5943, qst_f1: 0.5748, em: 0.0043, avg: 0.2993, multirc_loss: 0.7334
03/25 05:44:58 PM: Evaluate: task multirc, batch 312 (1212): ans_f1: 0.5919, qst_f1: 0.5718, em: 0.0042, avg: 0.2981, multirc_loss: 0.7341
03/25 05:45:09 PM: Evaluate: task multirc, batch 326 (1212): ans_f1: 0.5923, qst_f1: 0.5722, em: 0.0041, avg: 0.2982, multirc_loss: 0.7342
03/25 05:45:20 PM: Evaluate: task multirc, batch 340 (1212): ans_f1: 0.5926, qst_f1: 0.5724, em: 0.0039, avg: 0.2982, multirc_loss: 0.7341
03/25 05:45:31 PM: Evaluate: task multirc, batch 354 (1212): ans_f1: 0.5957, qst_f1: 0.5737, em: 0.0038, avg: 0.2998, multirc_loss: 0.7329
03/25 05:45:41 PM: Evaluate: task multirc, batch 368 (1212): ans_f1: 0.5980, qst_f1: 0.5762, em: 0.0036, avg: 0.3008, multirc_loss: 0.7323
03/25 05:45:52 PM: Evaluate: task multirc, batch 381 (1212): ans_f1: 0.5985, qst_f1: 0.5763, em: 0.0034, avg: 0.3010, multirc_loss: 0.7321
03/25 05:46:02 PM: Evaluate: task multirc, batch 394 (1212): ans_f1: 0.5971, qst_f1: 0.5734, em: 0.0033, avg: 0.3002, multirc_loss: 0.7327
03/25 05:46:12 PM: Evaluate: task multirc, batch 407 (1212): ans_f1: 0.5926, qst_f1: 0.5688, em: 0.0032, avg: 0.2979, multirc_loss: 0.7341
03/25 05:46:23 PM: Evaluate: task multirc, batch 421 (1212): ans_f1: 0.5879, qst_f1: 0.5639, em: 0.0031, avg: 0.2955, multirc_loss: 0.7378
03/25 05:46:33 PM: Evaluate: task multirc, batch 434 (1212): ans_f1: 0.5844, qst_f1: 0.5536, em: 0.0030, avg: 0.2937, multirc_loss: 0.7367
03/25 05:46:44 PM: Evaluate: task multirc, batch 447 (1212): ans_f1: 0.5814, qst_f1: 0.5500, em: 0.0029, avg: 0.2921, multirc_loss: 0.7373
03/25 05:46:54 PM: Evaluate: task multirc, batch 460 (1212): ans_f1: 0.5801, qst_f1: 0.5496, em: 0.0028, avg: 0.2914, multirc_loss: 0.7385
03/25 05:47:05 PM: Evaluate: task multirc, batch 473 (1212): ans_f1: 0.5824, qst_f1: 0.5534, em: 0.0027, avg: 0.2925, multirc_loss: 0.7372
03/25 05:47:16 PM: Evaluate: task multirc, batch 487 (1212): ans_f1: 0.5789, qst_f1: 0.5423, em: 0.0052, avg: 0.2921, multirc_loss: 0.7358
03/25 05:47:26 PM: Evaluate: task multirc, batch 501 (1212): ans_f1: 0.5813, qst_f1: 0.5444, em: 0.0051, avg: 0.2932, multirc_loss: 0.7346
03/25 05:47:37 PM: Evaluate: task multirc, batch 515 (1212): ans_f1: 0.5844, qst_f1: 0.5494, em: 0.0049, avg: 0.2947, multirc_loss: 0.7334
03/25 05:47:48 PM: Evaluate: task multirc, batch 528 (1212): ans_f1: 0.5867, qst_f1: 0.5533, em: 0.0048, avg: 0.2957, multirc_loss: 0.7329
03/25 05:47:59 PM: Evaluate: task multirc, batch 542 (1212): ans_f1: 0.5876, qst_f1: 0.5545, em: 0.0047, avg: 0.2961, multirc_loss: 0.7331
03/25 05:48:09 PM: Evaluate: task multirc, batch 554 (1212): ans_f1: 0.5861, qst_f1: 0.5537, em: 0.0046, avg: 0.2954, multirc_loss: 0.7342
03/25 05:48:20 PM: Evaluate: task multirc, batch 567 (1212): ans_f1: 0.5851, qst_f1: 0.5475, em: 0.0044, avg: 0.2948, multirc_loss: 0.7343
03/25 05:48:30 PM: Evaluate: task multirc, batch 581 (1212): ans_f1: 0.5868, qst_f1: 0.5502, em: 0.0044, avg: 0.2956, multirc_loss: 0.7333
03/25 05:48:41 PM: Evaluate: task multirc, batch 594 (1212): ans_f1: 0.5844, qst_f1: 0.5484, em: 0.0043, avg: 0.2943, multirc_loss: 0.7338
03/25 05:48:51 PM: Evaluate: task multirc, batch 607 (1212): ans_f1: 0.5829, qst_f1: 0.5474, em: 0.0042, avg: 0.2935, multirc_loss: 0.7339
03/25 05:49:02 PM: Evaluate: task multirc, batch 621 (1212): ans_f1: 0.5819, qst_f1: 0.5470, em: 0.0041, avg: 0.2930, multirc_loss: 0.7333
03/25 05:49:12 PM: Evaluate: task multirc, batch 635 (1212): ans_f1: 0.5815, qst_f1: 0.5464, em: 0.0040, avg: 0.2927, multirc_loss: 0.7328
03/25 05:49:23 PM: Evaluate: task multirc, batch 649 (1212): ans_f1: 0.5823, qst_f1: 0.5465, em: 0.0039, avg: 0.2931, multirc_loss: 0.7323
03/25 05:49:34 PM: Evaluate: task multirc, batch 663 (1212): ans_f1: 0.5847, qst_f1: 0.5495, em: 0.0038, avg: 0.2942, multirc_loss: 0.7317
03/25 05:49:44 PM: Evaluate: task multirc, batch 676 (1212): ans_f1: 0.5845, qst_f1: 0.5509, em: 0.0038, avg: 0.2941, multirc_loss: 0.7321
03/25 05:49:54 PM: Evaluate: task multirc, batch 688 (1212): ans_f1: 0.5823, qst_f1: 0.5494, em: 0.0037, avg: 0.2930, multirc_loss: 0.7332
03/25 05:50:05 PM: Evaluate: task multirc, batch 700 (1212): ans_f1: 0.5839, qst_f1: 0.5518, em: 0.0055, avg: 0.2947, multirc_loss: 0.7330
03/25 05:50:16 PM: Evaluate: task multirc, batch 713 (1212): ans_f1: 0.5888, qst_f1: 0.5565, em: 0.0036, avg: 0.2962, multirc_loss: 0.7314
03/25 05:50:26 PM: Evaluate: task multirc, batch 726 (1212): ans_f1: 0.5902, qst_f1: 0.5568, em: 0.0035, avg: 0.2969, multirc_loss: 0.7306
03/25 05:50:37 PM: Evaluate: task multirc, batch 739 (1212): ans_f1: 0.5888, qst_f1: 0.5552, em: 0.0035, avg: 0.2961, multirc_loss: 0.7304
03/25 05:50:47 PM: Evaluate: task multirc, batch 752 (1212): ans_f1: 0.5875, qst_f1: 0.5539, em: 0.0034, avg: 0.2955, multirc_loss: 0.7305
03/25 05:50:58 PM: Evaluate: task multirc, batch 765 (1212): ans_f1: 0.5876, qst_f1: 0.5544, em: 0.0034, avg: 0.2955, multirc_loss: 0.7305
03/25 05:51:08 PM: Evaluate: task multirc, batch 778 (1212): ans_f1: 0.5901, qst_f1: 0.5567, em: 0.0049, avg: 0.2975, multirc_loss: 0.7298
03/25 05:51:19 PM: Evaluate: task multirc, batch 791 (1212): ans_f1: 0.5901, qst_f1: 0.5573, em: 0.0048, avg: 0.2974, multirc_loss: 0.7292
03/25 05:51:29 PM: Evaluate: task multirc, batch 804 (1212): ans_f1: 0.5908, qst_f1: 0.5590, em: 0.0047, avg: 0.2978, multirc_loss: 0.7286
03/25 05:51:40 PM: Evaluate: task multirc, batch 818 (1212): ans_f1: 0.5961, qst_f1: 0.5653, em: 0.0108, avg: 0.3035, multirc_loss: 0.7273
03/25 05:51:51 PM: Evaluate: task multirc, batch 832 (1212): ans_f1: 0.5963, qst_f1: 0.5664, em: 0.0136, avg: 0.3050, multirc_loss: 0.7269
03/25 05:52:01 PM: Evaluate: task multirc, batch 846 (1212): ans_f1: 0.5952, qst_f1: 0.5656, em: 0.0134, avg: 0.3043, multirc_loss: 0.7273
03/25 05:52:12 PM: Evaluate: task multirc, batch 859 (1212): ans_f1: 0.5954, qst_f1: 0.5665, em: 0.0146, avg: 0.3050, multirc_loss: 0.7271
03/25 05:52:23 PM: Evaluate: task multirc, batch 873 (1212): ans_f1: 0.5964, qst_f1: 0.5675, em: 0.0128, avg: 0.3046, multirc_loss: 0.7270
03/25 05:52:33 PM: Evaluate: task multirc, batch 886 (1212): ans_f1: 0.5988, qst_f1: 0.5710, em: 0.0140, avg: 0.3064, multirc_loss: 0.7264
03/25 05:52:44 PM: Evaluate: task multirc, batch 899 (1212): ans_f1: 0.6010, qst_f1: 0.5732, em: 0.0138, avg: 0.3074, multirc_loss: 0.7258
03/25 05:52:55 PM: Evaluate: task multirc, batch 913 (1212): ans_f1: 0.5993, qst_f1: 0.5717, em: 0.0122, avg: 0.3058, multirc_loss: 0.7266
03/25 05:53:06 PM: Evaluate: task multirc, batch 927 (1212): ans_f1: 0.5992, qst_f1: 0.5722, em: 0.0120, avg: 0.3056, multirc_loss: 0.7269
03/25 05:53:17 PM: Evaluate: task multirc, batch 941 (1212): ans_f1: 0.5974, qst_f1: 0.5705, em: 0.0118, avg: 0.3046, multirc_loss: 0.7270
03/25 05:53:27 PM: Evaluate: task multirc, batch 954 (1212): ans_f1: 0.5974, qst_f1: 0.5700, em: 0.0117, avg: 0.3045, multirc_loss: 0.7267
03/25 05:53:38 PM: Evaluate: task multirc, batch 967 (1212): ans_f1: 0.5976, qst_f1: 0.5711, em: 0.0116, avg: 0.3046, multirc_loss: 0.7265
03/25 05:53:49 PM: Evaluate: task multirc, batch 981 (1212): ans_f1: 0.5968, qst_f1: 0.5710, em: 0.0127, avg: 0.3048, multirc_loss: 0.7274
03/25 05:54:00 PM: Evaluate: task multirc, batch 994 (1212): ans_f1: 0.5968, qst_f1: 0.5701, em: 0.0113, avg: 0.3041, multirc_loss: 0.7276
03/25 05:54:11 PM: Evaluate: task multirc, batch 1008 (1212): ans_f1: 0.5961, qst_f1: 0.5701, em: 0.0112, avg: 0.3036, multirc_loss: 0.7276
03/25 05:54:22 PM: Evaluate: task multirc, batch 1021 (1212): ans_f1: 0.5950, qst_f1: 0.5694, em: 0.0111, avg: 0.3031, multirc_loss: 0.7279
03/25 05:54:32 PM: Evaluate: task multirc, batch 1034 (1212): ans_f1: 0.5945, qst_f1: 0.5692, em: 0.0110, avg: 0.3028, multirc_loss: 0.7280
03/25 05:54:43 PM: Evaluate: task multirc, batch 1047 (1212): ans_f1: 0.5945, qst_f1: 0.5683, em: 0.0108, avg: 0.3027, multirc_loss: 0.7277
03/25 05:54:53 PM: Evaluate: task multirc, batch 1060 (1212): ans_f1: 0.5950, qst_f1: 0.5698, em: 0.0107, avg: 0.3028, multirc_loss: 0.7275
03/25 05:55:04 PM: Evaluate: task multirc, batch 1073 (1212): ans_f1: 0.5952, qst_f1: 0.5693, em: 0.0105, avg: 0.3029, multirc_loss: 0.7273
03/25 05:55:15 PM: Evaluate: task multirc, batch 1086 (1212): ans_f1: 0.5959, qst_f1: 0.5709, em: 0.0115, avg: 0.3037, multirc_loss: 0.7273
03/25 05:55:26 PM: Evaluate: task multirc, batch 1098 (1212): ans_f1: 0.5954, qst_f1: 0.5695, em: 0.0103, avg: 0.3028, multirc_loss: 0.7277
03/25 05:55:37 PM: Evaluate: task multirc, batch 1112 (1212): ans_f1: 0.5943, qst_f1: 0.5694, em: 0.0102, avg: 0.3023, multirc_loss: 0.7280
03/25 05:55:48 PM: Evaluate: task multirc, batch 1126 (1212): ans_f1: 0.5935, qst_f1: 0.5690, em: 0.0101, avg: 0.3018, multirc_loss: 0.7281
03/25 05:55:59 PM: Evaluate: task multirc, batch 1140 (1212): ans_f1: 0.5917, qst_f1: 0.5680, em: 0.0100, avg: 0.3009, multirc_loss: 0.7288
03/25 05:56:10 PM: Evaluate: task multirc, batch 1154 (1212): ans_f1: 0.5906, qst_f1: 0.5662, em: 0.0099, avg: 0.3003, multirc_loss: 0.7288
03/25 05:56:21 PM: Evaluate: task multirc, batch 1167 (1212): ans_f1: 0.5896, qst_f1: 0.5647, em: 0.0098, avg: 0.2997, multirc_loss: 0.7286
03/25 05:56:31 PM: Evaluate: task multirc, batch 1181 (1212): ans_f1: 0.5884, qst_f1: 0.5614, em: 0.0097, avg: 0.2990, multirc_loss: 0.7283
03/25 05:56:42 PM: Evaluate: task multirc, batch 1194 (1212): ans_f1: 0.5887, qst_f1: 0.5618, em: 0.0096, avg: 0.2991, multirc_loss: 0.7280
03/25 05:56:53 PM: Evaluate: task multirc, batch 1207 (1212): ans_f1: 0.5900, qst_f1: 0.5626, em: 0.0095, avg: 0.2998, multirc_loss: 0.7275
03/25 05:56:57 PM: Best result seen so far for multirc.
03/25 05:56:57 PM: Best result seen so far for micro.
03/25 05:56:57 PM: Best result seen so far for macro.
03/25 05:56:57 PM: Updating LR scheduler:
03/25 05:56:57 PM: 	Best result seen so far for macro_avg: 0.300
03/25 05:56:57 PM: 	# validation passes without improvement: 0
03/25 05:56:57 PM: multirc_loss: training: 0.675376 validation: 0.727207
03/25 05:56:57 PM: macro_avg: validation: 0.300053
03/25 05:56:57 PM: micro_avg: validation: 0.300053
03/25 05:56:57 PM: multirc_ans_f1: training: 0.400000 validation: 0.590663
03/25 05:56:57 PM: multirc_qst_f1: training: 0.250000 validation: 0.562989
03/25 05:56:57 PM: multirc_em: training: 0.250000 validation: 0.009444
03/25 05:56:57 PM: multirc_avg: training: 0.325000 validation: 0.300053
03/25 05:56:57 PM: Global learning rate: 0.0003
03/25 05:56:57 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 05:57:03 PM: Update 8: task multirc, steps since last val 1 (total steps = 8): ans_f1: 0.4000, qst_f1: 0.2500, em: 0.2500, avg: 0.3250, multirc_loss: 0.7542
03/25 05:57:03 PM: ***** Step 8 / Validation 8 *****
03/25 05:57:03 PM: multirc: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 05:57:03 PM: Validating...
03/25 05:57:13 PM: Evaluate: task multirc, batch 13 (1212): ans_f1: 0.6027, qst_f1: 0.6043, em: 0.0000, avg: 0.3014, multirc_loss: 0.7261
03/25 05:57:24 PM: Evaluate: task multirc, batch 26 (1212): ans_f1: 0.6174, qst_f1: 0.5981, em: 0.0000, avg: 0.3087, multirc_loss: 0.7310
03/25 05:57:35 PM: Evaluate: task multirc, batch 39 (1212): ans_f1: 0.6463, qst_f1: 0.6374, em: 0.0000, avg: 0.3231, multirc_loss: 0.7185
03/25 05:57:45 PM: Evaluate: task multirc, batch 52 (1212): ans_f1: 0.6515, qst_f1: 0.6450, em: 0.0208, avg: 0.3361, multirc_loss: 0.7155
03/25 05:57:55 PM: Evaluate: task multirc, batch 65 (1212): ans_f1: 0.6580, qst_f1: 0.6590, em: 0.0161, avg: 0.3371, multirc_loss: 0.7140
03/25 05:58:06 PM: Evaluate: task multirc, batch 78 (1212): ans_f1: 0.6566, qst_f1: 0.6531, em: 0.0141, avg: 0.3353, multirc_loss: 0.7163
03/25 05:58:16 PM: Evaluate: task multirc, batch 92 (1212): ans_f1: 0.6567, qst_f1: 0.6422, em: 0.0122, avg: 0.3345, multirc_loss: 0.7123
03/25 05:58:27 PM: Evaluate: task multirc, batch 106 (1212): ans_f1: 0.6465, qst_f1: 0.6055, em: 0.0105, avg: 0.3285, multirc_loss: 0.7089
03/25 05:58:37 PM: Evaluate: task multirc, batch 120 (1212): ans_f1: 0.6418, qst_f1: 0.5932, em: 0.0095, avg: 0.3257, multirc_loss: 0.7086
03/25 05:58:48 PM: Evaluate: task multirc, batch 133 (1212): ans_f1: 0.6323, qst_f1: 0.5916, em: 0.0088, avg: 0.3205, multirc_loss: 0.7095
03/25 05:58:58 PM: Evaluate: task multirc, batch 146 (1212): ans_f1: 0.6238, qst_f1: 0.5902, em: 0.0081, avg: 0.3159, multirc_loss: 0.7148
03/25 05:59:09 PM: Evaluate: task multirc, batch 158 (1212): ans_f1: 0.6178, qst_f1: 0.5902, em: 0.0150, avg: 0.3164, multirc_loss: 0.7171
03/25 05:59:19 PM: Evaluate: task multirc, batch 172 (1212): ans_f1: 0.6178, qst_f1: 0.5897, em: 0.0071, avg: 0.3124, multirc_loss: 0.7169
03/25 05:59:30 PM: Evaluate: task multirc, batch 186 (1212): ans_f1: 0.6124, qst_f1: 0.5854, em: 0.0067, avg: 0.3096, multirc_loss: 0.7189
03/25 05:59:40 PM: Evaluate: task multirc, batch 199 (1212): ans_f1: 0.6036, qst_f1: 0.5778, em: 0.0064, avg: 0.3050, multirc_loss: 0.7223
03/25 05:59:51 PM: Evaluate: task multirc, batch 213 (1212): ans_f1: 0.6002, qst_f1: 0.5772, em: 0.0121, avg: 0.3061, multirc_loss: 0.7234
03/25 06:00:02 PM: Evaluate: task multirc, batch 226 (1212): ans_f1: 0.6050, qst_f1: 0.5732, em: 0.0058, avg: 0.3054, multirc_loss: 0.7212
03/25 06:00:12 PM: Evaluate: task multirc, batch 240 (1212): ans_f1: 0.6110, qst_f1: 0.5793, em: 0.0054, avg: 0.3082, multirc_loss: 0.7203
03/25 06:00:23 PM: Evaluate: task multirc, batch 253 (1212): ans_f1: 0.6052, qst_f1: 0.5739, em: 0.0051, avg: 0.3051, multirc_loss: 0.7224
03/25 06:00:34 PM: Evaluate: task multirc, batch 267 (1212): ans_f1: 0.5982, qst_f1: 0.5656, em: 0.0048, avg: 0.3015, multirc_loss: 0.7263
03/25 06:00:44 PM: Evaluate: task multirc, batch 281 (1212): ans_f1: 0.5920, qst_f1: 0.5633, em: 0.0046, avg: 0.2983, multirc_loss: 0.7296
03/25 06:00:54 PM: Evaluate: task multirc, batch 295 (1212): ans_f1: 0.5880, qst_f1: 0.5599, em: 0.0044, avg: 0.2962, multirc_loss: 0.7323
03/25 06:01:04 PM: Evaluate: task multirc, batch 308 (1212): ans_f1: 0.5883, qst_f1: 0.5615, em: 0.0085, avg: 0.2984, multirc_loss: 0.7319
03/25 06:01:15 PM: Evaluate: task multirc, batch 322 (1212): ans_f1: 0.5880, qst_f1: 0.5596, em: 0.0041, avg: 0.2961, multirc_loss: 0.7327
03/25 06:01:26 PM: Evaluate: task multirc, batch 336 (1212): ans_f1: 0.5870, qst_f1: 0.5598, em: 0.0079, avg: 0.2975, multirc_loss: 0.7333
03/25 06:01:37 PM: Evaluate: task multirc, batch 350 (1212): ans_f1: 0.5883, qst_f1: 0.5596, em: 0.0038, avg: 0.2960, multirc_loss: 0.7326
03/25 06:01:47 PM: Evaluate: task multirc, batch 364 (1212): ans_f1: 0.5923, qst_f1: 0.5619, em: 0.0036, avg: 0.2979, multirc_loss: 0.7315
03/25 06:01:58 PM: Evaluate: task multirc, batch 377 (1212): ans_f1: 0.5937, qst_f1: 0.5649, em: 0.0035, avg: 0.2986, multirc_loss: 0.7311
03/25 06:02:08 PM: Evaluate: task multirc, batch 390 (1212): ans_f1: 0.5944, qst_f1: 0.5644, em: 0.0033, avg: 0.2989, multirc_loss: 0.7311
03/25 06:02:19 PM: Evaluate: task multirc, batch 404 (1212): ans_f1: 0.5888, qst_f1: 0.5586, em: 0.0032, avg: 0.2960, multirc_loss: 0.7327
03/25 06:02:30 PM: Evaluate: task multirc, batch 418 (1212): ans_f1: 0.5837, qst_f1: 0.5536, em: 0.0031, avg: 0.2934, multirc_loss: 0.7364
03/25 06:02:40 PM: Evaluate: task multirc, batch 431 (1212): ans_f1: 0.5806, qst_f1: 0.5462, em: 0.0030, avg: 0.2918, multirc_loss: 0.7359
03/25 06:02:50 PM: Evaluate: task multirc, batch 444 (1212): ans_f1: 0.5777, qst_f1: 0.5414, em: 0.0029, avg: 0.2903, multirc_loss: 0.7361
03/25 06:03:01 PM: Evaluate: task multirc, batch 457 (1212): ans_f1: 0.5765, qst_f1: 0.5404, em: 0.0028, avg: 0.2897, multirc_loss: 0.7371
03/25 06:03:11 PM: Evaluate: task multirc, batch 470 (1212): ans_f1: 0.5787, qst_f1: 0.5453, em: 0.0027, avg: 0.2907, multirc_loss: 0.7368
03/25 06:03:22 PM: Evaluate: task multirc, batch 483 (1212): ans_f1: 0.5756, qst_f1: 0.5378, em: 0.0026, avg: 0.2891, multirc_loss: 0.7355
03/25 06:03:32 PM: Evaluate: task multirc, batch 497 (1212): ans_f1: 0.5764, qst_f1: 0.5340, em: 0.0051, avg: 0.2908, multirc_loss: 0.7336
03/25 06:03:43 PM: Evaluate: task multirc, batch 511 (1212): ans_f1: 0.5766, qst_f1: 0.5354, em: 0.0050, avg: 0.2908, multirc_loss: 0.7328
03/25 06:03:54 PM: Evaluate: task multirc, batch 525 (1212): ans_f1: 0.5808, qst_f1: 0.5420, em: 0.0048, avg: 0.2928, multirc_loss: 0.7318
03/25 06:04:05 PM: Evaluate: task multirc, batch 538 (1212): ans_f1: 0.5807, qst_f1: 0.5430, em: 0.0070, avg: 0.2938, multirc_loss: 0.7324
03/25 06:04:15 PM: Evaluate: task multirc, batch 551 (1212): ans_f1: 0.5810, qst_f1: 0.5434, em: 0.0046, avg: 0.2928, multirc_loss: 0.7329
03/25 06:04:26 PM: Evaluate: task multirc, batch 564 (1212): ans_f1: 0.5804, qst_f1: 0.5421, em: 0.0045, avg: 0.2924, multirc_loss: 0.7335
03/25 06:04:37 PM: Evaluate: task multirc, batch 578 (1212): ans_f1: 0.5810, qst_f1: 0.5385, em: 0.0044, avg: 0.2927, multirc_loss: 0.7326
03/25 06:04:48 PM: Evaluate: task multirc, batch 592 (1212): ans_f1: 0.5791, qst_f1: 0.5375, em: 0.0043, avg: 0.2917, multirc_loss: 0.7329
03/25 06:04:59 PM: Evaluate: task multirc, batch 605 (1212): ans_f1: 0.5781, qst_f1: 0.5380, em: 0.0042, avg: 0.2912, multirc_loss: 0.7328
03/25 06:05:09 PM: Evaluate: task multirc, batch 619 (1212): ans_f1: 0.5774, qst_f1: 0.5378, em: 0.0041, avg: 0.2908, multirc_loss: 0.7323
03/25 06:05:20 PM: Evaluate: task multirc, batch 633 (1212): ans_f1: 0.5770, qst_f1: 0.5374, em: 0.0040, avg: 0.2905, multirc_loss: 0.7317
03/25 06:05:31 PM: Evaluate: task multirc, batch 647 (1212): ans_f1: 0.5784, qst_f1: 0.5388, em: 0.0040, avg: 0.2912, multirc_loss: 0.7309
03/25 06:05:41 PM: Evaluate: task multirc, batch 661 (1212): ans_f1: 0.5797, qst_f1: 0.5410, em: 0.0038, avg: 0.2918, multirc_loss: 0.7306
03/25 06:05:52 PM: Evaluate: task multirc, batch 675 (1212): ans_f1: 0.5797, qst_f1: 0.5418, em: 0.0038, avg: 0.2917, multirc_loss: 0.7311
03/25 06:06:03 PM: Evaluate: task multirc, batch 687 (1212): ans_f1: 0.5779, qst_f1: 0.5406, em: 0.0037, avg: 0.2908, multirc_loss: 0.7321
03/25 06:06:14 PM: Evaluate: task multirc, batch 700 (1212): ans_f1: 0.5794, qst_f1: 0.5432, em: 0.0055, avg: 0.2924, multirc_loss: 0.7319
03/25 06:06:24 PM: Evaluate: task multirc, batch 712 (1212): ans_f1: 0.5840, qst_f1: 0.5473, em: 0.0036, avg: 0.2938, multirc_loss: 0.7305
03/25 06:06:34 PM: Evaluate: task multirc, batch 725 (1212): ans_f1: 0.5864, qst_f1: 0.5488, em: 0.0035, avg: 0.2950, multirc_loss: 0.7295
03/25 06:06:45 PM: Evaluate: task multirc, batch 738 (1212): ans_f1: 0.5850, qst_f1: 0.5473, em: 0.0035, avg: 0.2942, multirc_loss: 0.7294
03/25 06:06:56 PM: Evaluate: task multirc, batch 752 (1212): ans_f1: 0.5836, qst_f1: 0.5460, em: 0.0034, avg: 0.2935, multirc_loss: 0.7294
03/25 06:07:07 PM: Evaluate: task multirc, batch 765 (1212): ans_f1: 0.5838, qst_f1: 0.5466, em: 0.0034, avg: 0.2936, multirc_loss: 0.7295
03/25 06:07:17 PM: Evaluate: task multirc, batch 778 (1212): ans_f1: 0.5863, qst_f1: 0.5491, em: 0.0049, avg: 0.2956, multirc_loss: 0.7288
03/25 06:07:28 PM: Evaluate: task multirc, batch 791 (1212): ans_f1: 0.5863, qst_f1: 0.5498, em: 0.0048, avg: 0.2956, multirc_loss: 0.7282
03/25 06:07:38 PM: Evaluate: task multirc, batch 804 (1212): ans_f1: 0.5873, qst_f1: 0.5519, em: 0.0047, avg: 0.2960, multirc_loss: 0.7276
03/25 06:07:49 PM: Evaluate: task multirc, batch 818 (1212): ans_f1: 0.5925, qst_f1: 0.5586, em: 0.0123, avg: 0.3024, multirc_loss: 0.7264
03/25 06:08:00 PM: Evaluate: task multirc, batch 832 (1212): ans_f1: 0.5931, qst_f1: 0.5601, em: 0.0151, avg: 0.3041, multirc_loss: 0.7261
03/25 06:08:11 PM: Evaluate: task multirc, batch 846 (1212): ans_f1: 0.5920, qst_f1: 0.5595, em: 0.0149, avg: 0.3034, multirc_loss: 0.7264
03/25 06:08:21 PM: Evaluate: task multirc, batch 859 (1212): ans_f1: 0.5924, qst_f1: 0.5605, em: 0.0161, avg: 0.3042, multirc_loss: 0.7263
03/25 06:08:32 PM: Evaluate: task multirc, batch 873 (1212): ans_f1: 0.5931, qst_f1: 0.5612, em: 0.0142, avg: 0.3037, multirc_loss: 0.7262
03/25 06:08:43 PM: Evaluate: task multirc, batch 886 (1212): ans_f1: 0.5956, qst_f1: 0.5648, em: 0.0154, avg: 0.3055, multirc_loss: 0.7256
03/25 06:08:53 PM: Evaluate: task multirc, batch 899 (1212): ans_f1: 0.5978, qst_f1: 0.5671, em: 0.0152, avg: 0.3065, multirc_loss: 0.7250
03/25 06:09:04 PM: Evaluate: task multirc, batch 913 (1212): ans_f1: 0.5962, qst_f1: 0.5657, em: 0.0136, avg: 0.3049, multirc_loss: 0.7258
03/25 06:09:15 PM: Evaluate: task multirc, batch 927 (1212): ans_f1: 0.5961, qst_f1: 0.5663, em: 0.0133, avg: 0.3047, multirc_loss: 0.7261
03/25 06:09:26 PM: Evaluate: task multirc, batch 941 (1212): ans_f1: 0.5941, qst_f1: 0.5638, em: 0.0131, avg: 0.3036, multirc_loss: 0.7262
03/25 06:09:37 PM: Evaluate: task multirc, batch 954 (1212): ans_f1: 0.5941, qst_f1: 0.5634, em: 0.0130, avg: 0.3035, multirc_loss: 0.7259
03/25 06:09:47 PM: Evaluate: task multirc, batch 967 (1212): ans_f1: 0.5938, qst_f1: 0.5638, em: 0.0128, avg: 0.3033, multirc_loss: 0.7256
03/25 06:09:58 PM: Evaluate: task multirc, batch 980 (1212): ans_f1: 0.5925, qst_f1: 0.5632, em: 0.0127, avg: 0.3026, multirc_loss: 0.7268
03/25 06:10:09 PM: Evaluate: task multirc, batch 993 (1212): ans_f1: 0.5930, qst_f1: 0.5636, em: 0.0126, avg: 0.3028, multirc_loss: 0.7267
03/25 06:10:19 PM: Evaluate: task multirc, batch 1006 (1212): ans_f1: 0.5928, qst_f1: 0.5633, em: 0.0125, avg: 0.3026, multirc_loss: 0.7268
03/25 06:10:30 PM: Evaluate: task multirc, batch 1019 (1212): ans_f1: 0.5913, qst_f1: 0.5618, em: 0.0124, avg: 0.3018, multirc_loss: 0.7271
03/25 06:10:40 PM: Evaluate: task multirc, batch 1032 (1212): ans_f1: 0.5911, qst_f1: 0.5622, em: 0.0122, avg: 0.3017, multirc_loss: 0.7271
03/25 06:10:51 PM: Evaluate: task multirc, batch 1045 (1212): ans_f1: 0.5914, qst_f1: 0.5626, em: 0.0121, avg: 0.3017, multirc_loss: 0.7268
03/25 06:11:01 PM: Evaluate: task multirc, batch 1058 (1212): ans_f1: 0.5915, qst_f1: 0.5626, em: 0.0119, avg: 0.3017, multirc_loss: 0.7266
03/25 06:11:12 PM: Evaluate: task multirc, batch 1071 (1212): ans_f1: 0.5918, qst_f1: 0.5625, em: 0.0117, avg: 0.3017, multirc_loss: 0.7264
03/25 06:11:23 PM: Evaluate: task multirc, batch 1084 (1212): ans_f1: 0.5921, qst_f1: 0.5629, em: 0.0116, avg: 0.3018, multirc_loss: 0.7265
03/25 06:11:34 PM: Evaluate: task multirc, batch 1096 (1212): ans_f1: 0.5922, qst_f1: 0.5630, em: 0.0115, avg: 0.3019, multirc_loss: 0.7267
03/25 06:11:45 PM: Evaluate: task multirc, batch 1110 (1212): ans_f1: 0.5910, qst_f1: 0.5624, em: 0.0114, avg: 0.3012, multirc_loss: 0.7271
03/25 06:11:55 PM: Evaluate: task multirc, batch 1123 (1212): ans_f1: 0.5901, qst_f1: 0.5619, em: 0.0113, avg: 0.3007, multirc_loss: 0.7271
03/25 06:12:06 PM: Evaluate: task multirc, batch 1137 (1212): ans_f1: 0.5882, qst_f1: 0.5602, em: 0.0112, avg: 0.2997, multirc_loss: 0.7277
03/25 06:12:17 PM: Evaluate: task multirc, batch 1151 (1212): ans_f1: 0.5875, qst_f1: 0.5600, em: 0.0110, avg: 0.2993, multirc_loss: 0.7277
03/25 06:12:28 PM: Evaluate: task multirc, batch 1165 (1212): ans_f1: 0.5859, qst_f1: 0.5579, em: 0.0109, avg: 0.2984, multirc_loss: 0.7277
03/25 06:12:40 PM: Evaluate: task multirc, batch 1179 (1212): ans_f1: 0.5844, qst_f1: 0.5541, em: 0.0108, avg: 0.2976, multirc_loss: 0.7273
03/25 06:12:51 PM: Evaluate: task multirc, batch 1193 (1212): ans_f1: 0.5842, qst_f1: 0.5532, em: 0.0107, avg: 0.2974, multirc_loss: 0.7270
03/25 06:13:01 PM: Evaluate: task multirc, batch 1206 (1212): ans_f1: 0.5861, qst_f1: 0.5554, em: 0.0106, avg: 0.2983, multirc_loss: 0.7264
03/25 06:13:06 PM: Updating LR scheduler:
03/25 06:13:06 PM: 	Best result seen so far for macro_avg: 0.300
03/25 06:13:06 PM: 	# validation passes without improvement: 1
03/25 06:13:06 PM: multirc_loss: training: 0.754191 validation: 0.726189
03/25 06:13:06 PM: macro_avg: validation: 0.298541
03/25 06:13:06 PM: micro_avg: validation: 0.298541
03/25 06:13:06 PM: multirc_ans_f1: training: 0.400000 validation: 0.586590
03/25 06:13:06 PM: multirc_qst_f1: training: 0.250000 validation: 0.555198
03/25 06:13:06 PM: multirc_em: training: 0.250000 validation: 0.010493
03/25 06:13:06 PM: multirc_avg: training: 0.325000 validation: 0.298541
03/25 06:13:06 PM: Global learning rate: 0.0003
03/25 06:13:06 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 06:13:12 PM: Update 9: task multirc, steps since last val 1 (total steps = 9): ans_f1: 0.8000, qst_f1: 0.5000, em: 0.7500, avg: 0.7750, multirc_loss: 0.6899
03/25 06:13:12 PM: ***** Step 9 / Validation 9 *****
03/25 06:13:12 PM: multirc: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 06:13:12 PM: Validating...
03/25 06:13:22 PM: Evaluate: task multirc, batch 13 (1212): ans_f1: 0.5672, qst_f1: 0.5619, em: 0.0000, avg: 0.2836, multirc_loss: 0.7248
03/25 06:13:33 PM: Evaluate: task multirc, batch 26 (1212): ans_f1: 0.5816, qst_f1: 0.5489, em: 0.0000, avg: 0.2908, multirc_loss: 0.7311
03/25 06:13:44 PM: Evaluate: task multirc, batch 39 (1212): ans_f1: 0.6182, qst_f1: 0.6021, em: 0.0000, avg: 0.3091, multirc_loss: 0.7191
03/25 06:13:54 PM: Evaluate: task multirc, batch 52 (1212): ans_f1: 0.6309, qst_f1: 0.6185, em: 0.0208, avg: 0.3259, multirc_loss: 0.7159
03/25 06:14:04 PM: Evaluate: task multirc, batch 65 (1212): ans_f1: 0.6419, qst_f1: 0.6385, em: 0.0161, avg: 0.3290, multirc_loss: 0.7143
03/25 06:14:15 PM: Evaluate: task multirc, batch 78 (1212): ans_f1: 0.6432, qst_f1: 0.6352, em: 0.0141, avg: 0.3286, multirc_loss: 0.7166
03/25 06:14:25 PM: Evaluate: task multirc, batch 92 (1212): ans_f1: 0.6450, qst_f1: 0.6185, em: 0.0122, avg: 0.3286, multirc_loss: 0.7124
03/25 06:14:36 PM: Evaluate: task multirc, batch 106 (1212): ans_f1: 0.6330, qst_f1: 0.5775, em: 0.0211, avg: 0.3270, multirc_loss: 0.7093
03/25 06:14:46 PM: Evaluate: task multirc, batch 120 (1212): ans_f1: 0.6277, qst_f1: 0.5670, em: 0.0190, avg: 0.3234, multirc_loss: 0.7091
03/25 06:14:57 PM: Evaluate: task multirc, batch 133 (1212): ans_f1: 0.6182, qst_f1: 0.5639, em: 0.0175, avg: 0.3179, multirc_loss: 0.7094
03/25 06:15:07 PM: Evaluate: task multirc, batch 146 (1212): ans_f1: 0.6107, qst_f1: 0.5648, em: 0.0161, avg: 0.3134, multirc_loss: 0.7143
03/25 06:15:17 PM: Evaluate: task multirc, batch 158 (1212): ans_f1: 0.6056, qst_f1: 0.5664, em: 0.0226, avg: 0.3141, multirc_loss: 0.7161
03/25 06:15:28 PM: Evaluate: task multirc, batch 172 (1212): ans_f1: 0.6066, qst_f1: 0.5673, em: 0.0142, avg: 0.3104, multirc_loss: 0.7157
03/25 06:15:39 PM: Evaluate: task multirc, batch 186 (1212): ans_f1: 0.6020, qst_f1: 0.5642, em: 0.0134, avg: 0.3077, multirc_loss: 0.7175
03/25 06:15:49 PM: Evaluate: task multirc, batch 199 (1212): ans_f1: 0.5937, qst_f1: 0.5577, em: 0.0127, avg: 0.3032, multirc_loss: 0.7209
03/25 06:16:00 PM: Evaluate: task multirc, batch 213 (1212): ans_f1: 0.5908, qst_f1: 0.5580, em: 0.0182, avg: 0.3045, multirc_loss: 0.7219
03/25 06:16:10 PM: Evaluate: task multirc, batch 226 (1212): ans_f1: 0.5972, qst_f1: 0.5556, em: 0.0116, avg: 0.3044, multirc_loss: 0.7198
03/25 06:16:21 PM: Evaluate: task multirc, batch 240 (1212): ans_f1: 0.6038, qst_f1: 0.5628, em: 0.0108, avg: 0.3073, multirc_loss: 0.7190
03/25 06:16:31 PM: Evaluate: task multirc, batch 253 (1212): ans_f1: 0.5983, qst_f1: 0.5584, em: 0.0102, avg: 0.3042, multirc_loss: 0.7211
03/25 06:16:42 PM: Evaluate: task multirc, batch 267 (1212): ans_f1: 0.5916, qst_f1: 0.5510, em: 0.0096, avg: 0.3006, multirc_loss: 0.7249
03/25 06:16:52 PM: Evaluate: task multirc, batch 281 (1212): ans_f1: 0.5855, qst_f1: 0.5492, em: 0.0092, avg: 0.2974, multirc_loss: 0.7282
03/25 06:17:03 PM: Evaluate: task multirc, batch 295 (1212): ans_f1: 0.5818, qst_f1: 0.5465, em: 0.0088, avg: 0.2953, multirc_loss: 0.7310
03/25 06:17:13 PM: Evaluate: task multirc, batch 308 (1212): ans_f1: 0.5824, qst_f1: 0.5486, em: 0.0127, avg: 0.2975, multirc_loss: 0.7306
03/25 06:17:24 PM: Evaluate: task multirc, batch 322 (1212): ans_f1: 0.5824, qst_f1: 0.5470, em: 0.0082, avg: 0.2953, multirc_loss: 0.7314
03/25 06:17:34 PM: Evaluate: task multirc, batch 336 (1212): ans_f1: 0.5816, qst_f1: 0.5478, em: 0.0118, avg: 0.2967, multirc_loss: 0.7320
03/25 06:17:45 PM: Evaluate: task multirc, batch 350 (1212): ans_f1: 0.5831, qst_f1: 0.5480, em: 0.0076, avg: 0.2953, multirc_loss: 0.7314
03/25 06:17:56 PM: Evaluate: task multirc, batch 364 (1212): ans_f1: 0.5873, qst_f1: 0.5509, em: 0.0072, avg: 0.2973, multirc_loss: 0.7304
03/25 06:18:06 PM: Evaluate: task multirc, batch 377 (1212): ans_f1: 0.5890, qst_f1: 0.5544, em: 0.0069, avg: 0.2979, multirc_loss: 0.7299
03/25 06:18:17 PM: Evaluate: task multirc, batch 390 (1212): ans_f1: 0.5898, qst_f1: 0.5542, em: 0.0067, avg: 0.2983, multirc_loss: 0.7299
03/25 06:18:28 PM: Evaluate: task multirc, batch 404 (1212): ans_f1: 0.5844, qst_f1: 0.5488, em: 0.0064, avg: 0.2954, multirc_loss: 0.7316
03/25 06:18:38 PM: Evaluate: task multirc, batch 418 (1212): ans_f1: 0.5793, qst_f1: 0.5442, em: 0.0062, avg: 0.2927, multirc_loss: 0.7354
03/25 06:18:48 PM: Evaluate: task multirc, batch 431 (1212): ans_f1: 0.5762, qst_f1: 0.5370, em: 0.0060, avg: 0.2911, multirc_loss: 0.7349
03/25 06:18:59 PM: Evaluate: task multirc, batch 444 (1212): ans_f1: 0.5735, qst_f1: 0.5325, em: 0.0058, avg: 0.2896, multirc_loss: 0.7351
03/25 06:19:09 PM: Evaluate: task multirc, batch 457 (1212): ans_f1: 0.5724, qst_f1: 0.5317, em: 0.0056, avg: 0.2890, multirc_loss: 0.7362
03/25 06:19:19 PM: Evaluate: task multirc, batch 470 (1212): ans_f1: 0.5747, qst_f1: 0.5370, em: 0.0054, avg: 0.2901, multirc_loss: 0.7359
03/25 06:19:30 PM: Evaluate: task multirc, batch 483 (1212): ans_f1: 0.5715, qst_f1: 0.5292, em: 0.0053, avg: 0.2884, multirc_loss: 0.7344
03/25 06:19:41 PM: Evaluate: task multirc, batch 497 (1212): ans_f1: 0.5708, qst_f1: 0.5228, em: 0.0051, avg: 0.2880, multirc_loss: 0.7324
03/25 06:19:51 PM: Evaluate: task multirc, batch 511 (1212): ans_f1: 0.5685, qst_f1: 0.5203, em: 0.0050, avg: 0.2868, multirc_loss: 0.7317
03/25 06:20:03 PM: Evaluate: task multirc, batch 525 (1212): ans_f1: 0.5731, qst_f1: 0.5274, em: 0.0048, avg: 0.2889, multirc_loss: 0.7307
03/25 06:20:13 PM: Evaluate: task multirc, batch 538 (1212): ans_f1: 0.5721, qst_f1: 0.5275, em: 0.0070, avg: 0.2896, multirc_loss: 0.7313
03/25 06:20:24 PM: Evaluate: task multirc, batch 551 (1212): ans_f1: 0.5727, qst_f1: 0.5282, em: 0.0046, avg: 0.2886, multirc_loss: 0.7318
03/25 06:20:34 PM: Evaluate: task multirc, batch 564 (1212): ans_f1: 0.5723, qst_f1: 0.5272, em: 0.0045, avg: 0.2884, multirc_loss: 0.7325
03/25 06:20:45 PM: Evaluate: task multirc, batch 577 (1212): ans_f1: 0.5737, qst_f1: 0.5256, em: 0.0044, avg: 0.2890, multirc_loss: 0.7315
03/25 06:20:56 PM: Evaluate: task multirc, batch 591 (1212): ans_f1: 0.5726, qst_f1: 0.5250, em: 0.0043, avg: 0.2885, multirc_loss: 0.7315
03/25 06:21:06 PM: Evaluate: task multirc, batch 604 (1212): ans_f1: 0.5709, qst_f1: 0.5245, em: 0.0042, avg: 0.2876, multirc_loss: 0.7316
03/25 06:21:17 PM: Evaluate: task multirc, batch 618 (1212): ans_f1: 0.5691, qst_f1: 0.5230, em: 0.0062, avg: 0.2876, multirc_loss: 0.7310
03/25 06:21:28 PM: Evaluate: task multirc, batch 632 (1212): ans_f1: 0.5688, qst_f1: 0.5226, em: 0.0061, avg: 0.2875, multirc_loss: 0.7304
03/25 06:21:38 PM: Evaluate: task multirc, batch 646 (1212): ans_f1: 0.5696, qst_f1: 0.5232, em: 0.0040, avg: 0.2868, multirc_loss: 0.7299
03/25 06:21:49 PM: Evaluate: task multirc, batch 660 (1212): ans_f1: 0.5715, qst_f1: 0.5259, em: 0.0039, avg: 0.2877, multirc_loss: 0.7294
03/25 06:22:00 PM: Evaluate: task multirc, batch 674 (1212): ans_f1: 0.5717, qst_f1: 0.5272, em: 0.0038, avg: 0.2878, multirc_loss: 0.7299
03/25 06:22:10 PM: Evaluate: task multirc, batch 686 (1212): ans_f1: 0.5705, qst_f1: 0.5267, em: 0.0037, avg: 0.2871, multirc_loss: 0.7308
03/25 06:22:21 PM: Evaluate: task multirc, batch 699 (1212): ans_f1: 0.5710, qst_f1: 0.5283, em: 0.0037, avg: 0.2873, multirc_loss: 0.7311
03/25 06:22:32 PM: Evaluate: task multirc, batch 712 (1212): ans_f1: 0.5767, qst_f1: 0.5338, em: 0.0036, avg: 0.2901, multirc_loss: 0.7293
03/25 06:22:42 PM: Evaluate: task multirc, batch 725 (1212): ans_f1: 0.5787, qst_f1: 0.5345, em: 0.0035, avg: 0.2911, multirc_loss: 0.7284
03/25 06:22:53 PM: Evaluate: task multirc, batch 738 (1212): ans_f1: 0.5776, qst_f1: 0.5334, em: 0.0035, avg: 0.2905, multirc_loss: 0.7282
03/25 06:23:04 PM: Evaluate: task multirc, batch 752 (1212): ans_f1: 0.5762, qst_f1: 0.5323, em: 0.0034, avg: 0.2898, multirc_loss: 0.7283
03/25 06:23:15 PM: Evaluate: task multirc, batch 765 (1212): ans_f1: 0.5766, qst_f1: 0.5331, em: 0.0034, avg: 0.2900, multirc_loss: 0.7283
03/25 06:23:25 PM: Evaluate: task multirc, batch 778 (1212): ans_f1: 0.5793, qst_f1: 0.5359, em: 0.0049, avg: 0.2921, multirc_loss: 0.7277
03/25 06:23:36 PM: Evaluate: task multirc, batch 791 (1212): ans_f1: 0.5798, qst_f1: 0.5377, em: 0.0048, avg: 0.2923, multirc_loss: 0.7272
03/25 06:23:47 PM: Evaluate: task multirc, batch 805 (1212): ans_f1: 0.5815, qst_f1: 0.5407, em: 0.0063, avg: 0.2939, multirc_loss: 0.7263
03/25 06:23:58 PM: Evaluate: task multirc, batch 819 (1212): ans_f1: 0.5829, qst_f1: 0.5424, em: 0.0123, avg: 0.2976, multirc_loss: 0.7254
03/25 06:24:08 PM: Evaluate: task multirc, batch 833 (1212): ans_f1: 0.5827, qst_f1: 0.5413, em: 0.0136, avg: 0.2982, multirc_loss: 0.7252
03/25 06:24:19 PM: Evaluate: task multirc, batch 847 (1212): ans_f1: 0.5821, qst_f1: 0.5412, em: 0.0134, avg: 0.2977, multirc_loss: 0.7254
03/25 06:24:30 PM: Evaluate: task multirc, batch 861 (1212): ans_f1: 0.5818, qst_f1: 0.5415, em: 0.0145, avg: 0.2982, multirc_loss: 0.7253
03/25 06:24:42 PM: Evaluate: task multirc, batch 875 (1212): ans_f1: 0.5831, qst_f1: 0.5431, em: 0.0128, avg: 0.2980, multirc_loss: 0.7252
03/25 06:24:52 PM: Evaluate: task multirc, batch 888 (1212): ans_f1: 0.5855, qst_f1: 0.5458, em: 0.0126, avg: 0.2991, multirc_loss: 0.7246
03/25 06:25:02 PM: Evaluate: task multirc, batch 901 (1212): ans_f1: 0.5874, qst_f1: 0.5477, em: 0.0124, avg: 0.2999, multirc_loss: 0.7243
03/25 06:25:13 PM: Evaluate: task multirc, batch 914 (1212): ans_f1: 0.5861, qst_f1: 0.5474, em: 0.0122, avg: 0.2992, multirc_loss: 0.7251
03/25 06:25:24 PM: Evaluate: task multirc, batch 928 (1212): ans_f1: 0.5858, qst_f1: 0.5476, em: 0.0120, avg: 0.2989, multirc_loss: 0.7254
03/25 06:25:34 PM: Evaluate: task multirc, batch 942 (1212): ans_f1: 0.5842, qst_f1: 0.5461, em: 0.0118, avg: 0.2980, multirc_loss: 0.7252
03/25 06:25:45 PM: Evaluate: task multirc, batch 955 (1212): ans_f1: 0.5847, qst_f1: 0.5467, em: 0.0117, avg: 0.2982, multirc_loss: 0.7249
03/25 06:25:56 PM: Evaluate: task multirc, batch 968 (1212): ans_f1: 0.5838, qst_f1: 0.5456, em: 0.0115, avg: 0.2977, multirc_loss: 0.7247
03/25 06:26:07 PM: Evaluate: task multirc, batch 982 (1212): ans_f1: 0.5830, qst_f1: 0.5459, em: 0.0127, avg: 0.2978, multirc_loss: 0.7258
03/25 06:26:18 PM: Evaluate: task multirc, batch 995 (1212): ans_f1: 0.5834, qst_f1: 0.5457, em: 0.0113, avg: 0.2974, multirc_loss: 0.7258
03/25 06:26:28 PM: Evaluate: task multirc, batch 1008 (1212): ans_f1: 0.5828, qst_f1: 0.5452, em: 0.0112, avg: 0.2970, multirc_loss: 0.7260
03/25 06:26:39 PM: Evaluate: task multirc, batch 1021 (1212): ans_f1: 0.5820, qst_f1: 0.5448, em: 0.0111, avg: 0.2966, multirc_loss: 0.7263
03/25 06:26:49 PM: Evaluate: task multirc, batch 1034 (1212): ans_f1: 0.5814, qst_f1: 0.5448, em: 0.0110, avg: 0.2962, multirc_loss: 0.7263
03/25 06:27:00 PM: Evaluate: task multirc, batch 1047 (1212): ans_f1: 0.5817, qst_f1: 0.5441, em: 0.0108, avg: 0.2962, multirc_loss: 0.7259
03/25 06:27:11 PM: Evaluate: task multirc, batch 1060 (1212): ans_f1: 0.5823, qst_f1: 0.5459, em: 0.0107, avg: 0.2965, multirc_loss: 0.7256
03/25 06:27:21 PM: Evaluate: task multirc, batch 1073 (1212): ans_f1: 0.5825, qst_f1: 0.5451, em: 0.0105, avg: 0.2965, multirc_loss: 0.7255
03/25 06:27:32 PM: Evaluate: task multirc, batch 1086 (1212): ans_f1: 0.5834, qst_f1: 0.5470, em: 0.0115, avg: 0.2975, multirc_loss: 0.7255
03/25 06:27:43 PM: Evaluate: task multirc, batch 1098 (1212): ans_f1: 0.5830, qst_f1: 0.5458, em: 0.0103, avg: 0.2966, multirc_loss: 0.7260
03/25 06:27:54 PM: Evaluate: task multirc, batch 1112 (1212): ans_f1: 0.5820, qst_f1: 0.5459, em: 0.0102, avg: 0.2961, multirc_loss: 0.7262
03/25 06:28:04 PM: Evaluate: task multirc, batch 1125 (1212): ans_f1: 0.5813, qst_f1: 0.5452, em: 0.0101, avg: 0.2957, multirc_loss: 0.7261
03/25 06:28:15 PM: Evaluate: task multirc, batch 1139 (1212): ans_f1: 0.5797, qst_f1: 0.5450, em: 0.0100, avg: 0.2949, multirc_loss: 0.7265
03/25 06:28:26 PM: Evaluate: task multirc, batch 1153 (1212): ans_f1: 0.5786, qst_f1: 0.5435, em: 0.0099, avg: 0.2943, multirc_loss: 0.7267
03/25 06:28:37 PM: Evaluate: task multirc, batch 1166 (1212): ans_f1: 0.5764, qst_f1: 0.5402, em: 0.0098, avg: 0.2931, multirc_loss: 0.7266
03/25 06:28:48 PM: Evaluate: task multirc, batch 1180 (1212): ans_f1: 0.5752, qst_f1: 0.5360, em: 0.0097, avg: 0.2924, multirc_loss: 0.7263
03/25 06:28:59 PM: Evaluate: task multirc, batch 1194 (1212): ans_f1: 0.5755, qst_f1: 0.5366, em: 0.0096, avg: 0.2926, multirc_loss: 0.7259
03/25 06:29:10 PM: Evaluate: task multirc, batch 1207 (1212): ans_f1: 0.5771, qst_f1: 0.5376, em: 0.0095, avg: 0.2933, multirc_loss: 0.7255
03/25 06:29:14 PM: Updating LR scheduler:
03/25 06:29:14 PM: 	Best result seen so far for macro_avg: 0.300
03/25 06:29:14 PM: 	# validation passes without improvement: 2
03/25 06:29:14 PM: multirc_loss: training: 0.689854 validation: 0.725205
03/25 06:29:14 PM: macro_avg: validation: 0.293614
03/25 06:29:14 PM: micro_avg: validation: 0.293614
03/25 06:29:14 PM: multirc_ans_f1: training: 0.800000 validation: 0.577785
03/25 06:29:14 PM: multirc_qst_f1: training: 0.500000 validation: 0.538139
03/25 06:29:14 PM: multirc_em: training: 0.750000 validation: 0.009444
03/25 06:29:14 PM: multirc_avg: training: 0.775000 validation: 0.293614
03/25 06:29:14 PM: Global learning rate: 0.0003
03/25 06:29:14 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 06:29:20 PM: Update 10: task multirc, steps since last val 1 (total steps = 10): ans_f1: 0.8571, qst_f1: 0.7500, em: 0.7500, avg: 0.8036, multirc_loss: 0.7342
03/25 06:29:20 PM: ***** Step 10 / Validation 10 *****
03/25 06:29:20 PM: multirc: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 06:29:20 PM: Validating...
03/25 06:29:30 PM: Evaluate: task multirc, batch 13 (1212): ans_f1: 0.5672, qst_f1: 0.5619, em: 0.0000, avg: 0.2836, multirc_loss: 0.7265
03/25 06:29:41 PM: Evaluate: task multirc, batch 26 (1212): ans_f1: 0.5816, qst_f1: 0.5489, em: 0.0000, avg: 0.2908, multirc_loss: 0.7339
03/25 06:29:51 PM: Evaluate: task multirc, batch 39 (1212): ans_f1: 0.6182, qst_f1: 0.6021, em: 0.0000, avg: 0.3091, multirc_loss: 0.7211
03/25 06:30:02 PM: Evaluate: task multirc, batch 52 (1212): ans_f1: 0.6309, qst_f1: 0.6185, em: 0.0208, avg: 0.3259, multirc_loss: 0.7174
03/25 06:30:12 PM: Evaluate: task multirc, batch 65 (1212): ans_f1: 0.6419, qst_f1: 0.6385, em: 0.0161, avg: 0.3290, multirc_loss: 0.7158
03/25 06:30:22 PM: Evaluate: task multirc, batch 78 (1212): ans_f1: 0.6432, qst_f1: 0.6352, em: 0.0141, avg: 0.3286, multirc_loss: 0.7183
03/25 06:30:33 PM: Evaluate: task multirc, batch 92 (1212): ans_f1: 0.6450, qst_f1: 0.6185, em: 0.0122, avg: 0.3286, multirc_loss: 0.7138
03/25 06:30:43 PM: Evaluate: task multirc, batch 106 (1212): ans_f1: 0.6379, qst_f1: 0.5868, em: 0.0105, avg: 0.3242, multirc_loss: 0.7103
03/25 06:30:54 PM: Evaluate: task multirc, batch 120 (1212): ans_f1: 0.6341, qst_f1: 0.5764, em: 0.0095, avg: 0.3218, multirc_loss: 0.7102
03/25 06:31:04 PM: Evaluate: task multirc, batch 133 (1212): ans_f1: 0.6250, qst_f1: 0.5726, em: 0.0088, avg: 0.3169, multirc_loss: 0.7104
03/25 06:31:15 PM: Evaluate: task multirc, batch 146 (1212): ans_f1: 0.6169, qst_f1: 0.5727, em: 0.0081, avg: 0.3125, multirc_loss: 0.7153
03/25 06:31:25 PM: Evaluate: task multirc, batch 158 (1212): ans_f1: 0.6114, qst_f1: 0.5738, em: 0.0150, avg: 0.3132, multirc_loss: 0.7173
03/25 06:31:36 PM: Evaluate: task multirc, batch 172 (1212): ans_f1: 0.6119, qst_f1: 0.5743, em: 0.0071, avg: 0.3095, multirc_loss: 0.7167
03/25 06:31:47 PM: Evaluate: task multirc, batch 186 (1212): ans_f1: 0.6069, qst_f1: 0.5708, em: 0.0067, avg: 0.3068, multirc_loss: 0.7185
03/25 06:31:57 PM: Evaluate: task multirc, batch 199 (1212): ans_f1: 0.5983, qst_f1: 0.5639, em: 0.0064, avg: 0.3024, multirc_loss: 0.7221
03/25 06:32:08 PM: Evaluate: task multirc, batch 213 (1212): ans_f1: 0.5952, qst_f1: 0.5640, em: 0.0121, avg: 0.3036, multirc_loss: 0.7232
03/25 06:32:18 PM: Evaluate: task multirc, batch 226 (1212): ans_f1: 0.6013, qst_f1: 0.5613, em: 0.0058, avg: 0.3035, multirc_loss: 0.7210
03/25 06:32:29 PM: Evaluate: task multirc, batch 240 (1212): ans_f1: 0.6076, qst_f1: 0.5681, em: 0.0054, avg: 0.3065, multirc_loss: 0.7202
03/25 06:32:39 PM: Evaluate: task multirc, batch 253 (1212): ans_f1: 0.6019, qst_f1: 0.5634, em: 0.0051, avg: 0.3035, multirc_loss: 0.7226
03/25 06:32:50 PM: Evaluate: task multirc, batch 267 (1212): ans_f1: 0.5950, qst_f1: 0.5557, em: 0.0048, avg: 0.2999, multirc_loss: 0.7266
03/25 06:33:00 PM: Evaluate: task multirc, batch 281 (1212): ans_f1: 0.5889, qst_f1: 0.5537, em: 0.0046, avg: 0.2967, multirc_loss: 0.7301
03/25 06:33:11 PM: Evaluate: task multirc, batch 295 (1212): ans_f1: 0.5850, qst_f1: 0.5509, em: 0.0044, avg: 0.2947, multirc_loss: 0.7331
03/25 06:33:21 PM: Evaluate: task multirc, batch 308 (1212): ans_f1: 0.5854, qst_f1: 0.5528, em: 0.0085, avg: 0.2969, multirc_loss: 0.7327
03/25 06:33:31 PM: Evaluate: task multirc, batch 322 (1212): ans_f1: 0.5853, qst_f1: 0.5511, em: 0.0041, avg: 0.2947, multirc_loss: 0.7335
03/25 06:33:42 PM: Evaluate: task multirc, batch 336 (1212): ans_f1: 0.5844, qst_f1: 0.5517, em: 0.0079, avg: 0.2961, multirc_loss: 0.7341
03/25 06:33:53 PM: Evaluate: task multirc, batch 350 (1212): ans_f1: 0.5858, qst_f1: 0.5517, em: 0.0038, avg: 0.2948, multirc_loss: 0.7335
03/25 06:34:04 PM: Evaluate: task multirc, batch 364 (1212): ans_f1: 0.5899, qst_f1: 0.5544, em: 0.0036, avg: 0.2967, multirc_loss: 0.7324
03/25 06:34:14 PM: Evaluate: task multirc, batch 377 (1212): ans_f1: 0.5914, qst_f1: 0.5578, em: 0.0035, avg: 0.2974, multirc_loss: 0.7319
03/25 06:34:25 PM: Evaluate: task multirc, batch 390 (1212): ans_f1: 0.5922, qst_f1: 0.5575, em: 0.0033, avg: 0.2978, multirc_loss: 0.7319
03/25 06:34:36 PM: Evaluate: task multirc, batch 404 (1212): ans_f1: 0.5866, qst_f1: 0.5520, em: 0.0032, avg: 0.2949, multirc_loss: 0.7338
03/25 06:34:46 PM: Evaluate: task multirc, batch 418 (1212): ans_f1: 0.5815, qst_f1: 0.5473, em: 0.0031, avg: 0.2923, multirc_loss: 0.7377
03/25 06:34:56 PM: Evaluate: task multirc, batch 431 (1212): ans_f1: 0.5775, qst_f1: 0.5400, em: 0.0030, avg: 0.2903, multirc_loss: 0.7375
03/25 06:35:07 PM: Evaluate: task multirc, batch 443 (1212): ans_f1: 0.5749, qst_f1: 0.5365, em: 0.0029, avg: 0.2889, multirc_loss: 0.7375
03/25 06:35:18 PM: Evaluate: task multirc, batch 456 (1212): ans_f1: 0.5738, qst_f1: 0.5361, em: 0.0028, avg: 0.2883, multirc_loss: 0.7387
03/25 06:35:28 PM: Evaluate: task multirc, batch 469 (1212): ans_f1: 0.5755, qst_f1: 0.5402, em: 0.0027, avg: 0.2891, multirc_loss: 0.7386
03/25 06:35:38 PM: Evaluate: task multirc, batch 482 (1212): ans_f1: 0.5728, qst_f1: 0.5330, em: 0.0027, avg: 0.2877, multirc_loss: 0.7371
03/25 06:35:49 PM: Evaluate: task multirc, batch 496 (1212): ans_f1: 0.5725, qst_f1: 0.5275, em: 0.0051, avg: 0.2888, multirc_loss: 0.7351
03/25 06:36:00 PM: Evaluate: task multirc, batch 510 (1212): ans_f1: 0.5702, qst_f1: 0.5241, em: 0.0025, avg: 0.2863, multirc_loss: 0.7342
03/25 06:36:11 PM: Evaluate: task multirc, batch 524 (1212): ans_f1: 0.5751, qst_f1: 0.5321, em: 0.0024, avg: 0.2888, multirc_loss: 0.7331
03/25 06:36:22 PM: Evaluate: task multirc, batch 537 (1212): ans_f1: 0.5746, qst_f1: 0.5325, em: 0.0047, avg: 0.2897, multirc_loss: 0.7337
03/25 06:36:32 PM: Evaluate: task multirc, batch 550 (1212): ans_f1: 0.5756, qst_f1: 0.5328, em: 0.0023, avg: 0.2890, multirc_loss: 0.7341
03/25 06:36:43 PM: Evaluate: task multirc, batch 563 (1212): ans_f1: 0.5752, qst_f1: 0.5328, em: 0.0022, avg: 0.2887, multirc_loss: 0.7350
03/25 06:36:53 PM: Evaluate: task multirc, batch 576 (1212): ans_f1: 0.5755, qst_f1: 0.5308, em: 0.0044, avg: 0.2900, multirc_loss: 0.7342
03/25 06:37:04 PM: Evaluate: task multirc, batch 590 (1212): ans_f1: 0.5752, qst_f1: 0.5290, em: 0.0022, avg: 0.2887, multirc_loss: 0.7339
03/25 06:37:15 PM: Evaluate: task multirc, batch 603 (1212): ans_f1: 0.5734, qst_f1: 0.5294, em: 0.0021, avg: 0.2878, multirc_loss: 0.7342
03/25 06:37:25 PM: Evaluate: task multirc, batch 617 (1212): ans_f1: 0.5715, qst_f1: 0.5263, em: 0.0021, avg: 0.2868, multirc_loss: 0.7335
03/25 06:37:36 PM: Evaluate: task multirc, batch 631 (1212): ans_f1: 0.5708, qst_f1: 0.5265, em: 0.0020, avg: 0.2864, multirc_loss: 0.7331
03/25 06:37:47 PM: Evaluate: task multirc, batch 645 (1212): ans_f1: 0.5715, qst_f1: 0.5274, em: 0.0020, avg: 0.2867, multirc_loss: 0.7325
03/25 06:37:57 PM: Evaluate: task multirc, batch 659 (1212): ans_f1: 0.5738, qst_f1: 0.5310, em: 0.0039, avg: 0.2888, multirc_loss: 0.7319
03/25 06:38:08 PM: Evaluate: task multirc, batch 673 (1212): ans_f1: 0.5744, qst_f1: 0.5319, em: 0.0019, avg: 0.2881, multirc_loss: 0.7323
03/25 06:38:19 PM: Evaluate: task multirc, batch 685 (1212): ans_f1: 0.5735, qst_f1: 0.5318, em: 0.0019, avg: 0.2877, multirc_loss: 0.7331
03/25 06:38:30 PM: Evaluate: task multirc, batch 698 (1212): ans_f1: 0.5727, qst_f1: 0.5324, em: 0.0018, avg: 0.2873, multirc_loss: 0.7338
03/25 06:38:41 PM: Evaluate: task multirc, batch 711 (1212): ans_f1: 0.5792, qst_f1: 0.5387, em: 0.0036, avg: 0.2914, multirc_loss: 0.7317
03/25 06:38:51 PM: Evaluate: task multirc, batch 724 (1212): ans_f1: 0.5809, qst_f1: 0.5395, em: 0.0018, avg: 0.2914, multirc_loss: 0.7309
03/25 06:39:02 PM: Evaluate: task multirc, batch 737 (1212): ans_f1: 0.5803, qst_f1: 0.5388, em: 0.0017, avg: 0.2910, multirc_loss: 0.7306
03/25 06:39:13 PM: Evaluate: task multirc, batch 751 (1212): ans_f1: 0.5782, qst_f1: 0.5360, em: 0.0017, avg: 0.2900, multirc_loss: 0.7307
03/25 06:39:24 PM: Evaluate: task multirc, batch 764 (1212): ans_f1: 0.5788, qst_f1: 0.5380, em: 0.0017, avg: 0.2903, multirc_loss: 0.7307
03/25 06:39:34 PM: Evaluate: task multirc, batch 777 (1212): ans_f1: 0.5812, qst_f1: 0.5408, em: 0.0033, avg: 0.2922, multirc_loss: 0.7302
03/25 06:39:44 PM: Evaluate: task multirc, batch 790 (1212): ans_f1: 0.5825, qst_f1: 0.5439, em: 0.0032, avg: 0.2929, multirc_loss: 0.7296
03/25 06:39:55 PM: Evaluate: task multirc, batch 803 (1212): ans_f1: 0.5835, qst_f1: 0.5455, em: 0.0047, avg: 0.2941, multirc_loss: 0.7290
03/25 06:40:06 PM: Evaluate: task multirc, batch 817 (1212): ans_f1: 0.5836, qst_f1: 0.5414, em: 0.0046, avg: 0.2941, multirc_loss: 0.7279
03/25 06:40:17 PM: Evaluate: task multirc, batch 831 (1212): ans_f1: 0.5824, qst_f1: 0.5383, em: 0.0076, avg: 0.2950, multirc_loss: 0.7275
03/25 06:40:27 PM: Evaluate: task multirc, batch 845 (1212): ans_f1: 0.5814, qst_f1: 0.5381, em: 0.0075, avg: 0.2944, multirc_loss: 0.7279
03/25 06:40:39 PM: Evaluate: task multirc, batch 859 (1212): ans_f1: 0.5818, qst_f1: 0.5393, em: 0.0088, avg: 0.2953, multirc_loss: 0.7278
03/25 06:40:49 PM: Evaluate: task multirc, batch 873 (1212): ans_f1: 0.5828, qst_f1: 0.5404, em: 0.0071, avg: 0.2950, multirc_loss: 0.7277
03/25 06:41:00 PM: Evaluate: task multirc, batch 886 (1212): ans_f1: 0.5858, qst_f1: 0.5446, em: 0.0084, avg: 0.2971, multirc_loss: 0.7271
03/25 06:41:10 PM: Evaluate: task multirc, batch 899 (1212): ans_f1: 0.5883, qst_f1: 0.5473, em: 0.0083, avg: 0.2983, multirc_loss: 0.7265
03/25 06:41:21 PM: Evaluate: task multirc, batch 913 (1212): ans_f1: 0.5867, qst_f1: 0.5462, em: 0.0068, avg: 0.2968, multirc_loss: 0.7274
03/25 06:41:32 PM: Evaluate: task multirc, batch 927 (1212): ans_f1: 0.5868, qst_f1: 0.5472, em: 0.0066, avg: 0.2967, multirc_loss: 0.7277
03/25 06:41:43 PM: Evaluate: task multirc, batch 941 (1212): ans_f1: 0.5849, qst_f1: 0.5450, em: 0.0066, avg: 0.2957, multirc_loss: 0.7277
03/25 06:41:54 PM: Evaluate: task multirc, batch 954 (1212): ans_f1: 0.5850, qst_f1: 0.5447, em: 0.0065, avg: 0.2957, multirc_loss: 0.7274
03/25 06:42:05 PM: Evaluate: task multirc, batch 967 (1212): ans_f1: 0.5843, qst_f1: 0.5444, em: 0.0064, avg: 0.2954, multirc_loss: 0.7271
03/25 06:42:16 PM: Evaluate: task multirc, batch 981 (1212): ans_f1: 0.5837, qst_f1: 0.5446, em: 0.0076, avg: 0.2957, multirc_loss: 0.7281
03/25 06:42:26 PM: Evaluate: task multirc, batch 994 (1212): ans_f1: 0.5839, qst_f1: 0.5439, em: 0.0063, avg: 0.2951, multirc_loss: 0.7284
03/25 06:42:38 PM: Evaluate: task multirc, batch 1008 (1212): ans_f1: 0.5833, qst_f1: 0.5442, em: 0.0062, avg: 0.2948, multirc_loss: 0.7286
03/25 06:42:48 PM: Evaluate: task multirc, batch 1021 (1212): ans_f1: 0.5826, qst_f1: 0.5438, em: 0.0062, avg: 0.2944, multirc_loss: 0.7290
03/25 06:42:59 PM: Evaluate: task multirc, batch 1034 (1212): ans_f1: 0.5820, qst_f1: 0.5438, em: 0.0061, avg: 0.2940, multirc_loss: 0.7290
03/25 06:43:09 PM: Evaluate: task multirc, batch 1047 (1212): ans_f1: 0.5822, qst_f1: 0.5432, em: 0.0060, avg: 0.2941, multirc_loss: 0.7286
03/25 06:43:20 PM: Evaluate: task multirc, batch 1060 (1212): ans_f1: 0.5829, qst_f1: 0.5450, em: 0.0059, avg: 0.2944, multirc_loss: 0.7283
03/25 06:43:31 PM: Evaluate: task multirc, batch 1073 (1212): ans_f1: 0.5830, qst_f1: 0.5441, em: 0.0058, avg: 0.2944, multirc_loss: 0.7281
03/25 06:43:42 PM: Evaluate: task multirc, batch 1086 (1212): ans_f1: 0.5839, qst_f1: 0.5460, em: 0.0069, avg: 0.2954, multirc_loss: 0.7282
03/25 06:43:52 PM: Evaluate: task multirc, batch 1098 (1212): ans_f1: 0.5834, qst_f1: 0.5448, em: 0.0057, avg: 0.2946, multirc_loss: 0.7287
03/25 06:44:03 PM: Evaluate: task multirc, batch 1112 (1212): ans_f1: 0.5825, qst_f1: 0.5449, em: 0.0057, avg: 0.2941, multirc_loss: 0.7288
03/25 06:44:14 PM: Evaluate: task multirc, batch 1126 (1212): ans_f1: 0.5816, qst_f1: 0.5446, em: 0.0056, avg: 0.2936, multirc_loss: 0.7287
03/25 06:44:25 PM: Evaluate: task multirc, batch 1140 (1212): ans_f1: 0.5800, qst_f1: 0.5439, em: 0.0056, avg: 0.2928, multirc_loss: 0.7292
03/25 06:44:36 PM: Evaluate: task multirc, batch 1154 (1212): ans_f1: 0.5792, qst_f1: 0.5427, em: 0.0055, avg: 0.2923, multirc_loss: 0.7294
03/25 06:44:47 PM: Evaluate: task multirc, batch 1167 (1212): ans_f1: 0.5769, qst_f1: 0.5390, em: 0.0054, avg: 0.2912, multirc_loss: 0.7293
03/25 06:44:58 PM: Evaluate: task multirc, batch 1181 (1212): ans_f1: 0.5757, qst_f1: 0.5354, em: 0.0054, avg: 0.2906, multirc_loss: 0.7290
03/25 06:45:09 PM: Evaluate: task multirc, batch 1194 (1212): ans_f1: 0.5762, qst_f1: 0.5361, em: 0.0053, avg: 0.2908, multirc_loss: 0.7287
03/25 06:45:19 PM: Evaluate: task multirc, batch 1207 (1212): ans_f1: 0.5777, qst_f1: 0.5371, em: 0.0053, avg: 0.2915, multirc_loss: 0.7282
03/25 06:45:23 PM: Updating LR scheduler:
03/25 06:45:23 PM: 	Best result seen so far for macro_avg: 0.300
03/25 06:45:23 PM: 	# validation passes without improvement: 0
03/25 06:45:23 PM: Ran out of early stopping patience. Stopping training.
03/25 06:45:23 PM: multirc_loss: training: 0.734223 validation: 0.727856
03/25 06:45:23 PM: macro_avg: validation: 0.291834
03/25 06:45:23 PM: micro_avg: validation: 0.291834
03/25 06:45:23 PM: multirc_ans_f1: training: 0.857143 validation: 0.578421
03/25 06:45:23 PM: multirc_qst_f1: training: 0.750000 validation: 0.537628
03/25 06:45:23 PM: multirc_em: training: 0.750000 validation: 0.005247
03/25 06:45:23 PM: multirc_avg: training: 0.803571 validation: 0.291834
03/25 06:45:23 PM: Global learning rate: 0.00015
03/25 06:45:23 PM: Saving checkpoints to: /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 06:45:25 PM: Stopped training after 10 validation checks
03/25 06:45:25 PM: Trained multirc for 10 steps or 0.001 epochs
03/25 06:45:25 PM: ***** VALIDATION RESULTS *****
03/25 06:45:25 PM: multirc_avg (for best val pass 7): multirc_loss: 0.72721, macro_avg: 0.30005, micro_avg: 0.30005, multirc_ans_f1: 0.59066, multirc_qst_f1: 0.56299, multirc_em: 0.00944, multirc_avg: 0.30005
03/25 06:45:25 PM: micro_avg (for best val pass 7): multirc_loss: 0.72721, macro_avg: 0.30005, micro_avg: 0.30005, multirc_ans_f1: 0.59066, multirc_qst_f1: 0.56299, multirc_em: 0.00944, multirc_avg: 0.30005
03/25 06:45:25 PM: macro_avg (for best val pass 7): multirc_loss: 0.72721, macro_avg: 0.30005, micro_avg: 0.30005, multirc_ans_f1: 0.59066, multirc_qst_f1: 0.56299, multirc_em: 0.00944, multirc_avg: 0.30005
03/25 06:45:25 PM: Evaluating...
03/25 06:45:25 PM: Loaded model state from /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc/multirc/model_state_target_train_val_7.best.th
03/25 06:45:25 PM: Evaluating on: multirc, split: val
03/25 06:45:56 PM: 	Task multirc: batch 37
03/25 06:46:26 PM: 	Task multirc: batch 75
03/25 06:46:57 PM: 	Task multirc: batch 116
03/25 06:47:27 PM: 	Task multirc: batch 154
03/25 06:47:57 PM: 	Task multirc: batch 193
03/25 06:48:27 PM: 	Task multirc: batch 232
03/25 06:48:58 PM: 	Task multirc: batch 272
03/25 06:49:29 PM: 	Task multirc: batch 313
03/25 06:49:59 PM: 	Task multirc: batch 353
03/25 06:50:30 PM: 	Task multirc: batch 392
03/25 06:51:00 PM: 	Task multirc: batch 432
03/25 06:51:31 PM: 	Task multirc: batch 471
03/25 06:52:01 PM: 	Task multirc: batch 511
03/25 06:52:32 PM: 	Task multirc: batch 550
03/25 06:53:02 PM: 	Task multirc: batch 589
03/25 06:53:33 PM: 	Task multirc: batch 629
03/25 06:54:03 PM: 	Task multirc: batch 670
03/25 06:54:34 PM: 	Task multirc: batch 707
03/25 06:55:04 PM: 	Task multirc: batch 746
03/25 06:55:35 PM: 	Task multirc: batch 785
03/25 06:56:05 PM: 	Task multirc: batch 825
03/25 06:56:36 PM: 	Task multirc: batch 865
03/25 06:57:06 PM: 	Task multirc: batch 905
03/25 06:57:37 PM: 	Task multirc: batch 945
03/25 06:58:07 PM: 	Task multirc: batch 984
03/25 06:58:38 PM: 	Task multirc: batch 1023
03/25 06:59:09 PM: 	Task multirc: batch 1062
03/25 06:59:39 PM: 	Task multirc: batch 1099
03/25 07:00:09 PM: 	Task multirc: batch 1139
03/25 07:00:39 PM: 	Task multirc: batch 1179
03/25 07:01:05 PM: Task 'multirc': sorting predictions by 'idx'
03/25 07:01:05 PM: Finished evaluating on: multirc
03/25 07:01:05 PM: Task 'multirc': Wrote predictions to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 07:01:05 PM: Wrote all preds for split 'val' to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 07:01:05 PM: Evaluating on: multirc, split: test
03/25 07:01:35 PM: 	Task multirc: batch 37
03/25 07:02:06 PM: 	Task multirc: batch 76
03/25 07:02:36 PM: 	Task multirc: batch 115
03/25 07:03:06 PM: 	Task multirc: batch 154
03/25 07:03:37 PM: 	Task multirc: batch 194
03/25 07:04:07 PM: 	Task multirc: batch 234
03/25 07:04:38 PM: 	Task multirc: batch 274
03/25 07:05:08 PM: 	Task multirc: batch 312
03/25 07:05:38 PM: 	Task multirc: batch 351
03/25 07:06:09 PM: 	Task multirc: batch 392
03/25 07:06:39 PM: 	Task multirc: batch 432
03/25 07:07:10 PM: 	Task multirc: batch 472
03/25 07:07:40 PM: 	Task multirc: batch 513
03/25 07:08:11 PM: 	Task multirc: batch 553
03/25 07:08:41 PM: 	Task multirc: batch 592
03/25 07:09:12 PM: 	Task multirc: batch 633
03/25 07:09:42 PM: 	Task multirc: batch 672
03/25 07:10:12 PM: 	Task multirc: batch 709
03/25 07:10:43 PM: 	Task multirc: batch 748
03/25 07:11:13 PM: 	Task multirc: batch 788
03/25 07:11:44 PM: 	Task multirc: batch 826
03/25 07:12:14 PM: 	Task multirc: batch 865
03/25 07:12:44 PM: 	Task multirc: batch 905
03/25 07:13:14 PM: 	Task multirc: batch 943
03/25 07:13:45 PM: 	Task multirc: batch 982
03/25 07:14:15 PM: 	Task multirc: batch 1022
03/25 07:14:45 PM: 	Task multirc: batch 1061
03/25 07:15:15 PM: 	Task multirc: batch 1099
03/25 07:15:46 PM: 	Task multirc: batch 1139
03/25 07:16:17 PM: 	Task multirc: batch 1179
03/25 07:16:47 PM: 	Task multirc: batch 1218
03/25 07:17:17 PM: 	Task multirc: batch 1257
03/25 07:17:48 PM: 	Task multirc: batch 1297
03/25 07:18:18 PM: 	Task multirc: batch 1335
03/25 07:18:49 PM: 	Task multirc: batch 1374
03/25 07:19:19 PM: 	Task multirc: batch 1414
03/25 07:19:49 PM: 	Task multirc: batch 1453
03/25 07:20:19 PM: 	Task multirc: batch 1492
03/25 07:20:50 PM: 	Task multirc: batch 1529
03/25 07:21:20 PM: 	Task multirc: batch 1568
03/25 07:21:50 PM: 	Task multirc: batch 1606
03/25 07:22:21 PM: 	Task multirc: batch 1643
03/25 07:22:51 PM: 	Task multirc: batch 1684
03/25 07:23:22 PM: 	Task multirc: batch 1722
03/25 07:23:52 PM: 	Task multirc: batch 1761
03/25 07:24:22 PM: 	Task multirc: batch 1799
03/25 07:24:53 PM: 	Task multirc: batch 1839
03/25 07:25:23 PM: 	Task multirc: batch 1879
03/25 07:25:53 PM: 	Task multirc: batch 1918
03/25 07:26:24 PM: 	Task multirc: batch 1958
03/25 07:26:54 PM: 	Task multirc: batch 1995
03/25 07:27:25 PM: 	Task multirc: batch 2033
03/25 07:27:55 PM: 	Task multirc: batch 2073
03/25 07:28:26 PM: 	Task multirc: batch 2112
03/25 07:28:56 PM: 	Task multirc: batch 2150
03/25 07:29:26 PM: 	Task multirc: batch 2189
03/25 07:29:57 PM: 	Task multirc: batch 2227
03/25 07:30:28 PM: 	Task multirc: batch 2266
03/25 07:30:58 PM: 	Task multirc: batch 2305
03/25 07:31:28 PM: 	Task multirc: batch 2344
03/25 07:31:59 PM: 	Task multirc: batch 2382
03/25 07:32:29 PM: 	Task multirc: batch 2420
03/25 07:32:32 PM: Task 'multirc': sorting predictions by 'idx'
03/25 07:32:32 PM: Finished evaluating on: multirc
03/25 07:32:33 PM: Task 'multirc': Wrote predictions to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 07:32:33 PM: Wrote all preds for split 'test' to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/bert_uncased_multirc
03/25 07:32:33 PM: Writing results for split 'val' to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/results.tsv
03/25 07:32:33 PM: micro_avg: 0.300, macro_avg: 0.300, multirc_ans_f1: 0.591, multirc_qst_f1: 0.563, multirc_em: 0.009, multirc_avg: 0.300
03/25 07:32:33 PM: Done!
