03/25 11:49:54 AM: Git branch: develop
03/25 11:49:54 AM: Git SHA: ed5c041a278c4702c8820ebddd1bcaafc4bf42d5
03/25 11:49:55 AM: Parsed args: 
{
  "classifier": "log_reg",
  "dropout": 0.1,
  "dropout_embs": 0.1,
  "exp_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/",
  "exp_name": "bert_uncased_multirc_PT",
  "input_module": "bert-base-cased",
  "local_log_path": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0/log.log",
  "lr": 5e-06,
  "lr_patience": 4,
  "max_epochs": 10,
  "max_seq_len": 256,
  "max_vals": 10,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "remote_log_name": "bert_uncased_multirc_PT__run0",
  "run_dir": "/Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0",
  "run_name": "run0",
  "s2s": {
    "attention": "none"
  },
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "multirc",
  "target_train_max_vals": 100,
  "target_train_val_interval": 1,
  "transfer_paradigm": "finetune",
  "transformers_output_mode": "top",
  "val_interval": 1,
  "write_preds": "val,test",
  "write_strict_glue_format": 1
}
03/25 11:49:55 AM: Saved config to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0/params.conf
03/25 11:49:55 AM: Using random seed 1234
03/25 11:49:55 AM: Loading tasks...
03/25 11:49:55 AM: Writing pre-preprocessed tasks to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/
03/25 11:49:55 AM: 	Loaded existing task multirc
03/25 11:49:55 AM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
03/25 11:49:55 AM: 	Loaded existing task sst
03/25 11:49:55 AM: 	Task 'sst': |train|=67349 |val|=872 |test|=1821
03/25 11:49:55 AM: 	Finished loading tasks: multirc sst.
03/25 11:49:55 AM: Loading token dictionary from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/vocab.
03/25 11:49:55 AM: 	Loaded vocab from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/vocab
03/25 11:49:55 AM: 	Vocab namespace chars: size 116
03/25 11:49:55 AM: 	Vocab namespace bert_cased: size 28998
03/25 11:49:55 AM: 	Vocab namespace tokens: size 18815
03/25 11:49:55 AM: 	Finished building vocab.
03/25 11:49:55 AM: 	Task 'multirc', split 'train': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/multirc__train_data
03/25 11:49:55 AM: 	Task 'multirc', split 'val': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/multirc__val_data
03/25 11:49:55 AM: 	Task 'multirc', split 'test': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/multirc__test_data
03/25 11:49:55 AM: 	Task 'sst', split 'train': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/sst__train_data
03/25 11:49:55 AM: 	Task 'sst', split 'val': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/sst__val_data
03/25 11:49:55 AM: 	Task 'sst', split 'test': Found preprocessed copy in /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/preproc/sst__test_data
03/25 11:49:55 AM: 	Finished indexing tasks
03/25 11:49:55 AM: 	Creating trimmed target-only version of multirc train.
03/25 11:49:55 AM: 	Creating trimmed pretraining-only version of sst train.
03/25 11:49:55 AM: 	  Training on sst
03/25 11:49:55 AM: 	  Evaluating on multirc
03/25 11:49:55 AM: 	Finished loading tasks in 0.335s
03/25 11:49:55 AM: 	 Tasks: ['multirc', 'sst']
03/25 11:49:55 AM: Building model...
03/25 11:49:55 AM: Using BERT model (bert-base-cased).
03/25 11:49:56 AM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/transformers_cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e
03/25 11:49:56 AM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

03/25 11:49:56 AM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/transformers_cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/25 11:49:58 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/transformers_cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/25 11:49:58 AM: Initializing parameters
03/25 11:49:58 AM: Done initializing parameters; the following parameters are using their default initialization from their code
03/25 11:49:58 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
03/25 11:49:58 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
03/25 11:49:58 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
03/25 11:49:58 AM:    _text_field_embedder.model.pooler.dense.bias
03/25 11:49:58 AM:    _text_field_embedder.model.pooler.dense.weight
03/25 11:49:58 AM: 	Task 'multirc' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
03/25 11:49:58 AM: 	Task 'sst' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "sst"
}
03/25 11:49:58 AM: Model specification:
03/25 11:49:58 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.1)
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (sst_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
03/25 11:49:58 AM: Model parameters:
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 22268928 with torch.Size([28996, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
03/25 11:49:58 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
03/25 11:49:58 AM: 	multirc_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/25 11:49:58 AM: 	multirc_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
03/25 11:49:58 AM: 	sst_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
03/25 11:49:58 AM: 	sst_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
03/25 11:49:58 AM: Total number of parameters: 108313348 (1.08313e+08)
03/25 11:49:58 AM: Number of trainable parameters: 108313348 (1.08313e+08)
03/25 11:49:58 AM: Finished building model in 2.992s
03/25 11:49:58 AM: Will run the following steps for this experiment:
Training model on tasks: sst 
Re-training model for individual target tasks 
Evaluating model on tasks: multirc 

03/25 11:49:58 AM: Training...
03/25 11:49:58 AM: patience = 5
03/25 11:49:58 AM: val_interval = 1
03/25 11:49:58 AM: max_vals = 10
03/25 11:49:58 AM: cuda_device = -1
03/25 11:49:58 AM: grad_norm = 5.0
03/25 11:49:58 AM: grad_clipping = None
03/25 11:49:58 AM: lr_decay = 0.99
03/25 11:49:58 AM: min_lr = 1e-06
03/25 11:49:58 AM: keep_all_checkpoints = 0
03/25 11:49:58 AM: val_data_limit = 5000
03/25 11:49:58 AM: max_epochs = 10
03/25 11:49:58 AM: dec_val_scale = 250
03/25 11:49:58 AM: training_data_fraction = 1
03/25 11:49:58 AM: accumulation_steps = 1
03/25 11:49:58 AM: type = bert_adam
03/25 11:49:58 AM: parameter_groups = None
03/25 11:49:58 AM: Number of trainable parameters: 108313348
03/25 11:49:58 AM: infer_type_and_cast = True
03/25 11:49:58 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/25 11:49:58 AM: CURRENTLY DEFINED PARAMETERS: 
03/25 11:49:58 AM: lr = 5e-06
03/25 11:49:58 AM: t_total = 10
03/25 11:49:58 AM: warmup = 0.1
03/25 11:49:58 AM: type = reduce_on_plateau
03/25 11:49:58 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/25 11:49:58 AM: CURRENTLY DEFINED PARAMETERS: 
03/25 11:49:58 AM: mode = max
03/25 11:49:58 AM: factor = 0.5
03/25 11:49:58 AM: patience = 4
03/25 11:49:58 AM: threshold = 0.0001
03/25 11:49:58 AM: threshold_mode = abs
03/25 11:49:58 AM: verbose = True
03/25 11:49:58 AM: load_model=1 but there is not checkpoint.                         Starting training without restoring from a checkpoint.
03/25 11:49:58 AM: Training examples per task, before any subsampling: {'sst': 67349}
03/25 11:49:58 AM: Beginning training with stopping criteria based on metric: sst_accuracy
03/25 11:50:03 AM: ***** Step 1 / Validation 1 *****
03/25 11:50:03 AM: sst: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 11:50:03 AM: Validating...
03/25 11:50:09 AM: Evaluate: task sst, batch 4 (28): accuracy: 0.4766, sst_loss: 0.7967
03/25 11:50:19 AM: Evaluate: task sst, batch 12 (28): accuracy: 0.4792, sst_loss: 0.7920
03/25 11:50:30 AM: Evaluate: task sst, batch 21 (28): accuracy: 0.4881, sst_loss: 0.7863
03/25 11:50:40 AM: Best result seen so far for sst.
03/25 11:50:40 AM: Best result seen so far for micro.
03/25 11:50:40 AM: Best result seen so far for macro.
03/25 11:50:40 AM: Updating LR scheduler:
03/25 11:50:40 AM: 	Best result seen so far for macro_avg: 0.491
03/25 11:50:40 AM: 	# validation passes without improvement: 0
03/25 11:50:40 AM: sst_loss: training: 0.788728 validation: 0.782865
03/25 11:50:40 AM: macro_avg: validation: 0.490826
03/25 11:50:40 AM: micro_avg: validation: 0.490826
03/25 11:50:40 AM: sst_accuracy: training: 0.500000 validation: 0.490826
03/25 11:50:40 AM: Global learning rate: 5e-06
03/25 11:50:40 AM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 11:50:48 AM: Update 2: task sst, steps since last val 1 (total steps = 2): accuracy: 0.4062, sst_loss: 0.8445
03/25 11:50:48 AM: ***** Step 2 / Validation 2 *****
03/25 11:50:48 AM: sst: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 11:50:48 AM: Validating...
03/25 11:50:59 AM: Evaluate: task sst, batch 8 (28): accuracy: 0.4883, sst_loss: 0.7231
03/25 11:51:09 AM: Evaluate: task sst, batch 16 (28): accuracy: 0.4648, sst_loss: 0.7341
03/25 11:51:20 AM: Evaluate: task sst, batch 25 (28): accuracy: 0.4913, sst_loss: 0.7205
03/25 11:51:23 AM: Updating LR scheduler:
03/25 11:51:23 AM: 	Best result seen so far for macro_avg: 0.491
03/25 11:51:23 AM: 	# validation passes without improvement: 1
03/25 11:51:23 AM: sst_loss: training: 0.844503 validation: 0.722095
03/25 11:51:23 AM: macro_avg: validation: 0.490826
03/25 11:51:23 AM: micro_avg: validation: 0.490826
03/25 11:51:23 AM: sst_accuracy: training: 0.406250 validation: 0.490826
03/25 11:51:23 AM: Global learning rate: 5e-06
03/25 11:51:23 AM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 11:51:26 AM: ***** Step 3 / Validation 3 *****
03/25 11:51:26 AM: sst: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 11:51:26 AM: Validating...
03/25 11:51:30 AM: Evaluate: task sst, batch 3 (28): accuracy: 0.4896, sst_loss: 0.6926
03/25 11:51:41 AM: Evaluate: task sst, batch 11 (28): accuracy: 0.5057, sst_loss: 0.6851
03/25 11:51:51 AM: Evaluate: task sst, batch 19 (28): accuracy: 0.5329, sst_loss: 0.6824
03/25 11:52:01 AM: Evaluate: task sst, batch 27 (28): accuracy: 0.5394, sst_loss: 0.6815
03/25 11:52:02 AM: Best result seen so far for sst.
03/25 11:52:02 AM: Best result seen so far for micro.
03/25 11:52:02 AM: Best result seen so far for macro.
03/25 11:52:02 AM: Updating LR scheduler:
03/25 11:52:02 AM: 	Best result seen so far for macro_avg: 0.538
03/25 11:52:02 AM: 	# validation passes without improvement: 0
03/25 11:52:02 AM: sst_loss: training: 0.780745 validation: 0.683008
03/25 11:52:02 AM: macro_avg: validation: 0.537844
03/25 11:52:02 AM: micro_avg: validation: 0.537844
03/25 11:52:02 AM: sst_accuracy: training: 0.343750 validation: 0.537844
03/25 11:52:02 AM: Global learning rate: 5e-06
03/25 11:52:02 AM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 11:52:05 AM: ***** Step 4 / Validation 4 *****
03/25 11:52:05 AM: sst: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 11:52:05 AM: Validating...
03/25 11:52:12 AM: Evaluate: task sst, batch 5 (28): accuracy: 0.5687, sst_loss: 0.6848
03/25 11:52:22 AM: Evaluate: task sst, batch 13 (28): accuracy: 0.5889, sst_loss: 0.6736
03/25 11:52:33 AM: Evaluate: task sst, batch 20 (28): accuracy: 0.5813, sst_loss: 0.6742
03/25 11:52:42 AM: Best result seen so far for sst.
03/25 11:52:42 AM: Best result seen so far for micro.
03/25 11:52:42 AM: Best result seen so far for macro.
03/25 11:52:42 AM: Updating LR scheduler:
03/25 11:52:42 AM: 	Best result seen so far for macro_avg: 0.591
03/25 11:52:42 AM: 	# validation passes without improvement: 0
03/25 11:52:42 AM: sst_loss: training: 0.710493 validation: 0.674480
03/25 11:52:42 AM: macro_avg: validation: 0.590596
03/25 11:52:42 AM: micro_avg: validation: 0.590596
03/25 11:52:42 AM: sst_accuracy: training: 0.500000 validation: 0.590596
03/25 11:52:42 AM: Global learning rate: 5e-06
03/25 11:52:42 AM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 11:52:48 AM: Update 5: task sst, steps since last val 1 (total steps = 5): accuracy: 0.5938, sst_loss: 0.7012
03/25 11:52:48 AM: ***** Step 5 / Validation 5 *****
03/25 11:52:48 AM: sst: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 11:52:48 AM: Validating...
03/25 11:52:59 AM: Evaluate: task sst, batch 8 (28): accuracy: 0.5391, sst_loss: 0.6837
03/25 11:53:10 AM: Evaluate: task sst, batch 17 (28): accuracy: 0.5533, sst_loss: 0.6740
03/25 11:53:21 AM: Evaluate: task sst, batch 26 (28): accuracy: 0.5288, sst_loss: 0.6828
03/25 11:53:23 AM: Updating LR scheduler:
03/25 11:53:23 AM: 	Best result seen so far for macro_avg: 0.591
03/25 11:53:23 AM: 	# validation passes without improvement: 1
03/25 11:53:23 AM: sst_loss: training: 0.701200 validation: 0.685300
03/25 11:53:23 AM: macro_avg: validation: 0.527523
03/25 11:53:23 AM: micro_avg: validation: 0.527523
03/25 11:53:23 AM: sst_accuracy: training: 0.593750 validation: 0.527523
03/25 11:53:23 AM: Global learning rate: 5e-06
03/25 11:53:23 AM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 11:53:27 AM: ***** Step 6 / Validation 6 *****
03/25 11:53:27 AM: sst: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 11:53:27 AM: Validating...
03/25 11:53:31 AM: Evaluate: task sst, batch 3 (28): accuracy: 0.5312, sst_loss: 0.6975
03/25 11:53:41 AM: Evaluate: task sst, batch 11 (28): accuracy: 0.5369, sst_loss: 0.6859
03/25 11:53:52 AM: Evaluate: task sst, batch 20 (28): accuracy: 0.5203, sst_loss: 0.6960
03/25 11:54:01 AM: Updating LR scheduler:
03/25 11:54:01 AM: 	Best result seen so far for macro_avg: 0.591
03/25 11:54:01 AM: 	# validation passes without improvement: 2
03/25 11:54:01 AM: sst_loss: training: 0.720270 validation: 0.696863
03/25 11:54:01 AM: macro_avg: validation: 0.514908
03/25 11:54:01 AM: micro_avg: validation: 0.514908
03/25 11:54:01 AM: sst_accuracy: training: 0.500000 validation: 0.514908
03/25 11:54:01 AM: Global learning rate: 5e-06
03/25 11:54:01 AM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 11:54:07 AM: Update 7: task sst, steps since last val 1 (total steps = 7): accuracy: 0.6562, sst_loss: 0.6219
03/25 11:54:07 AM: ***** Step 7 / Validation 7 *****
03/25 11:54:07 AM: sst: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 11:54:07 AM: Validating...
03/25 11:54:17 AM: Evaluate: task sst, batch 8 (28): accuracy: 0.5195, sst_loss: 0.7042
03/25 11:54:27 AM: Evaluate: task sst, batch 17 (28): accuracy: 0.5386, sst_loss: 0.6902
03/25 11:54:38 AM: Evaluate: task sst, batch 26 (28): accuracy: 0.5132, sst_loss: 0.7041
03/25 11:54:39 AM: Updating LR scheduler:
03/25 11:54:39 AM: 	Best result seen so far for macro_avg: 0.591
03/25 11:54:39 AM: 	# validation passes without improvement: 3
03/25 11:54:39 AM: sst_loss: training: 0.621871 validation: 0.706963
03/25 11:54:39 AM: macro_avg: validation: 0.512615
03/25 11:54:39 AM: micro_avg: validation: 0.512615
03/25 11:54:39 AM: sst_accuracy: training: 0.656250 validation: 0.512615
03/25 11:54:39 AM: Global learning rate: 5e-06
03/25 11:54:39 AM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 11:54:43 AM: ***** Step 8 / Validation 8 *****
03/25 11:54:43 AM: sst: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 11:54:43 AM: Validating...
03/25 11:54:48 AM: Evaluate: task sst, batch 4 (28): accuracy: 0.5234, sst_loss: 0.7095
03/25 11:54:59 AM: Evaluate: task sst, batch 13 (28): accuracy: 0.5192, sst_loss: 0.7080
03/25 11:55:09 AM: Evaluate: task sst, batch 22 (28): accuracy: 0.5142, sst_loss: 0.7123
03/25 11:55:16 AM: Updating LR scheduler:
03/25 11:55:16 AM: 	Best result seen so far for macro_avg: 0.591
03/25 11:55:16 AM: 	# validation passes without improvement: 4
03/25 11:55:16 AM: sst_loss: training: 0.621913 validation: 0.712872
03/25 11:55:16 AM: macro_avg: validation: 0.511468
03/25 11:55:16 AM: micro_avg: validation: 0.511468
03/25 11:55:16 AM: sst_accuracy: training: 0.656250 validation: 0.511468
03/25 11:55:16 AM: Global learning rate: 5e-06
03/25 11:55:16 AM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 11:55:19 AM: ***** Step 9 / Validation 9 *****
03/25 11:55:19 AM: sst: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 11:55:19 AM: Validating...
03/25 11:55:21 AM: Evaluate: task sst, batch 1 (28): accuracy: 0.4375, sst_loss: 0.7435
03/25 11:55:32 AM: Evaluate: task sst, batch 10 (28): accuracy: 0.5312, sst_loss: 0.6959
03/25 11:55:42 AM: Evaluate: task sst, batch 19 (28): accuracy: 0.5132, sst_loss: 0.7103
03/25 11:55:53 AM: Evaluate: task sst, batch 27 (28): accuracy: 0.5116, sst_loss: 0.7093
03/25 11:55:53 AM: Updating LR scheduler:
03/25 11:55:53 AM: 	Best result seen so far for macro_avg: 0.591
03/25 11:55:53 AM: 	# validation passes without improvement: 0
03/25 11:55:53 AM: sst_loss: training: 0.736125 validation: 0.711514
03/25 11:55:53 AM: macro_avg: validation: 0.511468
03/25 11:55:53 AM: micro_avg: validation: 0.511468
03/25 11:55:53 AM: sst_accuracy: training: 0.531250 validation: 0.511468
03/25 11:55:53 AM: Global learning rate: 2.5e-06
03/25 11:55:53 AM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 11:55:56 AM: ***** Step 10 / Validation 10 *****
03/25 11:55:56 AM: sst: trained on 1 steps (1 batches) since val, 0.000 epochs
03/25 11:55:56 AM: Validating...
03/25 11:56:04 AM: Evaluate: task sst, batch 6 (28): accuracy: 0.4896, sst_loss: 0.7352
03/25 11:56:15 AM: Evaluate: task sst, batch 14 (28): accuracy: 0.5268, sst_loss: 0.7004
03/25 11:56:25 AM: Evaluate: task sst, batch 22 (28): accuracy: 0.5142, sst_loss: 0.7100
03/25 11:56:32 AM: Updating LR scheduler:
03/25 11:56:32 AM: 	Best result seen so far for macro_avg: 0.591
03/25 11:56:32 AM: 	# validation passes without improvement: 1
03/25 11:56:32 AM: Ran out of early stopping patience. Stopping training.
03/25 11:56:32 AM: Maximum number of validations reached. Stopping training.
03/25 11:56:32 AM: sst_loss: training: 0.608399 validation: 0.710574
03/25 11:56:32 AM: macro_avg: validation: 0.511468
03/25 11:56:32 AM: micro_avg: validation: 0.511468
03/25 11:56:32 AM: sst_accuracy: training: 0.593750 validation: 0.511468
03/25 11:56:32 AM: Global learning rate: 2.5e-06
03/25 11:56:32 AM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 11:56:33 AM: Stopped training after 10 validation checks
03/25 11:56:33 AM: Trained sst for 10 steps or 0.005 epochs
03/25 11:56:33 AM: ***** VALIDATION RESULTS *****
03/25 11:56:33 AM: sst_accuracy (for best val pass 4): sst_loss: 0.67448, macro_avg: 0.59060, micro_avg: 0.59060, sst_accuracy: 0.59060
03/25 11:56:33 AM: micro_avg (for best val pass 4): sst_loss: 0.67448, macro_avg: 0.59060, micro_avg: 0.59060, sst_accuracy: 0.59060
03/25 11:56:33 AM: macro_avg (for best val pass 4): sst_loss: 0.67448, macro_avg: 0.59060, micro_avg: 0.59060, sst_accuracy: 0.59060
03/25 11:56:33 AM: Not loading task-specific parameters for task: multirc
03/25 11:56:34 AM: Loaded model state from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0/model_state_pretrain_val_4.best.th
03/25 11:56:34 AM: patience = 5
03/25 11:56:34 AM: val_interval = 1
03/25 11:56:34 AM: max_vals = 100
03/25 11:56:34 AM: cuda_device = -1
03/25 11:56:34 AM: grad_norm = 5.0
03/25 11:56:34 AM: grad_clipping = None
03/25 11:56:34 AM: lr_decay = 0.99
03/25 11:56:34 AM: min_lr = 1e-06
03/25 11:56:34 AM: keep_all_checkpoints = 0
03/25 11:56:34 AM: val_data_limit = 5000
03/25 11:56:34 AM: max_epochs = 10
03/25 11:56:34 AM: dec_val_scale = 250
03/25 11:56:34 AM: training_data_fraction = 1
03/25 11:56:34 AM: accumulation_steps = 1
03/25 11:56:34 AM: type = bert_adam
03/25 11:56:34 AM: parameter_groups = None
03/25 11:56:34 AM: Number of trainable parameters: 108313348
03/25 11:56:34 AM: infer_type_and_cast = True
03/25 11:56:34 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/25 11:56:34 AM: CURRENTLY DEFINED PARAMETERS: 
03/25 11:56:34 AM: lr = 5e-06
03/25 11:56:34 AM: t_total = 100
03/25 11:56:34 AM: warmup = 0.1
03/25 11:56:34 AM: type = reduce_on_plateau
03/25 11:56:34 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
03/25 11:56:34 AM: CURRENTLY DEFINED PARAMETERS: 
03/25 11:56:34 AM: mode = max
03/25 11:56:34 AM: factor = 0.5
03/25 11:56:34 AM: patience = 4
03/25 11:56:34 AM: threshold = 0.0001
03/25 11:56:34 AM: threshold_mode = abs
03/25 11:56:34 AM: verbose = True
03/25 11:56:34 AM: Starting training without restoring from a checkpoint.
03/25 11:56:34 AM: Training examples per task, before any subsampling: {'multirc': 27243}
03/25 11:56:34 AM: Beginning training with stopping criteria based on metric: multirc_avg
03/25 11:57:08 AM: Update 1: task multirc, steps since last val 1 (total steps = 1): ans_f1: 0.8571, qst_f1: 0.8000, em: 0.8000, avg: 0.8286, multirc_loss: 0.6537
03/25 11:57:08 AM: ***** Step 1 / Validation 1 *****
03/25 11:57:08 AM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 11:57:08 AM: Validating...
03/25 11:57:27 AM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.6118, qst_f1: 0.5933, em: 0.0769, avg: 0.3443, multirc_loss: 0.7050
03/25 11:57:38 AM: Evaluate: task multirc, batch 3 (152): ans_f1: 0.6047, qst_f1: 0.5828, em: 0.1000, avg: 0.3523, multirc_loss: 0.7046
03/25 11:57:54 AM: Evaluate: task multirc, batch 5 (152): ans_f1: 0.6055, qst_f1: 0.5874, em: 0.0270, avg: 0.3163, multirc_loss: 0.7015
03/25 11:58:12 AM: Evaluate: task multirc, batch 7 (152): ans_f1: 0.5847, qst_f1: 0.5517, em: 0.0189, avg: 0.3018, multirc_loss: 0.7008
03/25 11:58:32 AM: Evaluate: task multirc, batch 9 (152): ans_f1: 0.5729, qst_f1: 0.5453, em: 0.0441, avg: 0.3085, multirc_loss: 0.7037
03/25 11:58:48 AM: Evaluate: task multirc, batch 11 (152): ans_f1: 0.5895, qst_f1: 0.5550, em: 0.0253, avg: 0.3074, multirc_loss: 0.7019
03/25 11:59:04 AM: Evaluate: task multirc, batch 13 (152): ans_f1: 0.5857, qst_f1: 0.5463, em: 0.0319, avg: 0.3088, multirc_loss: 0.7029
03/25 11:59:15 AM: Evaluate: task multirc, batch 15 (152): ans_f1: 0.5649, qst_f1: 0.5057, em: 0.0286, avg: 0.2967, multirc_loss: 0.7016
03/25 11:59:36 AM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.5593, qst_f1: 0.5101, em: 0.0256, avg: 0.2925, multirc_loss: 0.7023
03/25 11:59:56 AM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.5479, qst_f1: 0.4955, em: 0.0233, avg: 0.2856, multirc_loss: 0.7022
03/25 12:00:06 PM: Evaluate: task multirc, batch 20 (152): ans_f1: 0.5415, qst_f1: 0.4837, em: 0.0224, avg: 0.2819, multirc_loss: 0.7024
03/25 12:00:21 PM: Evaluate: task multirc, batch 22 (152): ans_f1: 0.5317, qst_f1: 0.4736, em: 0.0210, avg: 0.2764, multirc_loss: 0.7024
03/25 12:00:41 PM: Evaluate: task multirc, batch 24 (152): ans_f1: 0.5338, qst_f1: 0.4756, em: 0.0196, avg: 0.2767, multirc_loss: 0.7022
03/25 12:00:59 PM: Evaluate: task multirc, batch 26 (152): ans_f1: 0.5317, qst_f1: 0.4729, em: 0.0185, avg: 0.2751, multirc_loss: 0.7023
03/25 12:01:16 PM: Evaluate: task multirc, batch 28 (152): ans_f1: 0.5443, qst_f1: 0.4854, em: 0.0174, avg: 0.2809, multirc_loss: 0.7016
03/25 12:01:33 PM: Evaluate: task multirc, batch 30 (152): ans_f1: 0.5568, qst_f1: 0.4976, em: 0.0162, avg: 0.2865, multirc_loss: 0.7005
03/25 12:01:53 PM: Evaluate: task multirc, batch 32 (152): ans_f1: 0.5523, qst_f1: 0.4943, em: 0.0151, avg: 0.2837, multirc_loss: 0.7018
03/25 12:02:11 PM: Evaluate: task multirc, batch 34 (152): ans_f1: 0.5406, qst_f1: 0.4780, em: 0.0189, avg: 0.2797, multirc_loss: 0.7021
03/25 12:02:31 PM: Evaluate: task multirc, batch 36 (152): ans_f1: 0.5367, qst_f1: 0.4793, em: 0.0135, avg: 0.2751, multirc_loss: 0.7023
03/25 12:02:46 PM: Evaluate: task multirc, batch 38 (152): ans_f1: 0.5374, qst_f1: 0.4801, em: 0.0128, avg: 0.2751, multirc_loss: 0.7027
03/25 12:03:04 PM: Evaluate: task multirc, batch 40 (152): ans_f1: 0.5390, qst_f1: 0.4825, em: 0.0124, avg: 0.2757, multirc_loss: 0.7024
03/25 12:03:22 PM: Evaluate: task multirc, batch 42 (152): ans_f1: 0.5352, qst_f1: 0.4732, em: 0.0118, avg: 0.2735, multirc_loss: 0.7022
03/25 12:03:38 PM: Evaluate: task multirc, batch 44 (152): ans_f1: 0.5305, qst_f1: 0.4640, em: 0.0113, avg: 0.2709, multirc_loss: 0.7020
03/25 12:03:55 PM: Evaluate: task multirc, batch 46 (152): ans_f1: 0.5324, qst_f1: 0.4638, em: 0.0107, avg: 0.2716, multirc_loss: 0.7016
03/25 12:04:12 PM: Evaluate: task multirc, batch 48 (152): ans_f1: 0.5363, qst_f1: 0.4675, em: 0.0102, avg: 0.2733, multirc_loss: 0.7017
03/25 12:04:30 PM: Evaluate: task multirc, batch 50 (152): ans_f1: 0.5355, qst_f1: 0.4687, em: 0.0097, avg: 0.2726, multirc_loss: 0.7024
03/25 12:04:46 PM: Evaluate: task multirc, batch 52 (152): ans_f1: 0.5281, qst_f1: 0.4631, em: 0.0093, avg: 0.2687, multirc_loss: 0.7038
03/25 12:05:04 PM: Evaluate: task multirc, batch 54 (152): ans_f1: 0.5241, qst_f1: 0.4618, em: 0.0090, avg: 0.2665, multirc_loss: 0.7045
03/25 12:05:22 PM: Evaluate: task multirc, batch 56 (152): ans_f1: 0.5255, qst_f1: 0.4633, em: 0.0087, avg: 0.2671, multirc_loss: 0.7048
03/25 12:05:40 PM: Evaluate: task multirc, batch 58 (152): ans_f1: 0.5257, qst_f1: 0.4644, em: 0.0084, avg: 0.2670, multirc_loss: 0.7054
03/25 12:05:59 PM: Evaluate: task multirc, batch 60 (152): ans_f1: 0.5289, qst_f1: 0.4723, em: 0.0160, avg: 0.2724, multirc_loss: 0.7048
03/25 12:06:17 PM: Evaluate: task multirc, batch 62 (152): ans_f1: 0.5294, qst_f1: 0.4708, em: 0.0154, avg: 0.2724, multirc_loss: 0.7046
03/25 12:06:31 PM: Evaluate: task multirc, batch 64 (152): ans_f1: 0.5299, qst_f1: 0.4691, em: 0.0149, avg: 0.2724, multirc_loss: 0.7042
03/25 12:06:50 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.5362, qst_f1: 0.4791, em: 0.0143, avg: 0.2753, multirc_loss: 0.7037
03/25 12:07:07 PM: Evaluate: task multirc, batch 68 (152): ans_f1: 0.5382, qst_f1: 0.4807, em: 0.0139, avg: 0.2760, multirc_loss: 0.7040
03/25 12:07:18 PM: Evaluate: task multirc, batch 69 (152): ans_f1: 0.5372, qst_f1: 0.4817, em: 0.0137, avg: 0.2755, multirc_loss: 0.7045
03/25 12:07:28 PM: Evaluate: task multirc, batch 70 (152): ans_f1: 0.5379, qst_f1: 0.4829, em: 0.0135, avg: 0.2757, multirc_loss: 0.7046
03/25 12:07:42 PM: Evaluate: task multirc, batch 72 (152): ans_f1: 0.5399, qst_f1: 0.4851, em: 0.0132, avg: 0.2766, multirc_loss: 0.7050
03/25 12:07:59 PM: Evaluate: task multirc, batch 74 (152): ans_f1: 0.5385, qst_f1: 0.4839, em: 0.0129, avg: 0.2757, multirc_loss: 0.7057
03/25 12:08:16 PM: Evaluate: task multirc, batch 76 (152): ans_f1: 0.5392, qst_f1: 0.4865, em: 0.0126, avg: 0.2759, multirc_loss: 0.7061
03/25 12:08:30 PM: Evaluate: task multirc, batch 78 (152): ans_f1: 0.5394, qst_f1: 0.4879, em: 0.0123, avg: 0.2758, multirc_loss: 0.7063
03/25 12:08:46 PM: Evaluate: task multirc, batch 80 (152): ans_f1: 0.5395, qst_f1: 0.4876, em: 0.0120, avg: 0.2757, multirc_loss: 0.7065
03/25 12:09:00 PM: Evaluate: task multirc, batch 82 (152): ans_f1: 0.5387, qst_f1: 0.4858, em: 0.0117, avg: 0.2752, multirc_loss: 0.7062
03/25 12:09:15 PM: Evaluate: task multirc, batch 84 (152): ans_f1: 0.5369, qst_f1: 0.4824, em: 0.0113, avg: 0.2741, multirc_loss: 0.7060
03/25 12:09:34 PM: Evaluate: task multirc, batch 86 (152): ans_f1: 0.5359, qst_f1: 0.4815, em: 0.0112, avg: 0.2735, multirc_loss: 0.7062
03/25 12:09:54 PM: Evaluate: task multirc, batch 88 (152): ans_f1: 0.5393, qst_f1: 0.4850, em: 0.0127, avg: 0.2760, multirc_loss: 0.7058
03/25 12:10:04 PM: Evaluate: task multirc, batch 89 (152): ans_f1: 0.5427, qst_f1: 0.4883, em: 0.0108, avg: 0.2767, multirc_loss: 0.7055
03/25 12:10:19 PM: Evaluate: task multirc, batch 91 (152): ans_f1: 0.5430, qst_f1: 0.4883, em: 0.0106, avg: 0.2768, multirc_loss: 0.7054
03/25 12:10:36 PM: Evaluate: task multirc, batch 93 (152): ans_f1: 0.5415, qst_f1: 0.4878, em: 0.0104, avg: 0.2759, multirc_loss: 0.7057
03/25 12:10:55 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.5431, qst_f1: 0.4899, em: 0.0102, avg: 0.2766, multirc_loss: 0.7058
03/25 12:11:13 PM: Evaluate: task multirc, batch 97 (152): ans_f1: 0.5462, qst_f1: 0.4935, em: 0.0116, avg: 0.2789, multirc_loss: 0.7056
03/25 12:11:32 PM: Evaluate: task multirc, batch 99 (152): ans_f1: 0.5482, qst_f1: 0.4963, em: 0.0145, avg: 0.2813, multirc_loss: 0.7052
03/25 12:11:46 PM: Evaluate: task multirc, batch 101 (152): ans_f1: 0.5522, qst_f1: 0.5011, em: 0.0126, avg: 0.2824, multirc_loss: 0.7049
03/25 12:11:58 PM: Evaluate: task multirc, batch 103 (152): ans_f1: 0.5466, qst_f1: 0.4858, em: 0.0122, avg: 0.2794, multirc_loss: 0.7050
03/25 12:12:14 PM: Evaluate: task multirc, batch 105 (152): ans_f1: 0.5454, qst_f1: 0.4854, em: 0.0120, avg: 0.2787, multirc_loss: 0.7054
03/25 12:12:30 PM: Evaluate: task multirc, batch 107 (152): ans_f1: 0.5457, qst_f1: 0.4863, em: 0.0118, avg: 0.2787, multirc_loss: 0.7056
03/25 12:12:46 PM: Evaluate: task multirc, batch 109 (152): ans_f1: 0.5491, qst_f1: 0.4930, em: 0.0143, avg: 0.2817, multirc_loss: 0.7053
03/25 12:12:59 PM: Evaluate: task multirc, batch 111 (152): ans_f1: 0.5520, qst_f1: 0.4964, em: 0.0140, avg: 0.2830, multirc_loss: 0.7052
03/25 12:13:16 PM: Evaluate: task multirc, batch 113 (152): ans_f1: 0.5546, qst_f1: 0.4998, em: 0.0137, avg: 0.2842, multirc_loss: 0.7050
03/25 12:13:34 PM: Evaluate: task multirc, batch 115 (152): ans_f1: 0.5540, qst_f1: 0.5009, em: 0.0148, avg: 0.2844, multirc_loss: 0.7049
03/25 12:13:51 PM: Evaluate: task multirc, batch 117 (152): ans_f1: 0.5535, qst_f1: 0.5004, em: 0.0158, avg: 0.2847, multirc_loss: 0.7049
03/25 12:14:08 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.5540, qst_f1: 0.5021, em: 0.0156, avg: 0.2848, multirc_loss: 0.7048
03/25 12:14:26 PM: Evaluate: task multirc, batch 121 (152): ans_f1: 0.5542, qst_f1: 0.5029, em: 0.0141, avg: 0.2842, multirc_loss: 0.7049
03/25 12:14:44 PM: Evaluate: task multirc, batch 123 (152): ans_f1: 0.5534, qst_f1: 0.5019, em: 0.0139, avg: 0.2837, multirc_loss: 0.7048
03/25 12:15:03 PM: Evaluate: task multirc, batch 125 (152): ans_f1: 0.5545, qst_f1: 0.5027, em: 0.0138, avg: 0.2841, multirc_loss: 0.7047
03/25 12:15:21 PM: Evaluate: task multirc, batch 127 (152): ans_f1: 0.5535, qst_f1: 0.5023, em: 0.0136, avg: 0.2836, multirc_loss: 0.7048
03/25 12:15:33 PM: Evaluate: task multirc, batch 129 (152): ans_f1: 0.5534, qst_f1: 0.5028, em: 0.0134, avg: 0.2834, multirc_loss: 0.7052
03/25 12:15:50 PM: Evaluate: task multirc, batch 131 (152): ans_f1: 0.5541, qst_f1: 0.5037, em: 0.0132, avg: 0.2837, multirc_loss: 0.7052
03/25 12:16:06 PM: Evaluate: task multirc, batch 133 (152): ans_f1: 0.5534, qst_f1: 0.5021, em: 0.0142, avg: 0.2838, multirc_loss: 0.7052
03/25 12:16:24 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.5541, qst_f1: 0.5030, em: 0.0128, avg: 0.2835, multirc_loss: 0.7051
03/25 12:16:43 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.5555, qst_f1: 0.5046, em: 0.0126, avg: 0.2841, multirc_loss: 0.7051
03/25 12:16:57 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.5542, qst_f1: 0.5041, em: 0.0125, avg: 0.2833, multirc_loss: 0.7052
03/25 12:17:10 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.5536, qst_f1: 0.5040, em: 0.0124, avg: 0.2830, multirc_loss: 0.7052
03/25 12:17:26 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.5527, qst_f1: 0.5038, em: 0.0122, avg: 0.2825, multirc_loss: 0.7054
03/25 12:17:42 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.5519, qst_f1: 0.5037, em: 0.0121, avg: 0.2820, multirc_loss: 0.7055
03/25 12:17:58 PM: Evaluate: task multirc, batch 147 (152): ans_f1: 0.5518, qst_f1: 0.5030, em: 0.0119, avg: 0.2818, multirc_loss: 0.7055
03/25 12:18:15 PM: Evaluate: task multirc, batch 149 (152): ans_f1: 0.5520, qst_f1: 0.5035, em: 0.0117, avg: 0.2819, multirc_loss: 0.7055
03/25 12:18:33 PM: Evaluate: task multirc, batch 151 (152): ans_f1: 0.5543, qst_f1: 0.5059, em: 0.0127, avg: 0.2835, multirc_loss: 0.7052
03/25 12:18:37 PM: Best result seen so far for multirc.
03/25 12:18:37 PM: Best result seen so far for micro.
03/25 12:18:37 PM: Best result seen so far for macro.
03/25 12:18:37 PM: Updating LR scheduler:
03/25 12:18:37 PM: 	Best result seen so far for macro_avg: 0.284
03/25 12:18:37 PM: 	# validation passes without improvement: 0
03/25 12:18:37 PM: multirc_loss: training: 0.653700 validation: 0.704844
03/25 12:18:37 PM: macro_avg: validation: 0.284010
03/25 12:18:37 PM: micro_avg: validation: 0.284010
03/25 12:18:37 PM: multirc_ans_f1: training: 0.857143 validation: 0.555429
03/25 12:18:37 PM: multirc_qst_f1: training: 0.800000 validation: 0.507320
03/25 12:18:37 PM: multirc_em: training: 0.800000 validation: 0.012592
03/25 12:18:37 PM: multirc_avg: training: 0.828571 validation: 0.284010
03/25 12:18:37 PM: Global learning rate: 5e-06
03/25 12:18:37 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 12:19:38 PM: Update 2: task multirc, steps since last val 1 (total steps = 2): ans_f1: 0.6190, qst_f1: 0.4222, em: 0.5000, avg: 0.5595, multirc_loss: 0.6967
03/25 12:19:38 PM: ***** Step 2 / Validation 2 *****
03/25 12:19:38 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 12:19:38 PM: Validating...
03/25 12:19:55 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.6136, qst_f1: 0.6061, em: 0.0000, avg: 0.3068, multirc_loss: 0.7088
03/25 12:20:11 PM: Evaluate: task multirc, batch 4 (152): ans_f1: 0.6631, qst_f1: 0.6384, em: 0.0000, avg: 0.3316, multirc_loss: 0.7036
03/25 12:20:28 PM: Evaluate: task multirc, batch 6 (152): ans_f1: 0.6523, qst_f1: 0.6406, em: 0.0000, avg: 0.3262, multirc_loss: 0.7017
03/25 12:20:47 PM: Evaluate: task multirc, batch 8 (152): ans_f1: 0.6431, qst_f1: 0.6241, em: 0.0000, avg: 0.3215, multirc_loss: 0.7023
03/25 12:21:04 PM: Evaluate: task multirc, batch 10 (152): ans_f1: 0.6360, qst_f1: 0.6156, em: 0.0000, avg: 0.3180, multirc_loss: 0.7035
03/25 12:21:19 PM: Evaluate: task multirc, batch 12 (152): ans_f1: 0.6375, qst_f1: 0.6186, em: 0.0000, avg: 0.3188, multirc_loss: 0.7047
03/25 12:21:33 PM: Evaluate: task multirc, batch 14 (152): ans_f1: 0.6375, qst_f1: 0.6150, em: 0.0000, avg: 0.3188, multirc_loss: 0.7028
03/25 12:21:48 PM: Evaluate: task multirc, batch 16 (152): ans_f1: 0.6227, qst_f1: 0.5830, em: 0.0000, avg: 0.3113, multirc_loss: 0.7026
03/25 12:21:58 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.6208, qst_f1: 0.5909, em: 0.0085, avg: 0.3147, multirc_loss: 0.7030
03/25 12:22:15 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.6084, qst_f1: 0.5788, em: 0.0078, avg: 0.3081, multirc_loss: 0.7037
03/25 12:22:31 PM: Evaluate: task multirc, batch 21 (152): ans_f1: 0.5964, qst_f1: 0.5685, em: 0.0072, avg: 0.3018, multirc_loss: 0.7039
03/25 12:22:47 PM: Evaluate: task multirc, batch 23 (152): ans_f1: 0.5934, qst_f1: 0.5649, em: 0.0068, avg: 0.3001, multirc_loss: 0.7044
03/25 12:23:04 PM: Evaluate: task multirc, batch 25 (152): ans_f1: 0.5872, qst_f1: 0.5634, em: 0.0127, avg: 0.2999, multirc_loss: 0.7048
03/25 12:23:15 PM: Evaluate: task multirc, batch 26 (152): ans_f1: 0.5827, qst_f1: 0.5546, em: 0.0062, avg: 0.2944, multirc_loss: 0.7047
03/25 12:23:32 PM: Evaluate: task multirc, batch 28 (152): ans_f1: 0.5895, qst_f1: 0.5617, em: 0.0058, avg: 0.2977, multirc_loss: 0.7040
03/25 12:23:49 PM: Evaluate: task multirc, batch 30 (152): ans_f1: 0.5975, qst_f1: 0.5686, em: 0.0054, avg: 0.3015, multirc_loss: 0.7028
03/25 12:24:09 PM: Evaluate: task multirc, batch 32 (152): ans_f1: 0.5915, qst_f1: 0.5610, em: 0.0050, avg: 0.2983, multirc_loss: 0.7044
03/25 12:24:25 PM: Evaluate: task multirc, batch 34 (152): ans_f1: 0.5796, qst_f1: 0.5469, em: 0.0047, avg: 0.2921, multirc_loss: 0.7051
03/25 12:24:40 PM: Evaluate: task multirc, batch 36 (152): ans_f1: 0.5724, qst_f1: 0.5432, em: 0.0045, avg: 0.2884, multirc_loss: 0.7056
03/25 12:25:19 PM: Evaluate: task multirc, batch 37 (152): ans_f1: 0.5734, qst_f1: 0.5446, em: 0.0044, avg: 0.2889, multirc_loss: 0.7058
03/25 12:26:04 PM: Evaluate: task multirc, batch 38 (152): ans_f1: 0.5691, qst_f1: 0.5393, em: 0.0043, avg: 0.2867, multirc_loss: 0.7061
03/25 12:27:20 PM: Evaluate: task multirc, batch 39 (152): ans_f1: 0.5671, qst_f1: 0.5385, em: 0.0042, avg: 0.2856, multirc_loss: 0.7061
03/25 12:27:58 PM: Evaluate: task multirc, batch 40 (152): ans_f1: 0.5691, qst_f1: 0.5405, em: 0.0041, avg: 0.2866, multirc_loss: 0.7057
03/25 12:28:15 PM: Evaluate: task multirc, batch 42 (152): ans_f1: 0.5676, qst_f1: 0.5356, em: 0.0039, avg: 0.2857, multirc_loss: 0.7056
03/25 12:28:32 PM: Evaluate: task multirc, batch 44 (152): ans_f1: 0.5652, qst_f1: 0.5300, em: 0.0075, avg: 0.2864, multirc_loss: 0.7053
03/25 12:28:48 PM: Evaluate: task multirc, batch 46 (152): ans_f1: 0.5670, qst_f1: 0.5309, em: 0.0071, avg: 0.2871, multirc_loss: 0.7047
03/25 12:29:07 PM: Evaluate: task multirc, batch 48 (152): ans_f1: 0.5690, qst_f1: 0.5315, em: 0.0068, avg: 0.2879, multirc_loss: 0.7049
03/25 12:29:24 PM: Evaluate: task multirc, batch 50 (152): ans_f1: 0.5667, qst_f1: 0.5288, em: 0.0065, avg: 0.2866, multirc_loss: 0.7057
03/25 12:29:40 PM: Evaluate: task multirc, batch 52 (152): ans_f1: 0.5592, qst_f1: 0.5221, em: 0.0062, avg: 0.2827, multirc_loss: 0.7075
03/25 12:29:58 PM: Evaluate: task multirc, batch 54 (152): ans_f1: 0.5555, qst_f1: 0.5196, em: 0.0060, avg: 0.2807, multirc_loss: 0.7084
03/25 12:30:16 PM: Evaluate: task multirc, batch 56 (152): ans_f1: 0.5555, qst_f1: 0.5193, em: 0.0058, avg: 0.2806, multirc_loss: 0.7087
03/25 12:30:34 PM: Evaluate: task multirc, batch 58 (152): ans_f1: 0.5545, qst_f1: 0.5183, em: 0.0056, avg: 0.2800, multirc_loss: 0.7095
03/25 12:30:53 PM: Evaluate: task multirc, batch 60 (152): ans_f1: 0.5581, qst_f1: 0.5268, em: 0.0133, avg: 0.2857, multirc_loss: 0.7088
03/25 12:31:10 PM: Evaluate: task multirc, batch 62 (152): ans_f1: 0.5561, qst_f1: 0.5221, em: 0.0103, avg: 0.2832, multirc_loss: 0.7086
03/25 12:31:24 PM: Evaluate: task multirc, batch 64 (152): ans_f1: 0.5553, qst_f1: 0.5182, em: 0.0099, avg: 0.2826, multirc_loss: 0.7082
03/25 12:31:42 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.5602, qst_f1: 0.5263, em: 0.0095, avg: 0.2849, multirc_loss: 0.7076
03/25 12:31:59 PM: Evaluate: task multirc, batch 68 (152): ans_f1: 0.5623, qst_f1: 0.5274, em: 0.0093, avg: 0.2858, multirc_loss: 0.7078
03/25 12:32:19 PM: Evaluate: task multirc, batch 70 (152): ans_f1: 0.5613, qst_f1: 0.5285, em: 0.0090, avg: 0.2852, multirc_loss: 0.7085
03/25 12:32:32 PM: Evaluate: task multirc, batch 72 (152): ans_f1: 0.5623, qst_f1: 0.5292, em: 0.0088, avg: 0.2855, multirc_loss: 0.7089
03/25 12:32:43 PM: Evaluate: task multirc, batch 73 (152): ans_f1: 0.5625, qst_f1: 0.5301, em: 0.0109, avg: 0.2867, multirc_loss: 0.7091
03/25 12:33:01 PM: Evaluate: task multirc, batch 75 (152): ans_f1: 0.5606, qst_f1: 0.5284, em: 0.0085, avg: 0.2846, multirc_loss: 0.7098
03/25 12:33:17 PM: Evaluate: task multirc, batch 77 (152): ans_f1: 0.5602, qst_f1: 0.5284, em: 0.0083, avg: 0.2843, multirc_loss: 0.7105
03/25 12:33:34 PM: Evaluate: task multirc, batch 79 (152): ans_f1: 0.5597, qst_f1: 0.5280, em: 0.0101, avg: 0.2849, multirc_loss: 0.7106
03/25 12:33:54 PM: Evaluate: task multirc, batch 81 (152): ans_f1: 0.5604, qst_f1: 0.5276, em: 0.0079, avg: 0.2841, multirc_loss: 0.7105
03/25 12:34:12 PM: Evaluate: task multirc, batch 83 (152): ans_f1: 0.5605, qst_f1: 0.5271, em: 0.0076, avg: 0.2841, multirc_loss: 0.7102
03/25 12:34:30 PM: Evaluate: task multirc, batch 85 (152): ans_f1: 0.5601, qst_f1: 0.5273, em: 0.0094, avg: 0.2847, multirc_loss: 0.7102
03/25 12:34:49 PM: Evaluate: task multirc, batch 87 (152): ans_f1: 0.5598, qst_f1: 0.5282, em: 0.0092, avg: 0.2845, multirc_loss: 0.7103
03/25 12:35:08 PM: Evaluate: task multirc, batch 89 (152): ans_f1: 0.5668, qst_f1: 0.5344, em: 0.0090, avg: 0.2879, multirc_loss: 0.7095
03/25 12:35:20 PM: Evaluate: task multirc, batch 91 (152): ans_f1: 0.5683, qst_f1: 0.5350, em: 0.0088, avg: 0.2886, multirc_loss: 0.7093
03/25 12:35:36 PM: Evaluate: task multirc, batch 93 (152): ans_f1: 0.5663, qst_f1: 0.5335, em: 0.0086, avg: 0.2875, multirc_loss: 0.7097
03/25 12:35:56 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.5673, qst_f1: 0.5346, em: 0.0085, avg: 0.2879, multirc_loss: 0.7098
03/25 12:36:14 PM: Evaluate: task multirc, batch 97 (152): ans_f1: 0.5696, qst_f1: 0.5373, em: 0.0099, avg: 0.2898, multirc_loss: 0.7095
03/25 12:36:33 PM: Evaluate: task multirc, batch 99 (152): ans_f1: 0.5720, qst_f1: 0.5406, em: 0.0096, avg: 0.2908, multirc_loss: 0.7091
03/25 12:36:49 PM: Evaluate: task multirc, batch 101 (152): ans_f1: 0.5761, qst_f1: 0.5455, em: 0.0094, avg: 0.2927, multirc_loss: 0.7086
03/25 12:37:02 PM: Evaluate: task multirc, batch 103 (152): ans_f1: 0.5723, qst_f1: 0.5315, em: 0.0091, avg: 0.2907, multirc_loss: 0.7086
03/25 12:37:19 PM: Evaluate: task multirc, batch 105 (152): ans_f1: 0.5710, qst_f1: 0.5310, em: 0.0090, avg: 0.2900, multirc_loss: 0.7091
03/25 12:37:34 PM: Evaluate: task multirc, batch 107 (152): ans_f1: 0.5708, qst_f1: 0.5311, em: 0.0088, avg: 0.2898, multirc_loss: 0.7093
03/25 12:37:50 PM: Evaluate: task multirc, batch 109 (152): ans_f1: 0.5734, qst_f1: 0.5358, em: 0.0086, avg: 0.2910, multirc_loss: 0.7089
03/25 12:38:03 PM: Evaluate: task multirc, batch 111 (152): ans_f1: 0.5762, qst_f1: 0.5389, em: 0.0084, avg: 0.2923, multirc_loss: 0.7087
03/25 12:38:19 PM: Evaluate: task multirc, batch 113 (152): ans_f1: 0.5784, qst_f1: 0.5417, em: 0.0082, avg: 0.2933, multirc_loss: 0.7085
03/25 12:38:38 PM: Evaluate: task multirc, batch 115 (152): ans_f1: 0.5771, qst_f1: 0.5417, em: 0.0094, avg: 0.2932, multirc_loss: 0.7084
03/25 12:38:53 PM: Evaluate: task multirc, batch 117 (152): ans_f1: 0.5766, qst_f1: 0.5414, em: 0.0092, avg: 0.2929, multirc_loss: 0.7085
03/25 12:39:11 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.5773, qst_f1: 0.5429, em: 0.0104, avg: 0.2939, multirc_loss: 0.7084
03/25 12:39:31 PM: Evaluate: task multirc, batch 121 (152): ans_f1: 0.5773, qst_f1: 0.5433, em: 0.0090, avg: 0.2931, multirc_loss: 0.7085
03/25 12:39:43 PM: Evaluate: task multirc, batch 122 (152): ans_f1: 0.5766, qst_f1: 0.5425, em: 0.0089, avg: 0.2928, multirc_loss: 0.7085
03/25 12:39:53 PM: Evaluate: task multirc, batch 123 (152): ans_f1: 0.5769, qst_f1: 0.5429, em: 0.0089, avg: 0.2929, multirc_loss: 0.7085
03/25 12:40:04 PM: Evaluate: task multirc, batch 124 (152): ans_f1: 0.5771, qst_f1: 0.5431, em: 0.0088, avg: 0.2930, multirc_loss: 0.7084
03/25 12:40:17 PM: Evaluate: task multirc, batch 125 (152): ans_f1: 0.5774, qst_f1: 0.5432, em: 0.0088, avg: 0.2931, multirc_loss: 0.7084
03/25 12:40:30 PM: Evaluate: task multirc, batch 126 (152): ans_f1: 0.5766, qst_f1: 0.5426, em: 0.0087, avg: 0.2927, multirc_loss: 0.7085
03/25 12:40:47 PM: Evaluate: task multirc, batch 128 (152): ans_f1: 0.5756, qst_f1: 0.5420, em: 0.0086, avg: 0.2921, multirc_loss: 0.7087
03/25 12:41:04 PM: Evaluate: task multirc, batch 130 (152): ans_f1: 0.5767, qst_f1: 0.5438, em: 0.0097, avg: 0.2932, multirc_loss: 0.7088
03/25 12:41:22 PM: Evaluate: task multirc, batch 132 (152): ans_f1: 0.5761, qst_f1: 0.5430, em: 0.0095, avg: 0.2928, multirc_loss: 0.7089
03/25 12:41:40 PM: Evaluate: task multirc, batch 134 (152): ans_f1: 0.5767, qst_f1: 0.5430, em: 0.0082, avg: 0.2924, multirc_loss: 0.7088
03/25 12:41:51 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.5767, qst_f1: 0.5430, em: 0.0081, avg: 0.2924, multirc_loss: 0.7088
03/25 12:42:02 PM: Evaluate: task multirc, batch 136 (152): ans_f1: 0.5775, qst_f1: 0.5438, em: 0.0081, avg: 0.2928, multirc_loss: 0.7088
03/25 12:42:15 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.5773, qst_f1: 0.5436, em: 0.0080, avg: 0.2927, multirc_loss: 0.7088
03/25 12:42:32 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.5762, qst_f1: 0.5430, em: 0.0080, avg: 0.2921, multirc_loss: 0.7090
03/25 12:42:46 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.5754, qst_f1: 0.5427, em: 0.0079, avg: 0.2916, multirc_loss: 0.7090
03/25 12:43:04 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.5739, qst_f1: 0.5418, em: 0.0078, avg: 0.2908, multirc_loss: 0.7092
03/25 12:43:23 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.5729, qst_f1: 0.5412, em: 0.0077, avg: 0.2903, multirc_loss: 0.7094
03/25 12:43:41 PM: Evaluate: task multirc, batch 147 (152): ans_f1: 0.5734, qst_f1: 0.5416, em: 0.0076, avg: 0.2905, multirc_loss: 0.7094
03/25 12:43:53 PM: Evaluate: task multirc, batch 148 (152): ans_f1: 0.5733, qst_f1: 0.5411, em: 0.0075, avg: 0.2904, multirc_loss: 0.7094
03/25 12:44:03 PM: Evaluate: task multirc, batch 149 (152): ans_f1: 0.5737, qst_f1: 0.5422, em: 0.0075, avg: 0.2906, multirc_loss: 0.7093
03/25 12:44:22 PM: Evaluate: task multirc, batch 151 (152): ans_f1: 0.5762, qst_f1: 0.5446, em: 0.0074, avg: 0.2918, multirc_loss: 0.7090
03/25 12:44:26 PM: Best result seen so far for multirc.
03/25 12:44:26 PM: Best result seen so far for micro.
03/25 12:44:26 PM: Best result seen so far for macro.
03/25 12:44:26 PM: Updating LR scheduler:
03/25 12:44:26 PM: 	Best result seen so far for macro_avg: 0.292
03/25 12:44:26 PM: 	# validation passes without improvement: 0
03/25 12:44:26 PM: multirc_loss: training: 0.696683 validation: 0.708597
03/25 12:44:26 PM: macro_avg: validation: 0.292264
03/25 12:44:26 PM: micro_avg: validation: 0.292264
03/25 12:44:26 PM: multirc_ans_f1: training: 0.619048 validation: 0.577183
03/25 12:44:26 PM: multirc_qst_f1: training: 0.422222 validation: 0.545848
03/25 12:44:26 PM: multirc_em: training: 0.500000 validation: 0.007345
03/25 12:44:26 PM: multirc_avg: training: 0.559524 validation: 0.292264
03/25 12:44:26 PM: Global learning rate: 5e-06
03/25 12:44:26 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 12:45:21 PM: Update 3: task multirc, steps since last val 1 (total steps = 3): ans_f1: 0.4286, qst_f1: 0.1875, em: 0.5000, avg: 0.4643, multirc_loss: 0.7161
03/25 12:45:21 PM: ***** Step 3 / Validation 3 *****
03/25 12:45:21 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 12:45:21 PM: Validating...
03/25 12:45:32 PM: Evaluate: task multirc, batch 1 (152): ans_f1: 0.6222, qst_f1: 0.6117, em: 0.0000, avg: 0.3111, multirc_loss: 0.7026
03/25 12:45:44 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.6154, qst_f1: 0.6087, em: 0.0000, avg: 0.3077, multirc_loss: 0.7115
03/25 12:45:56 PM: Evaluate: task multirc, batch 3 (152): ans_f1: 0.6232, qst_f1: 0.6228, em: 0.0000, avg: 0.3116, multirc_loss: 0.7105
03/25 12:46:13 PM: Evaluate: task multirc, batch 5 (152): ans_f1: 0.6695, qst_f1: 0.6602, em: 0.0270, avg: 0.3482, multirc_loss: 0.7027
03/25 12:46:24 PM: Evaluate: task multirc, batch 6 (152): ans_f1: 0.6620, qst_f1: 0.6515, em: 0.0227, avg: 0.3423, multirc_loss: 0.7024
03/25 12:46:35 PM: Evaluate: task multirc, batch 7 (152): ans_f1: 0.6586, qst_f1: 0.6434, em: 0.0189, avg: 0.3387, multirc_loss: 0.7018
03/25 12:46:46 PM: Evaluate: task multirc, batch 8 (152): ans_f1: 0.6614, qst_f1: 0.6532, em: 0.0323, avg: 0.3468, multirc_loss: 0.7027
03/25 12:46:58 PM: Evaluate: task multirc, batch 9 (152): ans_f1: 0.6493, qst_f1: 0.6441, em: 0.0441, avg: 0.3467, multirc_loss: 0.7057
03/25 12:47:16 PM: Evaluate: task multirc, batch 11 (152): ans_f1: 0.6576, qst_f1: 0.6472, em: 0.0253, avg: 0.3415, multirc_loss: 0.7031
03/25 12:47:33 PM: Evaluate: task multirc, batch 13 (152): ans_f1: 0.6569, qst_f1: 0.6470, em: 0.0319, avg: 0.3444, multirc_loss: 0.7039
03/25 12:47:46 PM: Evaluate: task multirc, batch 15 (152): ans_f1: 0.6618, qst_f1: 0.6373, em: 0.0381, avg: 0.3500, multirc_loss: 0.7019
03/25 12:48:05 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.6519, qst_f1: 0.6341, em: 0.0342, avg: 0.3431, multirc_loss: 0.7034
03/25 12:48:24 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.6375, qst_f1: 0.6240, em: 0.0310, avg: 0.3343, multirc_loss: 0.7046
03/25 12:48:35 PM: Evaluate: task multirc, batch 20 (152): ans_f1: 0.6330, qst_f1: 0.6215, em: 0.0299, avg: 0.3314, multirc_loss: 0.7050
03/25 12:48:51 PM: Evaluate: task multirc, batch 22 (152): ans_f1: 0.6311, qst_f1: 0.6174, em: 0.0280, avg: 0.3296, multirc_loss: 0.7052
03/25 12:49:08 PM: Evaluate: task multirc, batch 24 (152): ans_f1: 0.6225, qst_f1: 0.6089, em: 0.0261, avg: 0.3243, multirc_loss: 0.7055
03/25 12:49:24 PM: Evaluate: task multirc, batch 26 (152): ans_f1: 0.6134, qst_f1: 0.5983, em: 0.0247, avg: 0.3190, multirc_loss: 0.7062
03/25 12:49:41 PM: Evaluate: task multirc, batch 28 (152): ans_f1: 0.6175, qst_f1: 0.6030, em: 0.0233, avg: 0.3204, multirc_loss: 0.7054
03/25 12:49:58 PM: Evaluate: task multirc, batch 30 (152): ans_f1: 0.6231, qst_f1: 0.6069, em: 0.0216, avg: 0.3224, multirc_loss: 0.7042
03/25 12:50:17 PM: Evaluate: task multirc, batch 32 (152): ans_f1: 0.6158, qst_f1: 0.5967, em: 0.0201, avg: 0.3180, multirc_loss: 0.7059
03/25 12:50:32 PM: Evaluate: task multirc, batch 34 (152): ans_f1: 0.6050, qst_f1: 0.5839, em: 0.0189, avg: 0.3119, multirc_loss: 0.7069
03/25 12:50:48 PM: Evaluate: task multirc, batch 36 (152): ans_f1: 0.5972, qst_f1: 0.5794, em: 0.0180, avg: 0.3076, multirc_loss: 0.7076
03/25 12:51:03 PM: Evaluate: task multirc, batch 38 (152): ans_f1: 0.5928, qst_f1: 0.5736, em: 0.0171, avg: 0.3049, multirc_loss: 0.7080
03/25 12:51:20 PM: Evaluate: task multirc, batch 40 (152): ans_f1: 0.5923, qst_f1: 0.5741, em: 0.0165, avg: 0.3044, multirc_loss: 0.7077
03/25 12:51:35 PM: Evaluate: task multirc, batch 42 (152): ans_f1: 0.5909, qst_f1: 0.5722, em: 0.0157, avg: 0.3033, multirc_loss: 0.7076
03/25 12:51:53 PM: Evaluate: task multirc, batch 44 (152): ans_f1: 0.5906, qst_f1: 0.5705, em: 0.0189, avg: 0.3047, multirc_loss: 0.7072
03/25 12:52:08 PM: Evaluate: task multirc, batch 46 (152): ans_f1: 0.5920, qst_f1: 0.5709, em: 0.0179, avg: 0.3049, multirc_loss: 0.7065
03/25 12:52:25 PM: Evaluate: task multirc, batch 48 (152): ans_f1: 0.5923, qst_f1: 0.5695, em: 0.0170, avg: 0.3047, multirc_loss: 0.7067
03/25 12:52:42 PM: Evaluate: task multirc, batch 50 (152): ans_f1: 0.5892, qst_f1: 0.5649, em: 0.0161, avg: 0.3026, multirc_loss: 0.7077
03/25 12:52:57 PM: Evaluate: task multirc, batch 52 (152): ans_f1: 0.5812, qst_f1: 0.5567, em: 0.0155, avg: 0.2983, multirc_loss: 0.7097
03/25 12:53:14 PM: Evaluate: task multirc, batch 54 (152): ans_f1: 0.5768, qst_f1: 0.5529, em: 0.0149, avg: 0.2959, multirc_loss: 0.7107
03/25 12:53:31 PM: Evaluate: task multirc, batch 56 (152): ans_f1: 0.5758, qst_f1: 0.5516, em: 0.0145, avg: 0.2951, multirc_loss: 0.7111
03/25 12:53:49 PM: Evaluate: task multirc, batch 58 (152): ans_f1: 0.5742, qst_f1: 0.5495, em: 0.0139, avg: 0.2940, multirc_loss: 0.7119
03/25 12:54:07 PM: Evaluate: task multirc, batch 60 (152): ans_f1: 0.5764, qst_f1: 0.5556, em: 0.0186, avg: 0.2975, multirc_loss: 0.7112
03/25 12:54:17 PM: Evaluate: task multirc, batch 61 (152): ans_f1: 0.5743, qst_f1: 0.5519, em: 0.0157, avg: 0.2950, multirc_loss: 0.7112
03/25 12:54:32 PM: Evaluate: task multirc, batch 63 (152): ans_f1: 0.5740, qst_f1: 0.5505, em: 0.0152, avg: 0.2946, multirc_loss: 0.7108
03/25 12:54:49 PM: Evaluate: task multirc, batch 65 (152): ans_f1: 0.5773, qst_f1: 0.5532, em: 0.0169, avg: 0.2971, multirc_loss: 0.7097
03/25 12:54:59 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.5778, qst_f1: 0.5539, em: 0.0143, avg: 0.2960, multirc_loss: 0.7098
03/25 12:55:17 PM: Evaluate: task multirc, batch 68 (152): ans_f1: 0.5797, qst_f1: 0.5545, em: 0.0139, avg: 0.2968, multirc_loss: 0.7100
03/25 12:55:28 PM: Evaluate: task multirc, batch 69 (152): ans_f1: 0.5783, qst_f1: 0.5546, em: 0.0137, avg: 0.2960, multirc_loss: 0.7106
03/25 12:55:46 PM: Evaluate: task multirc, batch 71 (152): ans_f1: 0.5782, qst_f1: 0.5546, em: 0.0133, avg: 0.2958, multirc_loss: 0.7110
03/25 12:56:02 PM: Evaluate: task multirc, batch 73 (152): ans_f1: 0.5791, qst_f1: 0.5558, em: 0.0153, avg: 0.2972, multirc_loss: 0.7114
03/25 12:56:19 PM: Evaluate: task multirc, batch 75 (152): ans_f1: 0.5769, qst_f1: 0.5536, em: 0.0127, avg: 0.2948, multirc_loss: 0.7122
03/25 12:56:35 PM: Evaluate: task multirc, batch 77 (152): ans_f1: 0.5761, qst_f1: 0.5530, em: 0.0125, avg: 0.2943, multirc_loss: 0.7128
03/25 12:56:53 PM: Evaluate: task multirc, batch 79 (152): ans_f1: 0.5756, qst_f1: 0.5532, em: 0.0141, avg: 0.2949, multirc_loss: 0.7130
03/25 12:57:09 PM: Evaluate: task multirc, batch 81 (152): ans_f1: 0.5761, qst_f1: 0.5525, em: 0.0118, avg: 0.2940, multirc_loss: 0.7129
03/25 12:57:24 PM: Evaluate: task multirc, batch 83 (152): ans_f1: 0.5774, qst_f1: 0.5535, em: 0.0115, avg: 0.2944, multirc_loss: 0.7126
03/25 12:57:43 PM: Evaluate: task multirc, batch 85 (152): ans_f1: 0.5763, qst_f1: 0.5523, em: 0.0112, avg: 0.2938, multirc_loss: 0.7126
03/25 12:57:54 PM: Evaluate: task multirc, batch 86 (152): ans_f1: 0.5752, qst_f1: 0.5516, em: 0.0112, avg: 0.2932, multirc_loss: 0.7128
03/25 12:58:14 PM: Evaluate: task multirc, batch 88 (152): ans_f1: 0.5786, qst_f1: 0.5554, em: 0.0127, avg: 0.2957, multirc_loss: 0.7123
03/25 12:58:31 PM: Evaluate: task multirc, batch 90 (152): ans_f1: 0.5830, qst_f1: 0.5589, em: 0.0107, avg: 0.2968, multirc_loss: 0.7116
03/25 12:58:45 PM: Evaluate: task multirc, batch 92 (152): ans_f1: 0.5821, qst_f1: 0.5577, em: 0.0105, avg: 0.2963, multirc_loss: 0.7117
03/25 12:59:04 PM: Evaluate: task multirc, batch 94 (152): ans_f1: 0.5810, qst_f1: 0.5566, em: 0.0103, avg: 0.2956, multirc_loss: 0.7121
03/25 12:59:14 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.5813, qst_f1: 0.5570, em: 0.0102, avg: 0.2957, multirc_loss: 0.7121
03/25 12:59:33 PM: Evaluate: task multirc, batch 97 (152): ans_f1: 0.5833, qst_f1: 0.5592, em: 0.0116, avg: 0.2974, multirc_loss: 0.7118
03/25 12:59:51 PM: Evaluate: task multirc, batch 99 (152): ans_f1: 0.5852, qst_f1: 0.5618, em: 0.0113, avg: 0.2982, multirc_loss: 0.7113
03/25 01:00:06 PM: Evaluate: task multirc, batch 101 (152): ans_f1: 0.5889, qst_f1: 0.5663, em: 0.0110, avg: 0.3000, multirc_loss: 0.7108
03/25 01:00:19 PM: Evaluate: task multirc, batch 103 (152): ans_f1: 0.5862, qst_f1: 0.5539, em: 0.0122, avg: 0.2992, multirc_loss: 0.7106
03/25 01:00:35 PM: Evaluate: task multirc, batch 105 (152): ans_f1: 0.5845, qst_f1: 0.5530, em: 0.0120, avg: 0.2982, multirc_loss: 0.7112
03/25 01:00:52 PM: Evaluate: task multirc, batch 107 (152): ans_f1: 0.5841, qst_f1: 0.5526, em: 0.0118, avg: 0.2979, multirc_loss: 0.7114
03/25 01:01:09 PM: Evaluate: task multirc, batch 109 (152): ans_f1: 0.5863, qst_f1: 0.5567, em: 0.0114, avg: 0.2989, multirc_loss: 0.7110
03/25 01:01:23 PM: Evaluate: task multirc, batch 111 (152): ans_f1: 0.5888, qst_f1: 0.5595, em: 0.0112, avg: 0.3000, multirc_loss: 0.7108
03/25 01:01:40 PM: Evaluate: task multirc, batch 113 (152): ans_f1: 0.5907, qst_f1: 0.5619, em: 0.0110, avg: 0.3008, multirc_loss: 0.7105
03/25 01:01:59 PM: Evaluate: task multirc, batch 115 (152): ans_f1: 0.5892, qst_f1: 0.5614, em: 0.0121, avg: 0.3007, multirc_loss: 0.7105
03/25 01:02:16 PM: Evaluate: task multirc, batch 117 (152): ans_f1: 0.5884, qst_f1: 0.5604, em: 0.0105, avg: 0.2995, multirc_loss: 0.7105
03/25 01:02:35 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.5888, qst_f1: 0.5613, em: 0.0104, avg: 0.2996, multirc_loss: 0.7105
03/25 01:02:54 PM: Evaluate: task multirc, batch 121 (152): ans_f1: 0.5884, qst_f1: 0.5614, em: 0.0103, avg: 0.2993, multirc_loss: 0.7105
03/25 01:03:13 PM: Evaluate: task multirc, batch 123 (152): ans_f1: 0.5873, qst_f1: 0.5609, em: 0.0101, avg: 0.2987, multirc_loss: 0.7106
03/25 01:03:23 PM: Evaluate: task multirc, batch 124 (152): ans_f1: 0.5874, qst_f1: 0.5610, em: 0.0101, avg: 0.2988, multirc_loss: 0.7105
03/25 01:03:43 PM: Evaluate: task multirc, batch 126 (152): ans_f1: 0.5868, qst_f1: 0.5603, em: 0.0100, avg: 0.2984, multirc_loss: 0.7106
03/25 01:03:58 PM: Evaluate: task multirc, batch 128 (152): ans_f1: 0.5856, qst_f1: 0.5595, em: 0.0099, avg: 0.2977, multirc_loss: 0.7109
03/25 01:04:14 PM: Evaluate: task multirc, batch 130 (152): ans_f1: 0.5865, qst_f1: 0.5610, em: 0.0109, avg: 0.2987, multirc_loss: 0.7109
03/25 01:04:31 PM: Evaluate: task multirc, batch 132 (152): ans_f1: 0.5863, qst_f1: 0.5603, em: 0.0107, avg: 0.2985, multirc_loss: 0.7110
03/25 01:04:48 PM: Evaluate: task multirc, batch 134 (152): ans_f1: 0.5871, qst_f1: 0.5618, em: 0.0105, avg: 0.2988, multirc_loss: 0.7109
03/25 01:04:59 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.5870, qst_f1: 0.5616, em: 0.0105, avg: 0.2987, multirc_loss: 0.7109
03/25 01:05:20 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.5874, qst_f1: 0.5619, em: 0.0103, avg: 0.2988, multirc_loss: 0.7109
03/25 01:05:35 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.5861, qst_f1: 0.5611, em: 0.0102, avg: 0.2981, multirc_loss: 0.7111
03/25 01:05:48 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.5851, qst_f1: 0.5606, em: 0.0101, avg: 0.2976, multirc_loss: 0.7112
03/25 01:06:06 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.5835, qst_f1: 0.5595, em: 0.0100, avg: 0.2968, multirc_loss: 0.7114
03/25 01:06:23 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.5824, qst_f1: 0.5586, em: 0.0099, avg: 0.2961, multirc_loss: 0.7116
03/25 01:06:40 PM: Evaluate: task multirc, batch 147 (152): ans_f1: 0.5832, qst_f1: 0.5595, em: 0.0097, avg: 0.2965, multirc_loss: 0.7115
03/25 01:06:58 PM: Evaluate: task multirc, batch 149 (152): ans_f1: 0.5834, qst_f1: 0.5598, em: 0.0096, avg: 0.2965, multirc_loss: 0.7115
03/25 01:07:09 PM: Evaluate: task multirc, batch 150 (152): ans_f1: 0.5838, qst_f1: 0.5598, em: 0.0095, avg: 0.2967, multirc_loss: 0.7114
03/25 01:07:22 PM: Evaluate: task multirc, batch 152 (152): ans_f1: 0.5866, qst_f1: 0.5630, em: 0.0094, avg: 0.2980, multirc_loss: 0.7107
03/25 01:07:23 PM: Best result seen so far for multirc.
03/25 01:07:23 PM: Best result seen so far for micro.
03/25 01:07:23 PM: Best result seen so far for macro.
03/25 01:07:23 PM: Updating LR scheduler:
03/25 01:07:23 PM: 	Best result seen so far for macro_avg: 0.298
03/25 01:07:23 PM: 	# validation passes without improvement: 0
03/25 01:07:23 PM: multirc_loss: training: 0.716142 validation: 0.710696
03/25 01:07:23 PM: macro_avg: validation: 0.297999
03/25 01:07:23 PM: micro_avg: validation: 0.297999
03/25 01:07:23 PM: multirc_ans_f1: training: 0.428571 validation: 0.586554
03/25 01:07:23 PM: multirc_qst_f1: training: 0.187500 validation: 0.563034
03/25 01:07:23 PM: multirc_em: training: 0.500000 validation: 0.009444
03/25 01:07:23 PM: multirc_avg: training: 0.464286 validation: 0.297999
03/25 01:07:23 PM: Global learning rate: 5e-06
03/25 01:07:23 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 01:08:05 PM: Update 4: task multirc, steps since last val 1 (total steps = 4): ans_f1: 0.4865, qst_f1: 0.2903, em: 0.4194, avg: 0.4529, multirc_loss: 0.7135
03/25 01:08:05 PM: ***** Step 4 / Validation 4 *****
03/25 01:08:05 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 01:08:05 PM: Validating...
03/25 01:08:23 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.6154, qst_f1: 0.6087, em: 0.0000, avg: 0.3077, multirc_loss: 0.7123
03/25 01:08:40 PM: Evaluate: task multirc, batch 4 (152): ans_f1: 0.6702, qst_f1: 0.6499, em: 0.0000, avg: 0.3351, multirc_loss: 0.7047
03/25 01:08:58 PM: Evaluate: task multirc, batch 6 (152): ans_f1: 0.6620, qst_f1: 0.6515, em: 0.0227, avg: 0.3423, multirc_loss: 0.7024
03/25 01:09:17 PM: Evaluate: task multirc, batch 8 (152): ans_f1: 0.6614, qst_f1: 0.6532, em: 0.0323, avg: 0.3468, multirc_loss: 0.7026
03/25 01:09:35 PM: Evaluate: task multirc, batch 10 (152): ans_f1: 0.6538, qst_f1: 0.6437, em: 0.0278, avg: 0.3408, multirc_loss: 0.7040
03/25 01:09:51 PM: Evaluate: task multirc, batch 12 (152): ans_f1: 0.6524, qst_f1: 0.6421, em: 0.0233, avg: 0.3378, multirc_loss: 0.7053
03/25 01:10:06 PM: Evaluate: task multirc, batch 14 (152): ans_f1: 0.6717, qst_f1: 0.6589, em: 0.0200, avg: 0.3458, multirc_loss: 0.7024
03/25 01:10:21 PM: Evaluate: task multirc, batch 16 (152): ans_f1: 0.6602, qst_f1: 0.6369, em: 0.0273, avg: 0.3438, multirc_loss: 0.7025
03/25 01:10:32 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.6563, qst_f1: 0.6415, em: 0.0342, avg: 0.3452, multirc_loss: 0.7031
03/25 01:10:49 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.6408, qst_f1: 0.6308, em: 0.0310, avg: 0.3359, multirc_loss: 0.7044
03/25 01:11:06 PM: Evaluate: task multirc, batch 21 (152): ans_f1: 0.6340, qst_f1: 0.6232, em: 0.0288, avg: 0.3314, multirc_loss: 0.7047
03/25 01:11:23 PM: Evaluate: task multirc, batch 23 (152): ans_f1: 0.6296, qst_f1: 0.6187, em: 0.0270, avg: 0.3283, multirc_loss: 0.7053
03/25 01:11:41 PM: Evaluate: task multirc, batch 25 (152): ans_f1: 0.6201, qst_f1: 0.6128, em: 0.0316, avg: 0.3259, multirc_loss: 0.7059
03/25 01:11:58 PM: Evaluate: task multirc, batch 27 (152): ans_f1: 0.6174, qst_f1: 0.6080, em: 0.0240, avg: 0.3207, multirc_loss: 0.7057
03/25 01:12:18 PM: Evaluate: task multirc, batch 29 (152): ans_f1: 0.6243, qst_f1: 0.6130, em: 0.0281, avg: 0.3262, multirc_loss: 0.7040
03/25 01:12:35 PM: Evaluate: task multirc, batch 31 (152): ans_f1: 0.6203, qst_f1: 0.6065, em: 0.0208, avg: 0.3206, multirc_loss: 0.7052
03/25 01:12:46 PM: Evaluate: task multirc, batch 32 (152): ans_f1: 0.6174, qst_f1: 0.6011, em: 0.0201, avg: 0.3188, multirc_loss: 0.7057
03/25 01:13:03 PM: Evaluate: task multirc, batch 34 (152): ans_f1: 0.6065, qst_f1: 0.5880, em: 0.0189, avg: 0.3127, multirc_loss: 0.7067
03/25 01:13:19 PM: Evaluate: task multirc, batch 36 (152): ans_f1: 0.5987, qst_f1: 0.5833, em: 0.0180, avg: 0.3084, multirc_loss: 0.7074
03/25 01:13:36 PM: Evaluate: task multirc, batch 38 (152): ans_f1: 0.5942, qst_f1: 0.5773, em: 0.0171, avg: 0.3057, multirc_loss: 0.7078
03/25 01:13:54 PM: Evaluate: task multirc, batch 40 (152): ans_f1: 0.5937, qst_f1: 0.5777, em: 0.0165, avg: 0.3051, multirc_loss: 0.7075
03/25 01:14:11 PM: Evaluate: task multirc, batch 42 (152): ans_f1: 0.5930, qst_f1: 0.5763, em: 0.0197, avg: 0.3064, multirc_loss: 0.7073
03/25 01:14:28 PM: Evaluate: task multirc, batch 44 (152): ans_f1: 0.5920, qst_f1: 0.5731, em: 0.0189, avg: 0.3054, multirc_loss: 0.7070
03/25 01:14:44 PM: Evaluate: task multirc, batch 46 (152): ans_f1: 0.5933, qst_f1: 0.5733, em: 0.0179, avg: 0.3056, multirc_loss: 0.7063
03/25 01:15:03 PM: Evaluate: task multirc, batch 48 (152): ans_f1: 0.5939, qst_f1: 0.5719, em: 0.0170, avg: 0.3055, multirc_loss: 0.7065
03/25 01:15:21 PM: Evaluate: task multirc, batch 50 (152): ans_f1: 0.5907, qst_f1: 0.5671, em: 0.0161, avg: 0.3034, multirc_loss: 0.7075
03/25 01:15:37 PM: Evaluate: task multirc, batch 52 (152): ans_f1: 0.5827, qst_f1: 0.5588, em: 0.0155, avg: 0.2991, multirc_loss: 0.7096
03/25 01:15:56 PM: Evaluate: task multirc, batch 54 (152): ans_f1: 0.5783, qst_f1: 0.5550, em: 0.0149, avg: 0.2966, multirc_loss: 0.7106
03/25 01:16:15 PM: Evaluate: task multirc, batch 56 (152): ans_f1: 0.5774, qst_f1: 0.5536, em: 0.0145, avg: 0.2959, multirc_loss: 0.7109
03/25 01:16:34 PM: Evaluate: task multirc, batch 58 (152): ans_f1: 0.5757, qst_f1: 0.5514, em: 0.0139, avg: 0.2948, multirc_loss: 0.7118
03/25 01:16:44 PM: Evaluate: task multirc, batch 59 (152): ans_f1: 0.5793, qst_f1: 0.5586, em: 0.0163, avg: 0.2978, multirc_loss: 0.7111
03/25 01:17:02 PM: Evaluate: task multirc, batch 61 (152): ans_f1: 0.5760, qst_f1: 0.5537, em: 0.0157, avg: 0.2959, multirc_loss: 0.7110
03/25 01:17:13 PM: Evaluate: task multirc, batch 62 (152): ans_f1: 0.5764, qst_f1: 0.5532, em: 0.0154, avg: 0.2959, multirc_loss: 0.7107
03/25 01:17:27 PM: Evaluate: task multirc, batch 64 (152): ans_f1: 0.5751, qst_f1: 0.5485, em: 0.0149, avg: 0.2950, multirc_loss: 0.7103
03/25 01:17:46 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.5792, qst_f1: 0.5554, em: 0.0143, avg: 0.2967, multirc_loss: 0.7095
03/25 01:17:56 PM: Evaluate: task multirc, batch 67 (152): ans_f1: 0.5791, qst_f1: 0.5558, em: 0.0165, avg: 0.2978, multirc_loss: 0.7098
03/25 01:18:14 PM: Evaluate: task multirc, batch 69 (152): ans_f1: 0.5796, qst_f1: 0.5561, em: 0.0137, avg: 0.2966, multirc_loss: 0.7103
03/25 01:18:30 PM: Evaluate: task multirc, batch 71 (152): ans_f1: 0.5795, qst_f1: 0.5560, em: 0.0133, avg: 0.2964, multirc_loss: 0.7108
03/25 01:18:45 PM: Evaluate: task multirc, batch 73 (152): ans_f1: 0.5804, qst_f1: 0.5572, em: 0.0153, avg: 0.2978, multirc_loss: 0.7111
03/25 01:19:00 PM: Evaluate: task multirc, batch 75 (152): ans_f1: 0.5781, qst_f1: 0.5549, em: 0.0127, avg: 0.2954, multirc_loss: 0.7119
03/25 01:19:15 PM: Evaluate: task multirc, batch 77 (152): ans_f1: 0.5773, qst_f1: 0.5544, em: 0.0125, avg: 0.2949, multirc_loss: 0.7125
03/25 01:19:31 PM: Evaluate: task multirc, batch 79 (152): ans_f1: 0.5772, qst_f1: 0.5552, em: 0.0162, avg: 0.2967, multirc_loss: 0.7128
03/25 01:19:46 PM: Evaluate: task multirc, batch 81 (152): ans_f1: 0.5776, qst_f1: 0.5544, em: 0.0138, avg: 0.2957, multirc_loss: 0.7126
03/25 01:20:00 PM: Evaluate: task multirc, batch 83 (152): ans_f1: 0.5789, qst_f1: 0.5554, em: 0.0134, avg: 0.2961, multirc_loss: 0.7123
03/25 01:20:17 PM: Evaluate: task multirc, batch 85 (152): ans_f1: 0.5778, qst_f1: 0.5540, em: 0.0131, avg: 0.2955, multirc_loss: 0.7123
03/25 01:20:36 PM: Evaluate: task multirc, batch 87 (152): ans_f1: 0.5767, qst_f1: 0.5541, em: 0.0129, avg: 0.2948, multirc_loss: 0.7125
03/25 01:20:54 PM: Evaluate: task multirc, batch 89 (152): ans_f1: 0.5829, qst_f1: 0.5596, em: 0.0125, avg: 0.2977, multirc_loss: 0.7115
03/25 01:21:06 PM: Evaluate: task multirc, batch 91 (152): ans_f1: 0.5850, qst_f1: 0.5607, em: 0.0124, avg: 0.2987, multirc_loss: 0.7113
03/25 01:21:22 PM: Evaluate: task multirc, batch 93 (152): ans_f1: 0.5827, qst_f1: 0.5586, em: 0.0121, avg: 0.2974, multirc_loss: 0.7117
03/25 01:21:40 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.5833, qst_f1: 0.5592, em: 0.0118, avg: 0.2976, multirc_loss: 0.7118
03/25 01:21:57 PM: Evaluate: task multirc, batch 97 (152): ans_f1: 0.5852, qst_f1: 0.5613, em: 0.0132, avg: 0.2992, multirc_loss: 0.7115
03/25 01:22:15 PM: Evaluate: task multirc, batch 99 (152): ans_f1: 0.5871, qst_f1: 0.5639, em: 0.0129, avg: 0.3000, multirc_loss: 0.7111
03/25 01:22:29 PM: Evaluate: task multirc, batch 101 (152): ans_f1: 0.5908, qst_f1: 0.5683, em: 0.0126, avg: 0.3017, multirc_loss: 0.7105
03/25 01:22:41 PM: Evaluate: task multirc, batch 103 (152): ans_f1: 0.5880, qst_f1: 0.5563, em: 0.0137, avg: 0.3009, multirc_loss: 0.7104
03/25 01:22:56 PM: Evaluate: task multirc, batch 105 (152): ans_f1: 0.5863, qst_f1: 0.5554, em: 0.0135, avg: 0.2999, multirc_loss: 0.7109
03/25 01:23:12 PM: Evaluate: task multirc, batch 107 (152): ans_f1: 0.5858, qst_f1: 0.5550, em: 0.0132, avg: 0.2995, multirc_loss: 0.7112
03/25 01:23:27 PM: Evaluate: task multirc, batch 109 (152): ans_f1: 0.5880, qst_f1: 0.5590, em: 0.0128, avg: 0.3004, multirc_loss: 0.7108
03/25 01:23:40 PM: Evaluate: task multirc, batch 111 (152): ans_f1: 0.5905, qst_f1: 0.5617, em: 0.0126, avg: 0.3015, multirc_loss: 0.7105
03/25 01:23:56 PM: Evaluate: task multirc, batch 113 (152): ans_f1: 0.5923, qst_f1: 0.5641, em: 0.0124, avg: 0.3024, multirc_loss: 0.7102
03/25 01:24:14 PM: Evaluate: task multirc, batch 115 (152): ans_f1: 0.5908, qst_f1: 0.5636, em: 0.0135, avg: 0.3021, multirc_loss: 0.7102
03/25 01:24:32 PM: Evaluate: task multirc, batch 117 (152): ans_f1: 0.5900, qst_f1: 0.5625, em: 0.0119, avg: 0.3009, multirc_loss: 0.7102
03/25 01:24:53 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.5904, qst_f1: 0.5635, em: 0.0117, avg: 0.3011, multirc_loss: 0.7102
03/25 01:25:11 PM: Evaluate: task multirc, batch 121 (152): ans_f1: 0.5901, qst_f1: 0.5636, em: 0.0115, avg: 0.3008, multirc_loss: 0.7102
03/25 01:25:23 PM: Evaluate: task multirc, batch 122 (152): ans_f1: 0.5889, qst_f1: 0.5627, em: 0.0115, avg: 0.3002, multirc_loss: 0.7104
03/25 01:25:42 PM: Evaluate: task multirc, batch 124 (152): ans_f1: 0.5890, qst_f1: 0.5631, em: 0.0113, avg: 0.3002, multirc_loss: 0.7102
03/25 01:26:02 PM: Evaluate: task multirc, batch 126 (152): ans_f1: 0.5884, qst_f1: 0.5625, em: 0.0112, avg: 0.2998, multirc_loss: 0.7103
03/25 01:26:17 PM: Evaluate: task multirc, batch 128 (152): ans_f1: 0.5873, qst_f1: 0.5616, em: 0.0111, avg: 0.2992, multirc_loss: 0.7105
03/25 01:26:31 PM: Evaluate: task multirc, batch 130 (152): ans_f1: 0.5881, qst_f1: 0.5631, em: 0.0121, avg: 0.3001, multirc_loss: 0.7106
03/25 01:26:48 PM: Evaluate: task multirc, batch 132 (152): ans_f1: 0.5879, qst_f1: 0.5624, em: 0.0119, avg: 0.2999, multirc_loss: 0.7107
03/25 01:27:05 PM: Evaluate: task multirc, batch 134 (152): ans_f1: 0.5887, qst_f1: 0.5638, em: 0.0117, avg: 0.3002, multirc_loss: 0.7106
03/25 01:27:24 PM: Evaluate: task multirc, batch 136 (152): ans_f1: 0.5892, qst_f1: 0.5643, em: 0.0115, avg: 0.3004, multirc_loss: 0.7105
03/25 01:27:42 PM: Evaluate: task multirc, batch 138 (152): ans_f1: 0.5880, qst_f1: 0.5636, em: 0.0125, avg: 0.3003, multirc_loss: 0.7107
03/25 01:27:54 PM: Evaluate: task multirc, batch 140 (152): ans_f1: 0.5878, qst_f1: 0.5634, em: 0.0113, avg: 0.2996, multirc_loss: 0.7108
03/25 01:28:09 PM: Evaluate: task multirc, batch 142 (152): ans_f1: 0.5856, qst_f1: 0.5620, em: 0.0112, avg: 0.2984, multirc_loss: 0.7109
03/25 01:28:25 PM: Evaluate: task multirc, batch 144 (152): ans_f1: 0.5851, qst_f1: 0.5616, em: 0.0110, avg: 0.2981, multirc_loss: 0.7110
03/25 01:28:41 PM: Evaluate: task multirc, batch 146 (152): ans_f1: 0.5853, qst_f1: 0.5623, em: 0.0120, avg: 0.2986, multirc_loss: 0.7110
03/25 01:28:59 PM: Evaluate: task multirc, batch 148 (152): ans_f1: 0.5845, qst_f1: 0.5606, em: 0.0107, avg: 0.2976, multirc_loss: 0.7112
03/25 01:29:16 PM: Evaluate: task multirc, batch 150 (152): ans_f1: 0.5852, qst_f1: 0.5616, em: 0.0106, avg: 0.2979, multirc_loss: 0.7110
03/25 01:29:29 PM: Evaluate: task multirc, batch 152 (152): ans_f1: 0.5879, qst_f1: 0.5648, em: 0.0105, avg: 0.2992, multirc_loss: 0.7103
03/25 01:29:29 PM: Best result seen so far for multirc.
03/25 01:29:29 PM: Best result seen so far for micro.
03/25 01:29:29 PM: Best result seen so far for macro.
03/25 01:29:29 PM: Updating LR scheduler:
03/25 01:29:29 PM: 	Best result seen so far for macro_avg: 0.299
03/25 01:29:29 PM: 	# validation passes without improvement: 0
03/25 01:29:29 PM: multirc_loss: training: 0.713515 validation: 0.710267
03/25 01:29:29 PM: macro_avg: validation: 0.299205
03/25 01:29:29 PM: micro_avg: validation: 0.299205
03/25 01:29:29 PM: multirc_ans_f1: training: 0.486486 validation: 0.587917
03/25 01:29:29 PM: multirc_qst_f1: training: 0.290323 validation: 0.564807
03/25 01:29:29 PM: multirc_em: training: 0.419355 validation: 0.010493
03/25 01:29:29 PM: multirc_avg: training: 0.452921 validation: 0.299205
03/25 01:29:29 PM: Global learning rate: 5e-06
03/25 01:29:29 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 01:30:01 PM: Update 5: task multirc, steps since last val 1 (total steps = 5): ans_f1: 0.3784, qst_f1: 0.2188, em: 0.2812, avg: 0.3298, multirc_loss: 0.7833
03/25 01:30:01 PM: ***** Step 5 / Validation 5 *****
03/25 01:30:02 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 01:30:02 PM: Validating...
03/25 01:30:19 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.6292, qst_f1: 0.6318, em: 0.0769, avg: 0.3531, multirc_loss: 0.7092
03/25 01:30:36 PM: Evaluate: task multirc, batch 4 (152): ans_f1: 0.6702, qst_f1: 0.6499, em: 0.0345, avg: 0.3523, multirc_loss: 0.7032
03/25 01:30:54 PM: Evaluate: task multirc, batch 6 (152): ans_f1: 0.6643, qst_f1: 0.6537, em: 0.0455, avg: 0.3549, multirc_loss: 0.7006
03/25 01:31:14 PM: Evaluate: task multirc, batch 8 (152): ans_f1: 0.6612, qst_f1: 0.6522, em: 0.0645, avg: 0.3629, multirc_loss: 0.7013
03/25 01:31:31 PM: Evaluate: task multirc, batch 10 (152): ans_f1: 0.6507, qst_f1: 0.6398, em: 0.0556, avg: 0.3531, multirc_loss: 0.7027
03/25 01:31:52 PM: Evaluate: task multirc, batch 12 (152): ans_f1: 0.6497, qst_f1: 0.6388, em: 0.0465, avg: 0.3481, multirc_loss: 0.7037
03/25 01:32:07 PM: Evaluate: task multirc, batch 14 (152): ans_f1: 0.6416, qst_f1: 0.6228, em: 0.0400, avg: 0.3408, multirc_loss: 0.7022
03/25 01:32:29 PM: Evaluate: task multirc, batch 16 (152): ans_f1: 0.6291, qst_f1: 0.5960, em: 0.0364, avg: 0.3328, multirc_loss: 0.7019
03/25 01:32:43 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.6277, qst_f1: 0.6034, em: 0.0427, avg: 0.3352, multirc_loss: 0.7022
03/25 01:32:54 PM: Evaluate: task multirc, batch 18 (152): ans_f1: 0.6194, qst_f1: 0.5991, em: 0.0410, avg: 0.3302, multirc_loss: 0.7031
03/25 01:33:06 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.6158, qst_f1: 0.5935, em: 0.0388, avg: 0.3273, multirc_loss: 0.7030
03/25 01:33:18 PM: Evaluate: task multirc, batch 20 (152): ans_f1: 0.6129, qst_f1: 0.5929, em: 0.0373, avg: 0.3251, multirc_loss: 0.7033
03/25 01:33:36 PM: Evaluate: task multirc, batch 22 (152): ans_f1: 0.6026, qst_f1: 0.5806, em: 0.0350, avg: 0.3188, multirc_loss: 0.7031
03/25 01:33:47 PM: Evaluate: task multirc, batch 23 (152): ans_f1: 0.6012, qst_f1: 0.5773, em: 0.0338, avg: 0.3175, multirc_loss: 0.7032
03/25 01:34:05 PM: Evaluate: task multirc, batch 25 (152): ans_f1: 0.5942, qst_f1: 0.5746, em: 0.0380, avg: 0.3161, multirc_loss: 0.7034
03/25 01:34:15 PM: Evaluate: task multirc, batch 26 (152): ans_f1: 0.5913, qst_f1: 0.5672, em: 0.0309, avg: 0.3111, multirc_loss: 0.7033
03/25 01:34:34 PM: Evaluate: task multirc, batch 28 (152): ans_f1: 0.5974, qst_f1: 0.5736, em: 0.0291, avg: 0.3133, multirc_loss: 0.7026
03/25 01:34:46 PM: Evaluate: task multirc, batch 29 (152): ans_f1: 0.6036, qst_f1: 0.5798, em: 0.0337, avg: 0.3186, multirc_loss: 0.7016
03/25 01:35:04 PM: Evaluate: task multirc, batch 31 (152): ans_f1: 0.6008, qst_f1: 0.5757, em: 0.0260, avg: 0.3134, multirc_loss: 0.7025
03/25 01:35:14 PM: Evaluate: task multirc, batch 32 (152): ans_f1: 0.5984, qst_f1: 0.5713, em: 0.0251, avg: 0.3118, multirc_loss: 0.7030
03/25 01:35:31 PM: Evaluate: task multirc, batch 34 (152): ans_f1: 0.5844, qst_f1: 0.5534, em: 0.0236, avg: 0.3040, multirc_loss: 0.7035
03/25 01:35:50 PM: Evaluate: task multirc, batch 36 (152): ans_f1: 0.5777, qst_f1: 0.5506, em: 0.0225, avg: 0.3001, multirc_loss: 0.7040
03/25 01:36:08 PM: Evaluate: task multirc, batch 38 (152): ans_f1: 0.5756, qst_f1: 0.5475, em: 0.0214, avg: 0.2985, multirc_loss: 0.7044
03/25 01:36:18 PM: Evaluate: task multirc, batch 39 (152): ans_f1: 0.5742, qst_f1: 0.5471, em: 0.0210, avg: 0.2976, multirc_loss: 0.7044
03/25 01:36:37 PM: Evaluate: task multirc, batch 41 (152): ans_f1: 0.5738, qst_f1: 0.5470, em: 0.0202, avg: 0.2970, multirc_loss: 0.7040
03/25 01:36:48 PM: Evaluate: task multirc, batch 42 (152): ans_f1: 0.5719, qst_f1: 0.5431, em: 0.0197, avg: 0.2958, multirc_loss: 0.7039
03/25 01:37:07 PM: Evaluate: task multirc, batch 44 (152): ans_f1: 0.5710, qst_f1: 0.5372, em: 0.0189, avg: 0.2949, multirc_loss: 0.7036
03/25 01:37:25 PM: Evaluate: task multirc, batch 46 (152): ans_f1: 0.5716, qst_f1: 0.5356, em: 0.0179, avg: 0.2947, multirc_loss: 0.7031
03/25 01:37:44 PM: Evaluate: task multirc, batch 48 (152): ans_f1: 0.5734, qst_f1: 0.5360, em: 0.0170, avg: 0.2952, multirc_loss: 0.7033
03/25 01:37:54 PM: Evaluate: task multirc, batch 49 (152): ans_f1: 0.5735, qst_f1: 0.5352, em: 0.0166, avg: 0.2951, multirc_loss: 0.7036
03/25 01:38:11 PM: Evaluate: task multirc, batch 51 (152): ans_f1: 0.5672, qst_f1: 0.5287, em: 0.0158, avg: 0.2915, multirc_loss: 0.7051
03/25 01:38:31 PM: Evaluate: task multirc, batch 53 (152): ans_f1: 0.5623, qst_f1: 0.5261, em: 0.0152, avg: 0.2888, multirc_loss: 0.7064
03/25 01:38:50 PM: Evaluate: task multirc, batch 55 (152): ans_f1: 0.5609, qst_f1: 0.5243, em: 0.0147, avg: 0.2878, multirc_loss: 0.7065
03/25 01:39:00 PM: Evaluate: task multirc, batch 56 (152): ans_f1: 0.5600, qst_f1: 0.5238, em: 0.0145, avg: 0.2872, multirc_loss: 0.7070
03/25 01:39:12 PM: Evaluate: task multirc, batch 57 (152): ans_f1: 0.5597, qst_f1: 0.5241, em: 0.0142, avg: 0.2869, multirc_loss: 0.7072
03/25 01:39:24 PM: Evaluate: task multirc, batch 58 (152): ans_f1: 0.5588, qst_f1: 0.5227, em: 0.0139, avg: 0.2864, multirc_loss: 0.7078
03/25 01:39:36 PM: Evaluate: task multirc, batch 59 (152): ans_f1: 0.5626, qst_f1: 0.5307, em: 0.0190, avg: 0.2908, multirc_loss: 0.7072
03/25 01:39:54 PM: Evaluate: task multirc, batch 61 (152): ans_f1: 0.5606, qst_f1: 0.5275, em: 0.0184, avg: 0.2895, multirc_loss: 0.7069
03/25 01:40:10 PM: Evaluate: task multirc, batch 63 (152): ans_f1: 0.5605, qst_f1: 0.5254, em: 0.0178, avg: 0.2891, multirc_loss: 0.7066
03/25 01:40:27 PM: Evaluate: task multirc, batch 65 (152): ans_f1: 0.5643, qst_f1: 0.5293, em: 0.0194, avg: 0.2918, multirc_loss: 0.7056
03/25 01:40:46 PM: Evaluate: task multirc, batch 67 (152): ans_f1: 0.5652, qst_f1: 0.5310, em: 0.0188, avg: 0.2920, multirc_loss: 0.7059
03/25 01:41:04 PM: Evaluate: task multirc, batch 69 (152): ans_f1: 0.5656, qst_f1: 0.5316, em: 0.0160, avg: 0.2908, multirc_loss: 0.7064
03/25 01:41:21 PM: Evaluate: task multirc, batch 71 (152): ans_f1: 0.5660, qst_f1: 0.5322, em: 0.0156, avg: 0.2908, multirc_loss: 0.7069
03/25 01:41:35 PM: Evaluate: task multirc, batch 73 (152): ans_f1: 0.5668, qst_f1: 0.5337, em: 0.0174, avg: 0.2921, multirc_loss: 0.7071
03/25 01:41:51 PM: Evaluate: task multirc, batch 75 (152): ans_f1: 0.5648, qst_f1: 0.5320, em: 0.0149, avg: 0.2898, multirc_loss: 0.7077
03/25 01:42:06 PM: Evaluate: task multirc, batch 77 (152): ans_f1: 0.5643, qst_f1: 0.5319, em: 0.0146, avg: 0.2894, multirc_loss: 0.7083
03/25 01:42:22 PM: Evaluate: task multirc, batch 79 (152): ans_f1: 0.5641, qst_f1: 0.5327, em: 0.0162, avg: 0.2901, multirc_loss: 0.7085
03/25 01:42:38 PM: Evaluate: task multirc, batch 81 (152): ans_f1: 0.5634, qst_f1: 0.5310, em: 0.0138, avg: 0.2886, multirc_loss: 0.7084
03/25 01:42:52 PM: Evaluate: task multirc, batch 83 (152): ans_f1: 0.5641, qst_f1: 0.5313, em: 0.0153, avg: 0.2897, multirc_loss: 0.7082
03/25 01:43:10 PM: Evaluate: task multirc, batch 85 (152): ans_f1: 0.5635, qst_f1: 0.5307, em: 0.0150, avg: 0.2892, multirc_loss: 0.7081
03/25 01:43:29 PM: Evaluate: task multirc, batch 87 (152): ans_f1: 0.5633, qst_f1: 0.5317, em: 0.0147, avg: 0.2890, multirc_loss: 0.7082
03/25 01:43:47 PM: Evaluate: task multirc, batch 89 (152): ans_f1: 0.5702, qst_f1: 0.5378, em: 0.0143, avg: 0.2923, multirc_loss: 0.7075
03/25 01:43:59 PM: Evaluate: task multirc, batch 91 (152): ans_f1: 0.5714, qst_f1: 0.5383, em: 0.0141, avg: 0.2928, multirc_loss: 0.7072
03/25 01:44:15 PM: Evaluate: task multirc, batch 93 (152): ans_f1: 0.5694, qst_f1: 0.5367, em: 0.0138, avg: 0.2916, multirc_loss: 0.7077
03/25 01:44:34 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.5703, qst_f1: 0.5378, em: 0.0135, avg: 0.2919, multirc_loss: 0.7078
03/25 01:44:52 PM: Evaluate: task multirc, batch 97 (152): ans_f1: 0.5725, qst_f1: 0.5404, em: 0.0149, avg: 0.2937, multirc_loss: 0.7075
03/25 01:45:03 PM: Evaluate: task multirc, batch 98 (152): ans_f1: 0.5738, qst_f1: 0.5414, em: 0.0146, avg: 0.2942, multirc_loss: 0.7072
03/25 01:45:15 PM: Evaluate: task multirc, batch 99 (152): ans_f1: 0.5745, qst_f1: 0.5425, em: 0.0145, avg: 0.2945, multirc_loss: 0.7072
03/25 01:45:32 PM: Evaluate: task multirc, batch 101 (152): ans_f1: 0.5786, qst_f1: 0.5474, em: 0.0142, avg: 0.2964, multirc_loss: 0.7067
03/25 01:45:44 PM: Evaluate: task multirc, batch 103 (152): ans_f1: 0.5730, qst_f1: 0.5307, em: 0.0137, avg: 0.2933, multirc_loss: 0.7068
03/25 01:46:02 PM: Evaluate: task multirc, batch 105 (152): ans_f1: 0.5715, qst_f1: 0.5301, em: 0.0135, avg: 0.2925, multirc_loss: 0.7072
03/25 01:46:18 PM: Evaluate: task multirc, batch 107 (152): ans_f1: 0.5713, qst_f1: 0.5302, em: 0.0132, avg: 0.2923, multirc_loss: 0.7074
03/25 01:46:34 PM: Evaluate: task multirc, batch 109 (152): ans_f1: 0.5743, qst_f1: 0.5360, em: 0.0157, avg: 0.2950, multirc_loss: 0.7071
03/25 01:46:47 PM: Evaluate: task multirc, batch 111 (152): ans_f1: 0.5771, qst_f1: 0.5391, em: 0.0154, avg: 0.2963, multirc_loss: 0.7069
03/25 01:47:05 PM: Evaluate: task multirc, batch 113 (152): ans_f1: 0.5793, qst_f1: 0.5419, em: 0.0151, avg: 0.2972, multirc_loss: 0.7067
03/25 01:47:23 PM: Evaluate: task multirc, batch 115 (152): ans_f1: 0.5780, qst_f1: 0.5418, em: 0.0162, avg: 0.2971, multirc_loss: 0.7066
03/25 01:47:40 PM: Evaluate: task multirc, batch 117 (152): ans_f1: 0.5777, qst_f1: 0.5418, em: 0.0184, avg: 0.2981, multirc_loss: 0.7066
03/25 01:47:59 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.5788, qst_f1: 0.5437, em: 0.0195, avg: 0.2991, multirc_loss: 0.7065
03/25 01:48:09 PM: Evaluate: task multirc, batch 120 (152): ans_f1: 0.5781, qst_f1: 0.5425, em: 0.0181, avg: 0.2981, multirc_loss: 0.7065
03/25 01:48:27 PM: Evaluate: task multirc, batch 122 (152): ans_f1: 0.5774, qst_f1: 0.5428, em: 0.0178, avg: 0.2976, multirc_loss: 0.7067
03/25 01:48:45 PM: Evaluate: task multirc, batch 124 (152): ans_f1: 0.5779, qst_f1: 0.5434, em: 0.0176, avg: 0.2978, multirc_loss: 0.7065
03/25 01:49:04 PM: Evaluate: task multirc, batch 126 (152): ans_f1: 0.5775, qst_f1: 0.5430, em: 0.0175, avg: 0.2975, multirc_loss: 0.7065
03/25 01:49:18 PM: Evaluate: task multirc, batch 128 (152): ans_f1: 0.5764, qst_f1: 0.5423, em: 0.0172, avg: 0.2968, multirc_loss: 0.7067
03/25 01:49:33 PM: Evaluate: task multirc, batch 130 (152): ans_f1: 0.5775, qst_f1: 0.5441, em: 0.0182, avg: 0.2978, multirc_loss: 0.7068
03/25 01:49:51 PM: Evaluate: task multirc, batch 132 (152): ans_f1: 0.5771, qst_f1: 0.5435, em: 0.0179, avg: 0.2975, multirc_loss: 0.7069
03/25 01:50:09 PM: Evaluate: task multirc, batch 134 (152): ans_f1: 0.5776, qst_f1: 0.5435, em: 0.0164, avg: 0.2970, multirc_loss: 0.7068
03/25 01:50:20 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.5777, qst_f1: 0.5436, em: 0.0163, avg: 0.2970, multirc_loss: 0.7068
03/25 01:50:41 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.5782, qst_f1: 0.5441, em: 0.0161, avg: 0.2971, multirc_loss: 0.7067
03/25 01:50:57 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.5771, qst_f1: 0.5435, em: 0.0159, avg: 0.2965, multirc_loss: 0.7069
03/25 01:51:10 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.5764, qst_f1: 0.5433, em: 0.0158, avg: 0.2961, multirc_loss: 0.7069
03/25 01:51:26 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.5753, qst_f1: 0.5426, em: 0.0156, avg: 0.2954, multirc_loss: 0.7070
03/25 01:51:42 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.5743, qst_f1: 0.5420, em: 0.0154, avg: 0.2948, multirc_loss: 0.7071
03/25 01:51:58 PM: Evaluate: task multirc, batch 147 (152): ans_f1: 0.5744, qst_f1: 0.5418, em: 0.0151, avg: 0.2947, multirc_loss: 0.7071
03/25 01:52:15 PM: Evaluate: task multirc, batch 149 (152): ans_f1: 0.5745, qst_f1: 0.5423, em: 0.0149, avg: 0.2947, multirc_loss: 0.7070
03/25 01:52:32 PM: Evaluate: task multirc, batch 151 (152): ans_f1: 0.5770, qst_f1: 0.5447, em: 0.0148, avg: 0.2959, multirc_loss: 0.7067
03/25 01:52:37 PM: Updating LR scheduler:
03/25 01:52:37 PM: 	Best result seen so far for macro_avg: 0.299
03/25 01:52:37 PM: 	# validation passes without improvement: 1
03/25 01:52:37 PM: multirc_loss: training: 0.783278 validation: 0.706312
03/25 01:52:37 PM: macro_avg: validation: 0.296348
03/25 01:52:37 PM: micro_avg: validation: 0.296348
03/25 01:52:37 PM: multirc_ans_f1: training: 0.378378 validation: 0.578006
03/25 01:52:37 PM: multirc_qst_f1: training: 0.218750 validation: 0.545909
03/25 01:52:37 PM: multirc_em: training: 0.281250 validation: 0.014690
03/25 01:52:37 PM: multirc_avg: training: 0.329814 validation: 0.296348
03/25 01:52:37 PM: Global learning rate: 5e-06
03/25 01:52:37 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 01:53:05 PM: Update 6: task multirc, steps since last val 1 (total steps = 6): ans_f1: 0.6829, qst_f1: 0.4713, em: 0.5862, avg: 0.6346, multirc_loss: 0.6694
03/25 01:53:05 PM: ***** Step 6 / Validation 6 *****
03/25 01:53:05 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 01:53:05 PM: Validating...
03/25 01:53:22 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.6222, qst_f1: 0.6241, em: 0.0769, avg: 0.3496, multirc_loss: 0.7105
03/25 01:53:38 PM: Evaluate: task multirc, batch 4 (152): ans_f1: 0.6737, qst_f1: 0.6568, em: 0.0345, avg: 0.3541, multirc_loss: 0.7031
03/25 01:53:54 PM: Evaluate: task multirc, batch 6 (152): ans_f1: 0.6667, qst_f1: 0.6583, em: 0.0455, avg: 0.3561, multirc_loss: 0.7001
03/25 01:54:13 PM: Evaluate: task multirc, batch 8 (152): ans_f1: 0.6649, qst_f1: 0.6581, em: 0.0645, avg: 0.3647, multirc_loss: 0.7007
03/25 01:54:31 PM: Evaluate: task multirc, batch 10 (152): ans_f1: 0.6537, qst_f1: 0.6449, em: 0.0556, avg: 0.3546, multirc_loss: 0.7023
03/25 01:54:47 PM: Evaluate: task multirc, batch 12 (152): ans_f1: 0.6523, qst_f1: 0.6431, em: 0.0465, avg: 0.3494, multirc_loss: 0.7034
03/25 01:55:01 PM: Evaluate: task multirc, batch 14 (152): ans_f1: 0.6533, qst_f1: 0.6383, em: 0.0400, avg: 0.3467, multirc_loss: 0.7016
03/25 01:55:16 PM: Evaluate: task multirc, batch 16 (152): ans_f1: 0.6402, qst_f1: 0.6069, em: 0.0364, avg: 0.3383, multirc_loss: 0.7013
03/25 01:55:27 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.6382, qst_f1: 0.6137, em: 0.0427, avg: 0.3405, multirc_loss: 0.7018
03/25 01:55:45 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.6232, qst_f1: 0.6016, em: 0.0388, avg: 0.3310, multirc_loss: 0.7030
03/25 01:56:01 PM: Evaluate: task multirc, batch 21 (152): ans_f1: 0.6186, qst_f1: 0.5966, em: 0.0360, avg: 0.3273, multirc_loss: 0.7030
03/25 01:56:17 PM: Evaluate: task multirc, batch 23 (152): ans_f1: 0.6160, qst_f1: 0.5937, em: 0.0338, avg: 0.3249, multirc_loss: 0.7033
03/25 01:56:35 PM: Evaluate: task multirc, batch 25 (152): ans_f1: 0.6089, qst_f1: 0.5910, em: 0.0380, avg: 0.3235, multirc_loss: 0.7036
03/25 01:56:51 PM: Evaluate: task multirc, batch 27 (152): ans_f1: 0.6081, qst_f1: 0.5880, em: 0.0299, avg: 0.3190, multirc_loss: 0.7034
03/25 01:57:09 PM: Evaluate: task multirc, batch 29 (152): ans_f1: 0.6159, qst_f1: 0.5943, em: 0.0337, avg: 0.3248, multirc_loss: 0.7017
03/25 01:57:27 PM: Evaluate: task multirc, batch 31 (152): ans_f1: 0.6123, qst_f1: 0.5892, em: 0.0260, avg: 0.3192, multirc_loss: 0.7028
03/25 01:57:45 PM: Evaluate: task multirc, batch 33 (152): ans_f1: 0.6041, qst_f1: 0.5774, em: 0.0291, avg: 0.3166, multirc_loss: 0.7033
03/25 01:58:01 PM: Evaluate: task multirc, batch 35 (152): ans_f1: 0.5935, qst_f1: 0.5660, em: 0.0231, avg: 0.3083, multirc_loss: 0.7041
03/25 01:58:17 PM: Evaluate: task multirc, batch 37 (152): ans_f1: 0.5900, qst_f1: 0.5636, em: 0.0218, avg: 0.3059, multirc_loss: 0.7042
03/25 01:58:36 PM: Evaluate: task multirc, batch 39 (152): ans_f1: 0.5848, qst_f1: 0.5580, em: 0.0210, avg: 0.3029, multirc_loss: 0.7045
03/25 01:58:55 PM: Evaluate: task multirc, batch 41 (152): ans_f1: 0.5830, qst_f1: 0.5570, em: 0.0202, avg: 0.3016, multirc_loss: 0.7041
03/25 01:59:12 PM: Evaluate: task multirc, batch 43 (152): ans_f1: 0.5791, qst_f1: 0.5463, em: 0.0192, avg: 0.2992, multirc_loss: 0.7038
03/25 01:59:29 PM: Evaluate: task multirc, batch 45 (152): ans_f1: 0.5793, qst_f1: 0.5454, em: 0.0184, avg: 0.2988, multirc_loss: 0.7034
03/25 01:59:47 PM: Evaluate: task multirc, batch 47 (152): ans_f1: 0.5821, qst_f1: 0.5458, em: 0.0174, avg: 0.2997, multirc_loss: 0.7027
03/25 01:59:57 PM: Evaluate: task multirc, batch 48 (152): ans_f1: 0.5820, qst_f1: 0.5455, em: 0.0170, avg: 0.2995, multirc_loss: 0.7033
03/25 02:00:07 PM: Evaluate: task multirc, batch 49 (152): ans_f1: 0.5819, qst_f1: 0.5445, em: 0.0166, avg: 0.2992, multirc_loss: 0.7037
03/25 02:00:24 PM: Evaluate: task multirc, batch 51 (152): ans_f1: 0.5754, qst_f1: 0.5376, em: 0.0158, avg: 0.2956, multirc_loss: 0.7052
03/25 02:00:44 PM: Evaluate: task multirc, batch 53 (152): ans_f1: 0.5705, qst_f1: 0.5349, em: 0.0152, avg: 0.2929, multirc_loss: 0.7066
03/25 02:01:03 PM: Evaluate: task multirc, batch 55 (152): ans_f1: 0.5691, qst_f1: 0.5328, em: 0.0147, avg: 0.2919, multirc_loss: 0.7067
03/25 02:01:22 PM: Evaluate: task multirc, batch 57 (152): ans_f1: 0.5676, qst_f1: 0.5323, em: 0.0142, avg: 0.2909, multirc_loss: 0.7074
03/25 02:01:32 PM: Evaluate: task multirc, batch 58 (152): ans_f1: 0.5666, qst_f1: 0.5307, em: 0.0139, avg: 0.2903, multirc_loss: 0.7079
03/25 02:01:43 PM: Evaluate: task multirc, batch 59 (152): ans_f1: 0.5702, qst_f1: 0.5385, em: 0.0190, avg: 0.2946, multirc_loss: 0.7074
03/25 02:02:02 PM: Evaluate: task multirc, batch 61 (152): ans_f1: 0.5683, qst_f1: 0.5353, em: 0.0184, avg: 0.2933, multirc_loss: 0.7071
03/25 02:02:17 PM: Evaluate: task multirc, batch 63 (152): ans_f1: 0.5676, qst_f1: 0.5338, em: 0.0178, avg: 0.2927, multirc_loss: 0.7067
03/25 02:02:34 PM: Evaluate: task multirc, batch 65 (152): ans_f1: 0.5710, qst_f1: 0.5372, em: 0.0194, avg: 0.2952, multirc_loss: 0.7057
03/25 02:02:45 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.5716, qst_f1: 0.5381, em: 0.0167, avg: 0.2942, multirc_loss: 0.7057
03/25 02:03:04 PM: Evaluate: task multirc, batch 68 (152): ans_f1: 0.5733, qst_f1: 0.5388, em: 0.0162, avg: 0.2948, multirc_loss: 0.7059
03/25 02:03:15 PM: Evaluate: task multirc, batch 69 (152): ans_f1: 0.5719, qst_f1: 0.5391, em: 0.0160, avg: 0.2940, multirc_loss: 0.7064
03/25 02:03:26 PM: Evaluate: task multirc, batch 70 (152): ans_f1: 0.5720, qst_f1: 0.5396, em: 0.0158, avg: 0.2939, multirc_loss: 0.7067
03/25 02:03:41 PM: Evaluate: task multirc, batch 72 (152): ans_f1: 0.5727, qst_f1: 0.5401, em: 0.0154, avg: 0.2940, multirc_loss: 0.7071
03/25 02:03:59 PM: Evaluate: task multirc, batch 74 (152): ans_f1: 0.5710, qst_f1: 0.5378, em: 0.0151, avg: 0.2930, multirc_loss: 0.7077
03/25 02:04:18 PM: Evaluate: task multirc, batch 76 (152): ans_f1: 0.5707, qst_f1: 0.5391, em: 0.0147, avg: 0.2927, multirc_loss: 0.7081
03/25 02:04:34 PM: Evaluate: task multirc, batch 78 (152): ans_f1: 0.5700, qst_f1: 0.5391, em: 0.0143, avg: 0.2922, multirc_loss: 0.7084
03/25 02:04:51 PM: Evaluate: task multirc, batch 80 (152): ans_f1: 0.5702, qst_f1: 0.5396, em: 0.0160, avg: 0.2931, multirc_loss: 0.7086
03/25 02:05:07 PM: Evaluate: task multirc, batch 82 (152): ans_f1: 0.5700, qst_f1: 0.5393, em: 0.0175, avg: 0.2938, multirc_loss: 0.7083
03/25 02:05:25 PM: Evaluate: task multirc, batch 84 (152): ans_f1: 0.5696, qst_f1: 0.5380, em: 0.0170, avg: 0.2933, multirc_loss: 0.7081
03/25 02:05:35 PM: Evaluate: task multirc, batch 85 (152): ans_f1: 0.5687, qst_f1: 0.5366, em: 0.0169, avg: 0.2928, multirc_loss: 0.7082
03/25 02:05:47 PM: Evaluate: task multirc, batch 86 (152): ans_f1: 0.5676, qst_f1: 0.5360, em: 0.0167, avg: 0.2922, multirc_loss: 0.7084
03/25 02:05:57 PM: Evaluate: task multirc, batch 87 (152): ans_f1: 0.5682, qst_f1: 0.5374, em: 0.0165, avg: 0.2924, multirc_loss: 0.7084
03/25 02:06:08 PM: Evaluate: task multirc, batch 88 (152): ans_f1: 0.5718, qst_f1: 0.5406, em: 0.0182, avg: 0.2950, multirc_loss: 0.7080
03/25 02:06:18 PM: Evaluate: task multirc, batch 89 (152): ans_f1: 0.5749, qst_f1: 0.5433, em: 0.0161, avg: 0.2955, multirc_loss: 0.7076
03/25 02:06:32 PM: Evaluate: task multirc, batch 91 (152): ans_f1: 0.5768, qst_f1: 0.5441, em: 0.0159, avg: 0.2963, multirc_loss: 0.7073
03/25 02:06:49 PM: Evaluate: task multirc, batch 93 (152): ans_f1: 0.5746, qst_f1: 0.5424, em: 0.0155, avg: 0.2951, multirc_loss: 0.7077
03/25 02:06:59 PM: Evaluate: task multirc, batch 94 (152): ans_f1: 0.5750, qst_f1: 0.5428, em: 0.0154, avg: 0.2952, multirc_loss: 0.7079
03/25 02:07:10 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.5754, qst_f1: 0.5433, em: 0.0152, avg: 0.2953, multirc_loss: 0.7078
03/25 02:07:21 PM: Evaluate: task multirc, batch 96 (152): ans_f1: 0.5768, qst_f1: 0.5445, em: 0.0151, avg: 0.2960, multirc_loss: 0.7077
03/25 02:07:40 PM: Evaluate: task multirc, batch 98 (152): ans_f1: 0.5787, qst_f1: 0.5468, em: 0.0163, avg: 0.2975, multirc_loss: 0.7073
03/25 02:07:50 PM: Evaluate: task multirc, batch 99 (152): ans_f1: 0.5793, qst_f1: 0.5478, em: 0.0161, avg: 0.2977, multirc_loss: 0.7072
03/25 02:08:07 PM: Evaluate: task multirc, batch 101 (152): ans_f1: 0.5833, qst_f1: 0.5525, em: 0.0157, avg: 0.2995, multirc_loss: 0.7068
03/25 02:08:20 PM: Evaluate: task multirc, batch 103 (152): ans_f1: 0.5780, qst_f1: 0.5365, em: 0.0152, avg: 0.2966, multirc_loss: 0.7068
03/25 02:08:38 PM: Evaluate: task multirc, batch 105 (152): ans_f1: 0.5764, qst_f1: 0.5358, em: 0.0150, avg: 0.2957, multirc_loss: 0.7072
03/25 02:08:55 PM: Evaluate: task multirc, batch 107 (152): ans_f1: 0.5762, qst_f1: 0.5358, em: 0.0147, avg: 0.2954, multirc_loss: 0.7074
03/25 02:09:13 PM: Evaluate: task multirc, batch 109 (152): ans_f1: 0.5790, qst_f1: 0.5414, em: 0.0171, avg: 0.2981, multirc_loss: 0.7071
03/25 02:09:27 PM: Evaluate: task multirc, batch 111 (152): ans_f1: 0.5817, qst_f1: 0.5444, em: 0.0168, avg: 0.2993, multirc_loss: 0.7069
03/25 02:09:45 PM: Evaluate: task multirc, batch 113 (152): ans_f1: 0.5838, qst_f1: 0.5471, em: 0.0165, avg: 0.3001, multirc_loss: 0.7066
03/25 02:09:55 PM: Evaluate: task multirc, batch 114 (152): ans_f1: 0.5826, qst_f1: 0.5457, em: 0.0163, avg: 0.2995, multirc_loss: 0.7067
03/25 02:10:13 PM: Evaluate: task multirc, batch 116 (152): ans_f1: 0.5823, qst_f1: 0.5462, em: 0.0173, avg: 0.2998, multirc_loss: 0.7065
03/25 02:10:31 PM: Evaluate: task multirc, batch 118 (152): ans_f1: 0.5819, qst_f1: 0.5467, em: 0.0197, avg: 0.3008, multirc_loss: 0.7065
03/25 02:10:41 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.5827, qst_f1: 0.5477, em: 0.0195, avg: 0.3011, multirc_loss: 0.7064
03/25 02:10:52 PM: Evaluate: task multirc, batch 120 (152): ans_f1: 0.5820, qst_f1: 0.5465, em: 0.0194, avg: 0.3007, multirc_loss: 0.7064
03/25 02:11:11 PM: Evaluate: task multirc, batch 122 (152): ans_f1: 0.5813, qst_f1: 0.5471, em: 0.0191, avg: 0.3002, multirc_loss: 0.7066
03/25 02:11:31 PM: Evaluate: task multirc, batch 124 (152): ans_f1: 0.5817, qst_f1: 0.5476, em: 0.0189, avg: 0.3003, multirc_loss: 0.7065
03/25 02:11:41 PM: Evaluate: task multirc, batch 125 (152): ans_f1: 0.5820, qst_f1: 0.5478, em: 0.0188, avg: 0.3004, multirc_loss: 0.7063
03/25 02:11:52 PM: Evaluate: task multirc, batch 126 (152): ans_f1: 0.5812, qst_f1: 0.5472, em: 0.0187, avg: 0.3000, multirc_loss: 0.7065
03/25 02:12:07 PM: Evaluate: task multirc, batch 128 (152): ans_f1: 0.5801, qst_f1: 0.5465, em: 0.0185, avg: 0.2993, multirc_loss: 0.7067
03/25 02:12:23 PM: Evaluate: task multirc, batch 130 (152): ans_f1: 0.5811, qst_f1: 0.5482, em: 0.0194, avg: 0.3002, multirc_loss: 0.7068
03/25 02:12:41 PM: Evaluate: task multirc, batch 132 (152): ans_f1: 0.5807, qst_f1: 0.5476, em: 0.0191, avg: 0.2999, multirc_loss: 0.7069
03/25 02:12:59 PM: Evaluate: task multirc, batch 134 (152): ans_f1: 0.5815, qst_f1: 0.5478, em: 0.0175, avg: 0.2995, multirc_loss: 0.7067
03/25 02:13:10 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.5817, qst_f1: 0.5479, em: 0.0174, avg: 0.2995, multirc_loss: 0.7067
03/25 02:13:20 PM: Evaluate: task multirc, batch 136 (152): ans_f1: 0.5824, qst_f1: 0.5486, em: 0.0173, avg: 0.2998, multirc_loss: 0.7067
03/25 02:13:32 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.5821, qst_f1: 0.5484, em: 0.0172, avg: 0.2996, multirc_loss: 0.7067
03/25 02:13:47 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.5809, qst_f1: 0.5477, em: 0.0170, avg: 0.2990, multirc_loss: 0.7068
03/25 02:14:00 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.5801, qst_f1: 0.5474, em: 0.0169, avg: 0.2985, multirc_loss: 0.7069
03/25 02:14:18 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.5792, qst_f1: 0.5473, em: 0.0178, avg: 0.2985, multirc_loss: 0.7070
03/25 02:14:36 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.5783, qst_f1: 0.5467, em: 0.0175, avg: 0.2979, multirc_loss: 0.7070
03/25 02:14:53 PM: Evaluate: task multirc, batch 147 (152): ans_f1: 0.5790, qst_f1: 0.5474, em: 0.0173, avg: 0.2981, multirc_loss: 0.7070
03/25 02:15:03 PM: Evaluate: task multirc, batch 148 (152): ans_f1: 0.5784, qst_f1: 0.5466, em: 0.0172, avg: 0.2978, multirc_loss: 0.7070
03/25 02:15:22 PM: Evaluate: task multirc, batch 150 (152): ans_f1: 0.5788, qst_f1: 0.5473, em: 0.0170, avg: 0.2979, multirc_loss: 0.7069
03/25 02:15:36 PM: Evaluate: task multirc, batch 152 (152): ans_f1: 0.5817, qst_f1: 0.5507, em: 0.0168, avg: 0.2992, multirc_loss: 0.7062
03/25 02:15:37 PM: Best result seen so far for multirc.
03/25 02:15:37 PM: Best result seen so far for micro.
03/25 02:15:37 PM: Best result seen so far for macro.
03/25 02:15:37 PM: Updating LR scheduler:
03/25 02:15:37 PM: 	Best result seen so far for macro_avg: 0.299
03/25 02:15:37 PM: 	# validation passes without improvement: 2
03/25 02:15:37 PM: multirc_loss: training: 0.669390 validation: 0.706209
03/25 02:15:37 PM: macro_avg: validation: 0.299245
03/25 02:15:37 PM: micro_avg: validation: 0.299245
03/25 02:15:37 PM: multirc_ans_f1: training: 0.682927 validation: 0.581700
03/25 02:15:37 PM: multirc_qst_f1: training: 0.471264 validation: 0.550703
03/25 02:15:37 PM: multirc_em: training: 0.586207 validation: 0.016789
03/25 02:15:37 PM: multirc_avg: training: 0.634567 validation: 0.299245
03/25 02:15:37 PM: Global learning rate: 5e-06
03/25 02:15:37 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 02:16:28 PM: Update 7: task multirc, steps since last val 1 (total steps = 7): ans_f1: 0.6047, qst_f1: 0.4062, em: 0.4688, avg: 0.5367, multirc_loss: 0.7043
03/25 02:16:28 PM: ***** Step 7 / Validation 7 *****
03/25 02:16:28 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 02:16:28 PM: Validating...
03/25 02:16:47 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.6087, qst_f1: 0.5984, em: 0.0000, avg: 0.3043, multirc_loss: 0.7125
03/25 02:16:57 PM: Evaluate: task multirc, batch 3 (152): ans_f1: 0.6187, qst_f1: 0.6161, em: 0.0000, avg: 0.3094, multirc_loss: 0.7121
03/25 02:17:13 PM: Evaluate: task multirc, batch 5 (152): ans_f1: 0.6695, qst_f1: 0.6593, em: 0.0270, avg: 0.3482, multirc_loss: 0.7000
03/25 02:17:33 PM: Evaluate: task multirc, batch 7 (152): ans_f1: 0.6606, qst_f1: 0.6428, em: 0.0189, avg: 0.3397, multirc_loss: 0.7003
03/25 02:17:43 PM: Evaluate: task multirc, batch 8 (152): ans_f1: 0.6631, qst_f1: 0.6554, em: 0.0484, avg: 0.3557, multirc_loss: 0.7003
03/25 02:18:02 PM: Evaluate: task multirc, batch 10 (152): ans_f1: 0.6537, qst_f1: 0.6433, em: 0.0417, avg: 0.3477, multirc_loss: 0.7021
03/25 02:18:12 PM: Evaluate: task multirc, batch 11 (152): ans_f1: 0.6588, qst_f1: 0.6477, em: 0.0380, avg: 0.3484, multirc_loss: 0.7011
03/25 02:18:30 PM: Evaluate: task multirc, batch 13 (152): ans_f1: 0.6534, qst_f1: 0.6406, em: 0.0426, avg: 0.3480, multirc_loss: 0.7021
03/25 02:18:43 PM: Evaluate: task multirc, batch 15 (152): ans_f1: 0.6528, qst_f1: 0.6226, em: 0.0381, avg: 0.3454, multirc_loss: 0.7001
03/25 02:19:03 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.6464, qst_f1: 0.6255, em: 0.0342, avg: 0.3403, multirc_loss: 0.7015
03/25 02:19:22 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.6308, qst_f1: 0.6129, em: 0.0310, avg: 0.3309, multirc_loss: 0.7032
03/25 02:19:32 PM: Evaluate: task multirc, batch 20 (152): ans_f1: 0.6266, qst_f1: 0.6107, em: 0.0299, avg: 0.3282, multirc_loss: 0.7036
03/25 02:19:47 PM: Evaluate: task multirc, batch 22 (152): ans_f1: 0.6241, qst_f1: 0.6096, em: 0.0280, avg: 0.3260, multirc_loss: 0.7035
03/25 02:20:06 PM: Evaluate: task multirc, batch 24 (152): ans_f1: 0.6173, qst_f1: 0.6038, em: 0.0261, avg: 0.3217, multirc_loss: 0.7034
03/25 02:20:24 PM: Evaluate: task multirc, batch 26 (152): ans_f1: 0.6095, qst_f1: 0.5942, em: 0.0247, avg: 0.3171, multirc_loss: 0.7041
03/25 02:20:42 PM: Evaluate: task multirc, batch 28 (152): ans_f1: 0.6140, qst_f1: 0.5991, em: 0.0233, avg: 0.3186, multirc_loss: 0.7033
03/25 02:20:52 PM: Evaluate: task multirc, batch 29 (152): ans_f1: 0.6192, qst_f1: 0.6044, em: 0.0281, avg: 0.3237, multirc_loss: 0.7021
03/25 02:21:11 PM: Evaluate: task multirc, batch 31 (152): ans_f1: 0.6159, qst_f1: 0.5989, em: 0.0208, avg: 0.3184, multirc_loss: 0.7033
03/25 02:21:22 PM: Evaluate: task multirc, batch 32 (152): ans_f1: 0.6132, qst_f1: 0.5937, em: 0.0201, avg: 0.3167, multirc_loss: 0.7037
03/25 02:21:42 PM: Evaluate: task multirc, batch 34 (152): ans_f1: 0.5995, qst_f1: 0.5744, em: 0.0189, avg: 0.3092, multirc_loss: 0.7043
03/25 02:21:59 PM: Evaluate: task multirc, batch 36 (152): ans_f1: 0.5922, qst_f1: 0.5707, em: 0.0180, avg: 0.3051, multirc_loss: 0.7048
03/25 02:22:16 PM: Evaluate: task multirc, batch 38 (152): ans_f1: 0.5897, qst_f1: 0.5666, em: 0.0171, avg: 0.3034, multirc_loss: 0.7051
03/25 02:22:35 PM: Evaluate: task multirc, batch 40 (152): ans_f1: 0.5886, qst_f1: 0.5667, em: 0.0165, avg: 0.3026, multirc_loss: 0.7047
03/25 02:22:53 PM: Evaluate: task multirc, batch 42 (152): ans_f1: 0.5857, qst_f1: 0.5622, em: 0.0197, avg: 0.3027, multirc_loss: 0.7044
03/25 02:23:10 PM: Evaluate: task multirc, batch 44 (152): ans_f1: 0.5860, qst_f1: 0.5601, em: 0.0189, avg: 0.3024, multirc_loss: 0.7040
03/25 02:23:27 PM: Evaluate: task multirc, batch 46 (152): ans_f1: 0.5863, qst_f1: 0.5573, em: 0.0179, avg: 0.3021, multirc_loss: 0.7035
03/25 02:23:46 PM: Evaluate: task multirc, batch 48 (152): ans_f1: 0.5874, qst_f1: 0.5566, em: 0.0170, avg: 0.3022, multirc_loss: 0.7037
03/25 02:24:04 PM: Evaluate: task multirc, batch 50 (152): ans_f1: 0.5849, qst_f1: 0.5533, em: 0.0161, avg: 0.3005, multirc_loss: 0.7046
03/25 02:24:21 PM: Evaluate: task multirc, batch 52 (152): ans_f1: 0.5772, qst_f1: 0.5459, em: 0.0155, avg: 0.2964, multirc_loss: 0.7069
03/25 02:24:32 PM: Evaluate: task multirc, batch 53 (152): ans_f1: 0.5761, qst_f1: 0.5451, em: 0.0152, avg: 0.2957, multirc_loss: 0.7072
03/25 02:24:51 PM: Evaluate: task multirc, batch 55 (152): ans_f1: 0.5741, qst_f1: 0.5417, em: 0.0147, avg: 0.2944, multirc_loss: 0.7072
03/25 02:25:09 PM: Evaluate: task multirc, batch 57 (152): ans_f1: 0.5725, qst_f1: 0.5409, em: 0.0142, avg: 0.2933, multirc_loss: 0.7080
03/25 02:25:29 PM: Evaluate: task multirc, batch 59 (152): ans_f1: 0.5749, qst_f1: 0.5467, em: 0.0190, avg: 0.2969, multirc_loss: 0.7080
03/25 02:25:48 PM: Evaluate: task multirc, batch 61 (152): ans_f1: 0.5729, qst_f1: 0.5433, em: 0.0184, avg: 0.2956, multirc_loss: 0.7076
03/25 02:26:02 PM: Evaluate: task multirc, batch 63 (152): ans_f1: 0.5729, qst_f1: 0.5428, em: 0.0178, avg: 0.2954, multirc_loss: 0.7072
03/25 02:26:20 PM: Evaluate: task multirc, batch 65 (152): ans_f1: 0.5768, qst_f1: 0.5475, em: 0.0194, avg: 0.2981, multirc_loss: 0.7061
03/25 02:26:31 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.5773, qst_f1: 0.5482, em: 0.0167, avg: 0.2970, multirc_loss: 0.7061
03/25 02:26:49 PM: Evaluate: task multirc, batch 68 (152): ans_f1: 0.5793, qst_f1: 0.5490, em: 0.0162, avg: 0.2977, multirc_loss: 0.7062
03/25 02:27:00 PM: Evaluate: task multirc, batch 69 (152): ans_f1: 0.5778, qst_f1: 0.5491, em: 0.0160, avg: 0.2969, multirc_loss: 0.7068
03/25 02:27:19 PM: Evaluate: task multirc, batch 71 (152): ans_f1: 0.5778, qst_f1: 0.5493, em: 0.0156, avg: 0.2967, multirc_loss: 0.7074
03/25 02:27:34 PM: Evaluate: task multirc, batch 73 (152): ans_f1: 0.5787, qst_f1: 0.5506, em: 0.0174, avg: 0.2981, multirc_loss: 0.7077
03/25 02:27:52 PM: Evaluate: task multirc, batch 75 (152): ans_f1: 0.5765, qst_f1: 0.5485, em: 0.0149, avg: 0.2957, multirc_loss: 0.7082
03/25 02:28:08 PM: Evaluate: task multirc, batch 77 (152): ans_f1: 0.5756, qst_f1: 0.5481, em: 0.0146, avg: 0.2951, multirc_loss: 0.7087
03/25 02:28:26 PM: Evaluate: task multirc, batch 79 (152): ans_f1: 0.5753, qst_f1: 0.5484, em: 0.0162, avg: 0.2957, multirc_loss: 0.7091
03/25 02:28:44 PM: Evaluate: task multirc, batch 81 (152): ans_f1: 0.5747, qst_f1: 0.5469, em: 0.0138, avg: 0.2942, multirc_loss: 0.7090
03/25 02:29:00 PM: Evaluate: task multirc, batch 83 (152): ans_f1: 0.5752, qst_f1: 0.5470, em: 0.0153, avg: 0.2952, multirc_loss: 0.7087
03/25 02:29:19 PM: Evaluate: task multirc, batch 85 (152): ans_f1: 0.5745, qst_f1: 0.5472, em: 0.0150, avg: 0.2948, multirc_loss: 0.7087
03/25 02:29:30 PM: Evaluate: task multirc, batch 86 (152): ans_f1: 0.5734, qst_f1: 0.5465, em: 0.0149, avg: 0.2941, multirc_loss: 0.7090
03/25 02:29:51 PM: Evaluate: task multirc, batch 88 (152): ans_f1: 0.5774, qst_f1: 0.5509, em: 0.0164, avg: 0.2969, multirc_loss: 0.7085
03/25 02:30:01 PM: Evaluate: task multirc, batch 89 (152): ans_f1: 0.5804, qst_f1: 0.5535, em: 0.0143, avg: 0.2974, multirc_loss: 0.7080
03/25 02:30:15 PM: Evaluate: task multirc, batch 91 (152): ans_f1: 0.5827, qst_f1: 0.5547, em: 0.0141, avg: 0.2984, multirc_loss: 0.7077
03/25 02:30:32 PM: Evaluate: task multirc, batch 93 (152): ans_f1: 0.5804, qst_f1: 0.5528, em: 0.0138, avg: 0.2971, multirc_loss: 0.7082
03/25 02:30:43 PM: Evaluate: task multirc, batch 94 (152): ans_f1: 0.5807, qst_f1: 0.5531, em: 0.0137, avg: 0.2972, multirc_loss: 0.7083
03/25 02:30:53 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.5811, qst_f1: 0.5535, em: 0.0135, avg: 0.2973, multirc_loss: 0.7083
03/25 02:31:04 PM: Evaluate: task multirc, batch 96 (152): ans_f1: 0.5824, qst_f1: 0.5546, em: 0.0134, avg: 0.2979, multirc_loss: 0.7082
03/25 02:31:23 PM: Evaluate: task multirc, batch 98 (152): ans_f1: 0.5841, qst_f1: 0.5566, em: 0.0146, avg: 0.2994, multirc_loss: 0.7077
03/25 02:31:41 PM: Evaluate: task multirc, batch 100 (152): ans_f1: 0.5856, qst_f1: 0.5594, em: 0.0143, avg: 0.3000, multirc_loss: 0.7076
03/25 02:31:55 PM: Evaluate: task multirc, batch 102 (152): ans_f1: 0.5865, qst_f1: 0.5544, em: 0.0139, avg: 0.3002, multirc_loss: 0.7072
03/25 02:32:11 PM: Evaluate: task multirc, batch 104 (152): ans_f1: 0.5838, qst_f1: 0.5480, em: 0.0136, avg: 0.2987, multirc_loss: 0.7075
03/25 02:32:28 PM: Evaluate: task multirc, batch 106 (152): ans_f1: 0.5828, qst_f1: 0.5474, em: 0.0134, avg: 0.2981, multirc_loss: 0.7076
03/25 02:32:46 PM: Evaluate: task multirc, batch 108 (152): ans_f1: 0.5839, qst_f1: 0.5490, em: 0.0130, avg: 0.2984, multirc_loss: 0.7076
03/25 02:33:03 PM: Evaluate: task multirc, batch 110 (152): ans_f1: 0.5864, qst_f1: 0.5530, em: 0.0156, avg: 0.3010, multirc_loss: 0.7073
03/25 02:33:18 PM: Evaluate: task multirc, batch 112 (152): ans_f1: 0.5895, qst_f1: 0.5564, em: 0.0153, avg: 0.3024, multirc_loss: 0.7069
03/25 02:33:37 PM: Evaluate: task multirc, batch 114 (152): ans_f1: 0.5885, qst_f1: 0.5556, em: 0.0150, avg: 0.3017, multirc_loss: 0.7070
03/25 02:33:56 PM: Evaluate: task multirc, batch 116 (152): ans_f1: 0.5882, qst_f1: 0.5561, em: 0.0159, avg: 0.3021, multirc_loss: 0.7068
03/25 02:34:13 PM: Evaluate: task multirc, batch 118 (152): ans_f1: 0.5874, qst_f1: 0.5560, em: 0.0170, avg: 0.3022, multirc_loss: 0.7068
03/25 02:34:26 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.5880, qst_f1: 0.5567, em: 0.0169, avg: 0.3025, multirc_loss: 0.7067
03/25 02:34:44 PM: Evaluate: task multirc, batch 121 (152): ans_f1: 0.5874, qst_f1: 0.5565, em: 0.0167, avg: 0.3020, multirc_loss: 0.7068
03/25 02:35:04 PM: Evaluate: task multirc, batch 123 (152): ans_f1: 0.5866, qst_f1: 0.5563, em: 0.0165, avg: 0.3016, multirc_loss: 0.7069
03/25 02:35:15 PM: Evaluate: task multirc, batch 124 (152): ans_f1: 0.5869, qst_f1: 0.5565, em: 0.0164, avg: 0.3017, multirc_loss: 0.7068
03/25 02:35:35 PM: Evaluate: task multirc, batch 126 (152): ans_f1: 0.5864, qst_f1: 0.5560, em: 0.0162, avg: 0.3013, multirc_loss: 0.7068
03/25 02:35:51 PM: Evaluate: task multirc, batch 128 (152): ans_f1: 0.5852, qst_f1: 0.5552, em: 0.0160, avg: 0.3006, multirc_loss: 0.7070
03/25 02:36:06 PM: Evaluate: task multirc, batch 130 (152): ans_f1: 0.5861, qst_f1: 0.5568, em: 0.0169, avg: 0.3015, multirc_loss: 0.7071
03/25 02:36:24 PM: Evaluate: task multirc, batch 132 (152): ans_f1: 0.5861, qst_f1: 0.5574, em: 0.0179, avg: 0.3020, multirc_loss: 0.7072
03/25 02:36:43 PM: Evaluate: task multirc, batch 134 (152): ans_f1: 0.5869, qst_f1: 0.5580, em: 0.0164, avg: 0.3016, multirc_loss: 0.7071
03/25 02:36:53 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.5870, qst_f1: 0.5580, em: 0.0163, avg: 0.3016, multirc_loss: 0.7070
03/25 02:37:04 PM: Evaluate: task multirc, batch 136 (152): ans_f1: 0.5877, qst_f1: 0.5587, em: 0.0161, avg: 0.3019, multirc_loss: 0.7069
03/25 02:37:15 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.5873, qst_f1: 0.5584, em: 0.0161, avg: 0.3017, multirc_loss: 0.7070
03/25 02:37:31 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.5861, qst_f1: 0.5576, em: 0.0159, avg: 0.3010, multirc_loss: 0.7072
03/25 02:37:44 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.5853, qst_f1: 0.5573, em: 0.0158, avg: 0.3005, multirc_loss: 0.7072
03/25 02:38:02 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.5842, qst_f1: 0.5570, em: 0.0167, avg: 0.3004, multirc_loss: 0.7073
03/25 02:38:20 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.5834, qst_f1: 0.5565, em: 0.0164, avg: 0.2999, multirc_loss: 0.7073
03/25 02:38:37 PM: Evaluate: task multirc, batch 147 (152): ans_f1: 0.5844, qst_f1: 0.5577, em: 0.0162, avg: 0.3003, multirc_loss: 0.7073
03/25 02:38:47 PM: Evaluate: task multirc, batch 148 (152): ans_f1: 0.5840, qst_f1: 0.5568, em: 0.0172, avg: 0.3006, multirc_loss: 0.7073
03/25 02:39:07 PM: Evaluate: task multirc, batch 150 (152): ans_f1: 0.5841, qst_f1: 0.5572, em: 0.0159, avg: 0.3000, multirc_loss: 0.7072
03/25 02:39:22 PM: Evaluate: task multirc, batch 152 (152): ans_f1: 0.5869, qst_f1: 0.5605, em: 0.0157, avg: 0.3013, multirc_loss: 0.7065
03/25 02:39:22 PM: Best result seen so far for multirc.
03/25 02:39:22 PM: Best result seen so far for micro.
03/25 02:39:22 PM: Best result seen so far for macro.
03/25 02:39:22 PM: Updating LR scheduler:
03/25 02:39:22 PM: 	Best result seen so far for macro_avg: 0.301
03/25 02:39:22 PM: 	# validation passes without improvement: 0
03/25 02:39:22 PM: multirc_loss: training: 0.704279 validation: 0.706464
03/25 02:39:22 PM: macro_avg: validation: 0.301320
03/25 02:39:22 PM: micro_avg: validation: 0.301320
03/25 02:39:22 PM: multirc_ans_f1: training: 0.604651 validation: 0.586900
03/25 02:39:22 PM: multirc_qst_f1: training: 0.406250 validation: 0.560477
03/25 02:39:22 PM: multirc_em: training: 0.468750 validation: 0.015740
03/25 02:39:22 PM: multirc_avg: training: 0.536701 validation: 0.301320
03/25 02:39:22 PM: Global learning rate: 5e-06
03/25 02:39:22 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 02:40:14 PM: Update 8: task multirc, steps since last val 1 (total steps = 8): ans_f1: 0.5263, qst_f1: 0.3118, em: 0.4194, avg: 0.4728, multirc_loss: 0.7018
03/25 02:40:14 PM: ***** Step 8 / Validation 8 *****
03/25 02:40:14 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 02:40:14 PM: Validating...
03/25 02:40:31 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.6154, qst_f1: 0.6087, em: 0.0000, avg: 0.3077, multirc_loss: 0.7066
03/25 02:40:47 PM: Evaluate: task multirc, batch 4 (152): ans_f1: 0.6378, qst_f1: 0.6081, em: 0.0000, avg: 0.3189, multirc_loss: 0.7003
03/25 02:41:06 PM: Evaluate: task multirc, batch 6 (152): ans_f1: 0.6418, qst_f1: 0.6201, em: 0.0455, avg: 0.3436, multirc_loss: 0.6958
03/25 02:41:16 PM: Evaluate: task multirc, batch 7 (152): ans_f1: 0.6275, qst_f1: 0.5897, em: 0.0377, avg: 0.3326, multirc_loss: 0.6978
03/25 02:41:34 PM: Evaluate: task multirc, batch 9 (152): ans_f1: 0.6146, qst_f1: 0.5857, em: 0.0735, avg: 0.3441, multirc_loss: 0.6992
03/25 02:41:51 PM: Evaluate: task multirc, batch 11 (152): ans_f1: 0.6108, qst_f1: 0.5821, em: 0.0506, avg: 0.3307, multirc_loss: 0.6987
03/25 02:42:07 PM: Evaluate: task multirc, batch 13 (152): ans_f1: 0.6065, qst_f1: 0.5753, em: 0.0532, avg: 0.3298, multirc_loss: 0.6998
03/25 02:42:21 PM: Evaluate: task multirc, batch 15 (152): ans_f1: 0.5948, qst_f1: 0.5468, em: 0.0476, avg: 0.3212, multirc_loss: 0.6981
03/25 02:42:40 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.5922, qst_f1: 0.5469, em: 0.0427, avg: 0.3174, multirc_loss: 0.6989
03/25 02:42:58 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.5845, qst_f1: 0.5442, em: 0.0465, avg: 0.3155, multirc_loss: 0.6999
03/25 02:43:08 PM: Evaluate: task multirc, batch 20 (152): ans_f1: 0.5814, qst_f1: 0.5437, em: 0.0448, avg: 0.3131, multirc_loss: 0.7000
03/25 02:43:23 PM: Evaluate: task multirc, batch 22 (152): ans_f1: 0.5859, qst_f1: 0.5489, em: 0.0420, avg: 0.3139, multirc_loss: 0.6997
03/25 02:43:42 PM: Evaluate: task multirc, batch 24 (152): ans_f1: 0.5821, qst_f1: 0.5457, em: 0.0392, avg: 0.3107, multirc_loss: 0.6992
03/25 02:43:58 PM: Evaluate: task multirc, batch 26 (152): ans_f1: 0.5756, qst_f1: 0.5368, em: 0.0370, avg: 0.3063, multirc_loss: 0.6993
03/25 02:44:15 PM: Evaluate: task multirc, batch 28 (152): ans_f1: 0.5853, qst_f1: 0.5468, em: 0.0349, avg: 0.3101, multirc_loss: 0.6985
03/25 02:44:33 PM: Evaluate: task multirc, batch 30 (152): ans_f1: 0.5930, qst_f1: 0.5525, em: 0.0378, avg: 0.3154, multirc_loss: 0.6973
03/25 02:44:45 PM: Evaluate: task multirc, batch 31 (152): ans_f1: 0.5880, qst_f1: 0.5483, em: 0.0365, avg: 0.3122, multirc_loss: 0.6984
03/25 02:44:58 PM: Evaluate: task multirc, batch 32 (152): ans_f1: 0.5859, qst_f1: 0.5449, em: 0.0352, avg: 0.3105, multirc_loss: 0.6986
03/25 02:45:17 PM: Evaluate: task multirc, batch 34 (152): ans_f1: 0.5754, qst_f1: 0.5244, em: 0.0377, avg: 0.3066, multirc_loss: 0.6982
03/25 02:45:35 PM: Evaluate: task multirc, batch 36 (152): ans_f1: 0.5700, qst_f1: 0.5211, em: 0.0315, avg: 0.3008, multirc_loss: 0.6982
03/25 02:45:53 PM: Evaluate: task multirc, batch 38 (152): ans_f1: 0.5693, qst_f1: 0.5198, em: 0.0299, avg: 0.2996, multirc_loss: 0.6983
03/25 02:46:03 PM: Evaluate: task multirc, batch 39 (152): ans_f1: 0.5683, qst_f1: 0.5192, em: 0.0294, avg: 0.2988, multirc_loss: 0.6982
03/25 02:46:21 PM: Evaluate: task multirc, batch 41 (152): ans_f1: 0.5673, qst_f1: 0.5163, em: 0.0283, avg: 0.2978, multirc_loss: 0.6977
03/25 02:46:38 PM: Evaluate: task multirc, batch 43 (152): ans_f1: 0.5637, qst_f1: 0.5032, em: 0.0269, avg: 0.2953, multirc_loss: 0.6974
03/25 02:46:56 PM: Evaluate: task multirc, batch 45 (152): ans_f1: 0.5626, qst_f1: 0.5019, em: 0.0257, avg: 0.2942, multirc_loss: 0.6972
03/25 02:47:13 PM: Evaluate: task multirc, batch 47 (152): ans_f1: 0.5676, qst_f1: 0.5074, em: 0.0312, avg: 0.2994, multirc_loss: 0.6967
03/25 02:47:33 PM: Evaluate: task multirc, batch 49 (152): ans_f1: 0.5689, qst_f1: 0.5086, em: 0.0299, avg: 0.2994, multirc_loss: 0.6975
03/25 02:47:44 PM: Evaluate: task multirc, batch 50 (152): ans_f1: 0.5673, qst_f1: 0.5098, em: 0.0323, avg: 0.2998, multirc_loss: 0.6979
03/25 02:48:03 PM: Evaluate: task multirc, batch 52 (152): ans_f1: 0.5590, qst_f1: 0.5026, em: 0.0310, avg: 0.2950, multirc_loss: 0.6998
03/25 02:48:16 PM: Evaluate: task multirc, batch 53 (152): ans_f1: 0.5578, qst_f1: 0.5021, em: 0.0304, avg: 0.2941, multirc_loss: 0.6999
03/25 02:48:26 PM: Evaluate: task multirc, batch 54 (152): ans_f1: 0.5547, qst_f1: 0.4992, em: 0.0299, avg: 0.2923, multirc_loss: 0.7002
03/25 02:48:37 PM: Evaluate: task multirc, batch 55 (152): ans_f1: 0.5571, qst_f1: 0.5001, em: 0.0294, avg: 0.2933, multirc_loss: 0.6996
03/25 02:48:55 PM: Evaluate: task multirc, batch 57 (152): ans_f1: 0.5561, qst_f1: 0.5007, em: 0.0283, avg: 0.2922, multirc_loss: 0.7000
03/25 02:49:07 PM: Evaluate: task multirc, batch 58 (152): ans_f1: 0.5555, qst_f1: 0.4998, em: 0.0279, avg: 0.2917, multirc_loss: 0.7004
03/25 02:49:18 PM: Evaluate: task multirc, batch 59 (152): ans_f1: 0.5569, qst_f1: 0.5005, em: 0.0298, avg: 0.2934, multirc_loss: 0.7001
03/25 02:49:37 PM: Evaluate: task multirc, batch 61 (152): ans_f1: 0.5544, qst_f1: 0.4964, em: 0.0289, avg: 0.2916, multirc_loss: 0.6995
03/25 02:49:52 PM: Evaluate: task multirc, batch 63 (152): ans_f1: 0.5554, qst_f1: 0.4972, em: 0.0279, avg: 0.2917, multirc_loss: 0.6992
03/25 02:50:09 PM: Evaluate: task multirc, batch 65 (152): ans_f1: 0.5604, qst_f1: 0.5041, em: 0.0291, avg: 0.2947, multirc_loss: 0.6983
03/25 02:50:20 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.5612, qst_f1: 0.5054, em: 0.0263, avg: 0.2937, multirc_loss: 0.6983
03/25 02:50:30 PM: Evaluate: task multirc, batch 67 (152): ans_f1: 0.5615, qst_f1: 0.5065, em: 0.0282, avg: 0.2949, multirc_loss: 0.6985
03/25 02:50:50 PM: Evaluate: task multirc, batch 69 (152): ans_f1: 0.5625, qst_f1: 0.5081, em: 0.0251, avg: 0.2938, multirc_loss: 0.6989
03/25 02:51:00 PM: Evaluate: task multirc, batch 70 (152): ans_f1: 0.5628, qst_f1: 0.5090, em: 0.0248, avg: 0.2938, multirc_loss: 0.6993
03/25 02:51:15 PM: Evaluate: task multirc, batch 72 (152): ans_f1: 0.5634, qst_f1: 0.5104, em: 0.0242, avg: 0.2938, multirc_loss: 0.6997
03/25 02:51:34 PM: Evaluate: task multirc, batch 74 (152): ans_f1: 0.5621, qst_f1: 0.5089, em: 0.0237, avg: 0.2929, multirc_loss: 0.7001
03/25 02:51:52 PM: Evaluate: task multirc, batch 76 (152): ans_f1: 0.5625, qst_f1: 0.5113, em: 0.0231, avg: 0.2928, multirc_loss: 0.7003
03/25 02:52:08 PM: Evaluate: task multirc, batch 78 (152): ans_f1: 0.5620, qst_f1: 0.5120, em: 0.0225, avg: 0.2922, multirc_loss: 0.7007
03/25 02:52:25 PM: Evaluate: task multirc, batch 80 (152): ans_f1: 0.5613, qst_f1: 0.5120, em: 0.0220, avg: 0.2916, multirc_loss: 0.7009
03/25 02:52:41 PM: Evaluate: task multirc, batch 82 (152): ans_f1: 0.5601, qst_f1: 0.5068, em: 0.0252, avg: 0.2926, multirc_loss: 0.7006
03/25 02:52:58 PM: Evaluate: task multirc, batch 84 (152): ans_f1: 0.5587, qst_f1: 0.5041, em: 0.0227, avg: 0.2907, multirc_loss: 0.7005
03/25 02:53:09 PM: Evaluate: task multirc, batch 85 (152): ans_f1: 0.5581, qst_f1: 0.5032, em: 0.0225, avg: 0.2903, multirc_loss: 0.7006
03/25 02:53:21 PM: Evaluate: task multirc, batch 86 (152): ans_f1: 0.5574, qst_f1: 0.5031, em: 0.0223, avg: 0.2898, multirc_loss: 0.7008
03/25 02:53:31 PM: Evaluate: task multirc, batch 87 (152): ans_f1: 0.5577, qst_f1: 0.5034, em: 0.0221, avg: 0.2899, multirc_loss: 0.7006
03/25 02:53:42 PM: Evaluate: task multirc, batch 88 (152): ans_f1: 0.5613, qst_f1: 0.5068, em: 0.0236, avg: 0.2925, multirc_loss: 0.7004
03/25 02:53:52 PM: Evaluate: task multirc, batch 89 (152): ans_f1: 0.5648, qst_f1: 0.5100, em: 0.0215, avg: 0.2932, multirc_loss: 0.7001
03/25 02:54:06 PM: Evaluate: task multirc, batch 91 (152): ans_f1: 0.5670, qst_f1: 0.5117, em: 0.0212, avg: 0.2941, multirc_loss: 0.6998
03/25 02:54:24 PM: Evaluate: task multirc, batch 93 (152): ans_f1: 0.5647, qst_f1: 0.5102, em: 0.0207, avg: 0.2927, multirc_loss: 0.7003
03/25 02:54:34 PM: Evaluate: task multirc, batch 94 (152): ans_f1: 0.5652, qst_f1: 0.5109, em: 0.0205, avg: 0.2929, multirc_loss: 0.7004
03/25 02:54:45 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.5659, qst_f1: 0.5120, em: 0.0203, avg: 0.2931, multirc_loss: 0.7004
03/25 02:54:56 PM: Evaluate: task multirc, batch 96 (152): ans_f1: 0.5676, qst_f1: 0.5135, em: 0.0201, avg: 0.2939, multirc_loss: 0.7003
03/25 02:55:15 PM: Evaluate: task multirc, batch 98 (152): ans_f1: 0.5702, qst_f1: 0.5163, em: 0.0293, avg: 0.2997, multirc_loss: 0.6999
03/25 02:55:34 PM: Evaluate: task multirc, batch 100 (152): ans_f1: 0.5716, qst_f1: 0.5197, em: 0.0271, avg: 0.2993, multirc_loss: 0.6999
03/25 02:55:47 PM: Evaluate: task multirc, batch 102 (152): ans_f1: 0.5717, qst_f1: 0.5139, em: 0.0263, avg: 0.2990, multirc_loss: 0.6998
03/25 02:56:03 PM: Evaluate: task multirc, batch 104 (152): ans_f1: 0.5682, qst_f1: 0.5066, em: 0.0257, avg: 0.2970, multirc_loss: 0.7001
03/25 02:56:20 PM: Evaluate: task multirc, batch 106 (152): ans_f1: 0.5670, qst_f1: 0.5061, em: 0.0253, avg: 0.2961, multirc_loss: 0.7001
03/25 02:56:37 PM: Evaluate: task multirc, batch 108 (152): ans_f1: 0.5683, qst_f1: 0.5086, em: 0.0246, avg: 0.2964, multirc_loss: 0.7002
03/25 02:56:55 PM: Evaluate: task multirc, batch 110 (152): ans_f1: 0.5703, qst_f1: 0.5107, em: 0.0255, avg: 0.2979, multirc_loss: 0.7000
03/25 02:57:09 PM: Evaluate: task multirc, batch 112 (152): ans_f1: 0.5737, qst_f1: 0.5147, em: 0.0250, avg: 0.2993, multirc_loss: 0.6997
03/25 02:57:29 PM: Evaluate: task multirc, batch 114 (152): ans_f1: 0.5725, qst_f1: 0.5139, em: 0.0245, avg: 0.2985, multirc_loss: 0.6997
03/25 02:57:39 PM: Evaluate: task multirc, batch 115 (152): ans_f1: 0.5731, qst_f1: 0.5168, em: 0.0269, avg: 0.3000, multirc_loss: 0.6996
03/25 02:57:56 PM: Evaluate: task multirc, batch 117 (152): ans_f1: 0.5730, qst_f1: 0.5161, em: 0.0277, avg: 0.3004, multirc_loss: 0.6994
03/25 02:58:15 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.5731, qst_f1: 0.5173, em: 0.0273, avg: 0.3002, multirc_loss: 0.6992
03/25 02:58:25 PM: Evaluate: task multirc, batch 120 (152): ans_f1: 0.5728, qst_f1: 0.5165, em: 0.0271, avg: 0.3000, multirc_loss: 0.6992
03/25 02:58:45 PM: Evaluate: task multirc, batch 122 (152): ans_f1: 0.5717, qst_f1: 0.5166, em: 0.0255, avg: 0.2986, multirc_loss: 0.6994
03/25 02:59:04 PM: Evaluate: task multirc, batch 124 (152): ans_f1: 0.5725, qst_f1: 0.5171, em: 0.0252, avg: 0.2989, multirc_loss: 0.6992
03/25 02:59:25 PM: Evaluate: task multirc, batch 126 (152): ans_f1: 0.5729, qst_f1: 0.5170, em: 0.0249, avg: 0.2989, multirc_loss: 0.6991
03/25 02:59:41 PM: Evaluate: task multirc, batch 128 (152): ans_f1: 0.5723, qst_f1: 0.5166, em: 0.0246, avg: 0.2985, multirc_loss: 0.6993
03/25 02:59:56 PM: Evaluate: task multirc, batch 130 (152): ans_f1: 0.5735, qst_f1: 0.5188, em: 0.0254, avg: 0.2995, multirc_loss: 0.6994
03/25 03:00:13 PM: Evaluate: task multirc, batch 132 (152): ans_f1: 0.5736, qst_f1: 0.5190, em: 0.0263, avg: 0.2999, multirc_loss: 0.6995
03/25 03:00:31 PM: Evaluate: task multirc, batch 134 (152): ans_f1: 0.5745, qst_f1: 0.5203, em: 0.0269, avg: 0.3007, multirc_loss: 0.6994
03/25 03:00:42 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.5747, qst_f1: 0.5204, em: 0.0267, avg: 0.3007, multirc_loss: 0.6993
03/25 03:00:52 PM: Evaluate: task multirc, batch 136 (152): ans_f1: 0.5753, qst_f1: 0.5210, em: 0.0265, avg: 0.3009, multirc_loss: 0.6993
03/25 03:01:04 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.5746, qst_f1: 0.5205, em: 0.0264, avg: 0.3005, multirc_loss: 0.6992
03/25 03:01:20 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.5739, qst_f1: 0.5203, em: 0.0261, avg: 0.3000, multirc_loss: 0.6994
03/25 03:01:33 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.5736, qst_f1: 0.5205, em: 0.0259, avg: 0.2997, multirc_loss: 0.6994
03/25 03:01:50 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.5729, qst_f1: 0.5200, em: 0.0256, avg: 0.2992, multirc_loss: 0.6994
03/25 03:02:08 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.5725, qst_f1: 0.5191, em: 0.0252, avg: 0.2988, multirc_loss: 0.6992
03/25 03:02:25 PM: Evaluate: task multirc, batch 147 (152): ans_f1: 0.5726, qst_f1: 0.5190, em: 0.0248, avg: 0.2987, multirc_loss: 0.6992
03/25 03:02:36 PM: Evaluate: task multirc, batch 148 (152): ans_f1: 0.5718, qst_f1: 0.5174, em: 0.0258, avg: 0.2988, multirc_loss: 0.6991
03/25 03:02:55 PM: Evaluate: task multirc, batch 150 (152): ans_f1: 0.5720, qst_f1: 0.5180, em: 0.0244, avg: 0.2982, multirc_loss: 0.6991
03/25 03:03:09 PM: Evaluate: task multirc, batch 152 (152): ans_f1: 0.5742, qst_f1: 0.5203, em: 0.0262, avg: 0.3002, multirc_loss: 0.6986
03/25 03:03:09 PM: Updating LR scheduler:
03/25 03:03:09 PM: 	Best result seen so far for macro_avg: 0.301
03/25 03:03:09 PM: 	# validation passes without improvement: 1
03/25 03:03:09 PM: multirc_loss: training: 0.701761 validation: 0.698591
03/25 03:03:09 PM: macro_avg: validation: 0.300217
03/25 03:03:09 PM: micro_avg: validation: 0.300217
03/25 03:03:09 PM: multirc_ans_f1: training: 0.526316 validation: 0.574201
03/25 03:03:09 PM: multirc_qst_f1: training: 0.311828 validation: 0.520346
03/25 03:03:09 PM: multirc_em: training: 0.419355 validation: 0.026233
03/25 03:03:09 PM: multirc_avg: training: 0.472835 validation: 0.300217
03/25 03:03:09 PM: Global learning rate: 5e-06
03/25 03:03:09 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 03:04:04 PM: Update 9: task multirc, steps since last val 1 (total steps = 9): ans_f1: 0.7805, qst_f1: 0.5000, em: 0.7188, avg: 0.7496, multirc_loss: 0.6437
03/25 03:04:04 PM: ***** Step 9 / Validation 9 *****
03/25 03:04:04 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 03:04:04 PM: Validating...
03/25 03:04:25 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.6222, qst_f1: 0.6160, em: 0.0000, avg: 0.3111, multirc_loss: 0.7019
03/25 03:04:41 PM: Evaluate: task multirc, batch 4 (152): ans_f1: 0.6292, qst_f1: 0.5665, em: 0.0345, avg: 0.3318, multirc_loss: 0.6975
03/25 03:04:59 PM: Evaluate: task multirc, batch 6 (152): ans_f1: 0.6235, qst_f1: 0.5534, em: 0.0455, avg: 0.3345, multirc_loss: 0.6922
03/25 03:05:18 PM: Evaluate: task multirc, batch 8 (152): ans_f1: 0.5968, qst_f1: 0.4987, em: 0.0323, avg: 0.3145, multirc_loss: 0.6946
03/25 03:05:36 PM: Evaluate: task multirc, batch 10 (152): ans_f1: 0.5767, qst_f1: 0.4889, em: 0.0278, avg: 0.3022, multirc_loss: 0.6975
03/25 03:05:52 PM: Evaluate: task multirc, batch 12 (152): ans_f1: 0.5770, qst_f1: 0.5020, em: 0.0233, avg: 0.3001, multirc_loss: 0.6983
03/25 03:06:05 PM: Evaluate: task multirc, batch 14 (152): ans_f1: 0.5655, qst_f1: 0.4818, em: 0.0200, avg: 0.2928, multirc_loss: 0.6983
03/25 03:06:20 PM: Evaluate: task multirc, batch 16 (152): ans_f1: 0.5508, qst_f1: 0.4627, em: 0.0182, avg: 0.2845, multirc_loss: 0.6968
03/25 03:06:30 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.5539, qst_f1: 0.4717, em: 0.0256, avg: 0.2898, multirc_loss: 0.6968
03/25 03:06:48 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.5499, qst_f1: 0.4747, em: 0.0233, avg: 0.2866, multirc_loss: 0.6971
03/25 03:07:05 PM: Evaluate: task multirc, batch 21 (152): ans_f1: 0.5569, qst_f1: 0.4778, em: 0.0216, avg: 0.2892, multirc_loss: 0.6965
03/25 03:07:22 PM: Evaluate: task multirc, batch 23 (152): ans_f1: 0.5573, qst_f1: 0.4806, em: 0.0203, avg: 0.2888, multirc_loss: 0.6964
03/25 03:07:40 PM: Evaluate: task multirc, batch 25 (152): ans_f1: 0.5497, qst_f1: 0.4715, em: 0.0190, avg: 0.2843, multirc_loss: 0.6958
03/25 03:07:59 PM: Evaluate: task multirc, batch 27 (152): ans_f1: 0.5528, qst_f1: 0.4774, em: 0.0180, avg: 0.2854, multirc_loss: 0.6953
03/25 03:08:09 PM: Evaluate: task multirc, batch 28 (152): ans_f1: 0.5600, qst_f1: 0.4809, em: 0.0174, avg: 0.2887, multirc_loss: 0.6946
03/25 03:08:21 PM: Evaluate: task multirc, batch 29 (152): ans_f1: 0.5693, qst_f1: 0.4887, em: 0.0281, avg: 0.2987, multirc_loss: 0.6937
03/25 03:08:39 PM: Evaluate: task multirc, batch 31 (152): ans_f1: 0.5640, qst_f1: 0.4825, em: 0.0208, avg: 0.2924, multirc_loss: 0.6946
03/25 03:08:51 PM: Evaluate: task multirc, batch 32 (152): ans_f1: 0.5620, qst_f1: 0.4790, em: 0.0201, avg: 0.2911, multirc_loss: 0.6946
03/25 03:09:01 PM: Evaluate: task multirc, batch 33 (152): ans_f1: 0.5615, qst_f1: 0.4780, em: 0.0243, avg: 0.2929, multirc_loss: 0.6940
03/25 03:09:18 PM: Evaluate: task multirc, batch 35 (152): ans_f1: 0.5543, qst_f1: 0.4697, em: 0.0231, avg: 0.2887, multirc_loss: 0.6932
03/25 03:09:35 PM: Evaluate: task multirc, batch 37 (152): ans_f1: 0.5534, qst_f1: 0.4624, em: 0.0218, avg: 0.2876, multirc_loss: 0.6923
03/25 03:09:54 PM: Evaluate: task multirc, batch 39 (152): ans_f1: 0.5492, qst_f1: 0.4584, em: 0.0210, avg: 0.2851, multirc_loss: 0.6925
03/25 03:10:10 PM: Evaluate: task multirc, batch 41 (152): ans_f1: 0.5504, qst_f1: 0.4592, em: 0.0243, avg: 0.2873, multirc_loss: 0.6920
03/25 03:10:27 PM: Evaluate: task multirc, batch 43 (152): ans_f1: 0.5460, qst_f1: 0.4463, em: 0.0192, avg: 0.2826, multirc_loss: 0.6915
03/25 03:10:43 PM: Evaluate: task multirc, batch 45 (152): ans_f1: 0.5404, qst_f1: 0.4412, em: 0.0184, avg: 0.2794, multirc_loss: 0.6918
03/25 03:10:59 PM: Evaluate: task multirc, batch 47 (152): ans_f1: 0.5415, qst_f1: 0.4394, em: 0.0208, avg: 0.2812, multirc_loss: 0.6914
03/25 03:11:17 PM: Evaluate: task multirc, batch 49 (152): ans_f1: 0.5434, qst_f1: 0.4428, em: 0.0199, avg: 0.2817, multirc_loss: 0.6921
03/25 03:11:34 PM: Evaluate: task multirc, batch 51 (152): ans_f1: 0.5379, qst_f1: 0.4412, em: 0.0221, avg: 0.2800, multirc_loss: 0.6929
03/25 03:11:54 PM: Evaluate: task multirc, batch 53 (152): ans_f1: 0.5326, qst_f1: 0.4391, em: 0.0213, avg: 0.2769, multirc_loss: 0.6938
03/25 03:12:14 PM: Evaluate: task multirc, batch 55 (152): ans_f1: 0.5324, qst_f1: 0.4388, em: 0.0206, avg: 0.2765, multirc_loss: 0.6931
03/25 03:12:32 PM: Evaluate: task multirc, batch 57 (152): ans_f1: 0.5319, qst_f1: 0.4396, em: 0.0198, avg: 0.2759, multirc_loss: 0.6932
03/25 03:12:51 PM: Evaluate: task multirc, batch 59 (152): ans_f1: 0.5308, qst_f1: 0.4364, em: 0.0190, avg: 0.2749, multirc_loss: 0.6934
03/25 03:13:09 PM: Evaluate: task multirc, batch 61 (152): ans_f1: 0.5279, qst_f1: 0.4290, em: 0.0210, avg: 0.2744, multirc_loss: 0.6924
03/25 03:13:24 PM: Evaluate: task multirc, batch 63 (152): ans_f1: 0.5291, qst_f1: 0.4306, em: 0.0203, avg: 0.2747, multirc_loss: 0.6923
03/25 03:13:40 PM: Evaluate: task multirc, batch 65 (152): ans_f1: 0.5363, qst_f1: 0.4418, em: 0.0242, avg: 0.2802, multirc_loss: 0.6916
03/25 03:13:52 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.5385, qst_f1: 0.4450, em: 0.0239, avg: 0.2812, multirc_loss: 0.6916
03/25 03:14:09 PM: Evaluate: task multirc, batch 68 (152): ans_f1: 0.5397, qst_f1: 0.4466, em: 0.0231, avg: 0.2814, multirc_loss: 0.6918
03/25 03:14:20 PM: Evaluate: task multirc, batch 69 (152): ans_f1: 0.5392, qst_f1: 0.4486, em: 0.0228, avg: 0.2810, multirc_loss: 0.6921
03/25 03:14:30 PM: Evaluate: task multirc, batch 70 (152): ans_f1: 0.5382, qst_f1: 0.4492, em: 0.0226, avg: 0.2804, multirc_loss: 0.6925
03/25 03:14:46 PM: Evaluate: task multirc, batch 72 (152): ans_f1: 0.5389, qst_f1: 0.4515, em: 0.0220, avg: 0.2805, multirc_loss: 0.6929
03/25 03:15:03 PM: Evaluate: task multirc, batch 74 (152): ans_f1: 0.5391, qst_f1: 0.4516, em: 0.0215, avg: 0.2803, multirc_loss: 0.6932
03/25 03:15:20 PM: Evaluate: task multirc, batch 76 (152): ans_f1: 0.5413, qst_f1: 0.4571, em: 0.0252, avg: 0.2832, multirc_loss: 0.6934
03/25 03:15:34 PM: Evaluate: task multirc, batch 78 (152): ans_f1: 0.5412, qst_f1: 0.4589, em: 0.0245, avg: 0.2828, multirc_loss: 0.6937
03/25 03:15:50 PM: Evaluate: task multirc, batch 80 (152): ans_f1: 0.5397, qst_f1: 0.4581, em: 0.0259, avg: 0.2828, multirc_loss: 0.6939
03/25 03:16:04 PM: Evaluate: task multirc, batch 82 (152): ans_f1: 0.5364, qst_f1: 0.4501, em: 0.0272, avg: 0.2818, multirc_loss: 0.6936
03/25 03:16:19 PM: Evaluate: task multirc, batch 84 (152): ans_f1: 0.5346, qst_f1: 0.4482, em: 0.0246, avg: 0.2796, multirc_loss: 0.6936
03/25 03:16:39 PM: Evaluate: task multirc, batch 86 (152): ans_f1: 0.5337, qst_f1: 0.4473, em: 0.0242, avg: 0.2789, multirc_loss: 0.6937
03/25 03:16:58 PM: Evaluate: task multirc, batch 88 (152): ans_f1: 0.5371, qst_f1: 0.4512, em: 0.0273, avg: 0.2822, multirc_loss: 0.6934
03/25 03:17:15 PM: Evaluate: task multirc, batch 90 (152): ans_f1: 0.5428, qst_f1: 0.4550, em: 0.0267, avg: 0.2848, multirc_loss: 0.6931
03/25 03:17:27 PM: Evaluate: task multirc, batch 92 (152): ans_f1: 0.5418, qst_f1: 0.4549, em: 0.0262, avg: 0.2840, multirc_loss: 0.6933
03/25 03:17:46 PM: Evaluate: task multirc, batch 94 (152): ans_f1: 0.5418, qst_f1: 0.4563, em: 0.0256, avg: 0.2837, multirc_loss: 0.6936
03/25 03:17:56 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.5426, qst_f1: 0.4577, em: 0.0254, avg: 0.2840, multirc_loss: 0.6935
03/25 03:18:14 PM: Evaluate: task multirc, batch 97 (152): ans_f1: 0.5455, qst_f1: 0.4609, em: 0.0265, avg: 0.2860, multirc_loss: 0.6934
03/25 03:18:34 PM: Evaluate: task multirc, batch 99 (152): ans_f1: 0.5455, qst_f1: 0.4600, em: 0.0305, avg: 0.2880, multirc_loss: 0.6934
03/25 03:18:48 PM: Evaluate: task multirc, batch 101 (152): ans_f1: 0.5514, qst_f1: 0.4669, em: 0.0299, avg: 0.2906, multirc_loss: 0.6931
03/25 03:19:01 PM: Evaluate: task multirc, batch 103 (152): ans_f1: 0.5454, qst_f1: 0.4526, em: 0.0290, avg: 0.2872, multirc_loss: 0.6936
03/25 03:19:18 PM: Evaluate: task multirc, batch 105 (152): ans_f1: 0.5439, qst_f1: 0.4520, em: 0.0285, avg: 0.2862, multirc_loss: 0.6937
03/25 03:19:34 PM: Evaluate: task multirc, batch 107 (152): ans_f1: 0.5440, qst_f1: 0.4531, em: 0.0279, avg: 0.2860, multirc_loss: 0.6939
03/25 03:19:50 PM: Evaluate: task multirc, batch 109 (152): ans_f1: 0.5464, qst_f1: 0.4572, em: 0.0328, avg: 0.2896, multirc_loss: 0.6937
03/25 03:20:03 PM: Evaluate: task multirc, batch 111 (152): ans_f1: 0.5493, qst_f1: 0.4609, em: 0.0322, avg: 0.2908, multirc_loss: 0.6937
03/25 03:20:19 PM: Evaluate: task multirc, batch 113 (152): ans_f1: 0.5515, qst_f1: 0.4634, em: 0.0316, avg: 0.2916, multirc_loss: 0.6935
03/25 03:20:37 PM: Evaluate: task multirc, batch 115 (152): ans_f1: 0.5521, qst_f1: 0.4661, em: 0.0336, avg: 0.2929, multirc_loss: 0.6933
03/25 03:20:53 PM: Evaluate: task multirc, batch 117 (152): ans_f1: 0.5512, qst_f1: 0.4637, em: 0.0369, avg: 0.2941, multirc_loss: 0.6930
03/25 03:21:11 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.5510, qst_f1: 0.4643, em: 0.0365, avg: 0.2937, multirc_loss: 0.6928
03/25 03:21:29 PM: Evaluate: task multirc, batch 121 (152): ans_f1: 0.5509, qst_f1: 0.4651, em: 0.0346, avg: 0.2928, multirc_loss: 0.6928
03/25 03:21:47 PM: Evaluate: task multirc, batch 123 (152): ans_f1: 0.5506, qst_f1: 0.4649, em: 0.0342, avg: 0.2924, multirc_loss: 0.6927
03/25 03:22:06 PM: Evaluate: task multirc, batch 125 (152): ans_f1: 0.5507, qst_f1: 0.4641, em: 0.0339, avg: 0.2923, multirc_loss: 0.6924
03/25 03:22:24 PM: Evaluate: task multirc, batch 127 (152): ans_f1: 0.5504, qst_f1: 0.4631, em: 0.0335, avg: 0.2919, multirc_loss: 0.6922
03/25 03:22:36 PM: Evaluate: task multirc, batch 129 (152): ans_f1: 0.5499, qst_f1: 0.4637, em: 0.0330, avg: 0.2915, multirc_loss: 0.6928
03/25 03:22:54 PM: Evaluate: task multirc, batch 131 (152): ans_f1: 0.5507, qst_f1: 0.4651, em: 0.0337, avg: 0.2922, multirc_loss: 0.6929
03/25 03:23:10 PM: Evaluate: task multirc, batch 133 (152): ans_f1: 0.5515, qst_f1: 0.4664, em: 0.0366, avg: 0.2941, multirc_loss: 0.6928
03/25 03:23:28 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.5527, qst_f1: 0.4676, em: 0.0372, avg: 0.2949, multirc_loss: 0.6926
03/25 03:23:48 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.5530, qst_f1: 0.4675, em: 0.0367, avg: 0.2949, multirc_loss: 0.6925
03/25 03:24:03 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.5524, qst_f1: 0.4673, em: 0.0364, avg: 0.2944, multirc_loss: 0.6926
03/25 03:24:15 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.5529, qst_f1: 0.4683, em: 0.0360, avg: 0.2945, multirc_loss: 0.6926
03/25 03:24:32 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.5520, qst_f1: 0.4673, em: 0.0356, avg: 0.2938, multirc_loss: 0.6925
03/25 03:24:49 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.5520, qst_f1: 0.4666, em: 0.0351, avg: 0.2935, multirc_loss: 0.6922
03/25 03:25:05 PM: Evaluate: task multirc, batch 147 (152): ans_f1: 0.5525, qst_f1: 0.4671, em: 0.0346, avg: 0.2935, multirc_loss: 0.6921
03/25 03:25:23 PM: Evaluate: task multirc, batch 149 (152): ans_f1: 0.5516, qst_f1: 0.4659, em: 0.0352, avg: 0.2934, multirc_loss: 0.6921
03/25 03:25:41 PM: Evaluate: task multirc, batch 151 (152): ans_f1: 0.5524, qst_f1: 0.4673, em: 0.0348, avg: 0.2936, multirc_loss: 0.6919
03/25 03:25:46 PM: Updating LR scheduler:
03/25 03:25:46 PM: 	Best result seen so far for macro_avg: 0.301
03/25 03:25:46 PM: 	# validation passes without improvement: 2
03/25 03:25:46 PM: multirc_loss: training: 0.643676 validation: 0.691751
03/25 03:25:46 PM: macro_avg: validation: 0.294503
03/25 03:25:46 PM: micro_avg: validation: 0.294503
03/25 03:25:46 PM: multirc_ans_f1: training: 0.780488 validation: 0.553328
03/25 03:25:46 PM: multirc_qst_f1: training: 0.500000 validation: 0.467680
03/25 03:25:46 PM: multirc_em: training: 0.718750 validation: 0.035677
03/25 03:25:46 PM: multirc_avg: training: 0.749619 validation: 0.294503
03/25 03:25:46 PM: Global learning rate: 5e-06
03/25 03:25:46 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 03:26:32 PM: Update 10: task multirc, steps since last val 1 (total steps = 10): ans_f1: 0.5600, qst_f1: 0.2151, em: 0.6452, avg: 0.6026, multirc_loss: 0.6936
03/25 03:26:32 PM: ***** Step 10 / Validation 10 *****
03/25 03:26:32 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 03:26:32 PM: Validating...
03/25 03:26:49 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.6133, qst_f1: 0.5611, em: 0.0000, avg: 0.3067, multirc_loss: 0.6900
03/25 03:27:05 PM: Evaluate: task multirc, batch 4 (152): ans_f1: 0.5844, qst_f1: 0.4808, em: 0.0345, avg: 0.3094, multirc_loss: 0.6928
03/25 03:27:23 PM: Evaluate: task multirc, batch 6 (152): ans_f1: 0.5742, qst_f1: 0.4574, em: 0.0227, avg: 0.2984, multirc_loss: 0.6874
03/25 03:27:34 PM: Evaluate: task multirc, batch 7 (152): ans_f1: 0.5277, qst_f1: 0.3867, em: 0.0189, avg: 0.2733, multirc_loss: 0.6938
03/25 03:27:54 PM: Evaluate: task multirc, batch 9 (152): ans_f1: 0.5053, qst_f1: 0.3676, em: 0.0147, avg: 0.2600, multirc_loss: 0.6922
03/25 03:28:09 PM: Evaluate: task multirc, batch 11 (152): ans_f1: 0.4667, qst_f1: 0.3407, em: 0.0127, avg: 0.2397, multirc_loss: 0.6968
03/25 03:28:26 PM: Evaluate: task multirc, batch 13 (152): ans_f1: 0.4501, qst_f1: 0.3160, em: 0.0106, avg: 0.2304, multirc_loss: 0.6984
03/25 03:28:39 PM: Evaluate: task multirc, batch 15 (152): ans_f1: 0.4196, qst_f1: 0.2898, em: 0.0190, avg: 0.2193, multirc_loss: 0.6980
03/25 03:29:01 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.4254, qst_f1: 0.2999, em: 0.0171, avg: 0.2212, multirc_loss: 0.6964
03/25 03:29:19 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.4323, qst_f1: 0.3130, em: 0.0310, avg: 0.2317, multirc_loss: 0.6941
03/25 03:29:36 PM: Evaluate: task multirc, batch 21 (152): ans_f1: 0.4325, qst_f1: 0.3148, em: 0.0360, avg: 0.2342, multirc_loss: 0.6926
03/25 03:29:52 PM: Evaluate: task multirc, batch 23 (152): ans_f1: 0.4377, qst_f1: 0.3201, em: 0.0338, avg: 0.2357, multirc_loss: 0.6920
03/25 03:30:10 PM: Evaluate: task multirc, batch 25 (152): ans_f1: 0.4241, qst_f1: 0.2998, em: 0.0316, avg: 0.2279, multirc_loss: 0.6900
03/25 03:30:26 PM: Evaluate: task multirc, batch 27 (152): ans_f1: 0.4345, qst_f1: 0.3102, em: 0.0359, avg: 0.2352, multirc_loss: 0.6891
03/25 03:30:45 PM: Evaluate: task multirc, batch 29 (152): ans_f1: 0.4485, qst_f1: 0.3177, em: 0.0393, avg: 0.2439, multirc_loss: 0.6881
03/25 03:31:05 PM: Evaluate: task multirc, batch 31 (152): ans_f1: 0.4464, qst_f1: 0.3140, em: 0.0312, avg: 0.2388, multirc_loss: 0.6885
03/25 03:31:15 PM: Evaluate: task multirc, batch 32 (152): ans_f1: 0.4447, qst_f1: 0.3117, em: 0.0302, avg: 0.2374, multirc_loss: 0.6880
03/25 03:31:31 PM: Evaluate: task multirc, batch 34 (152): ans_f1: 0.4363, qst_f1: 0.2926, em: 0.0330, avg: 0.2347, multirc_loss: 0.6850
03/25 03:31:47 PM: Evaluate: task multirc, batch 36 (152): ans_f1: 0.4263, qst_f1: 0.2794, em: 0.0270, avg: 0.2266, multirc_loss: 0.6836
03/25 03:32:04 PM: Evaluate: task multirc, batch 38 (152): ans_f1: 0.4238, qst_f1: 0.2788, em: 0.0342, avg: 0.2290, multirc_loss: 0.6833
03/25 03:32:21 PM: Evaluate: task multirc, batch 40 (152): ans_f1: 0.4199, qst_f1: 0.2761, em: 0.0289, avg: 0.2244, multirc_loss: 0.6831
03/25 03:32:38 PM: Evaluate: task multirc, batch 42 (152): ans_f1: 0.4132, qst_f1: 0.2696, em: 0.0315, avg: 0.2223, multirc_loss: 0.6824
03/25 03:32:54 PM: Evaluate: task multirc, batch 44 (152): ans_f1: 0.4052, qst_f1: 0.2616, em: 0.0302, avg: 0.2177, multirc_loss: 0.6829
03/25 03:33:13 PM: Evaluate: task multirc, batch 46 (152): ans_f1: 0.4019, qst_f1: 0.2553, em: 0.0286, avg: 0.2152, multirc_loss: 0.6834
03/25 03:33:32 PM: Evaluate: task multirc, batch 48 (152): ans_f1: 0.4090, qst_f1: 0.2635, em: 0.0306, avg: 0.2198, multirc_loss: 0.6833
03/25 03:33:48 PM: Evaluate: task multirc, batch 50 (152): ans_f1: 0.4086, qst_f1: 0.2631, em: 0.0355, avg: 0.2220, multirc_loss: 0.6833
03/25 03:34:04 PM: Evaluate: task multirc, batch 52 (152): ans_f1: 0.4059, qst_f1: 0.2639, em: 0.0341, avg: 0.2200, multirc_loss: 0.6837
03/25 03:34:22 PM: Evaluate: task multirc, batch 54 (152): ans_f1: 0.4029, qst_f1: 0.2616, em: 0.0328, avg: 0.2179, multirc_loss: 0.6830
03/25 03:34:39 PM: Evaluate: task multirc, batch 56 (152): ans_f1: 0.4053, qst_f1: 0.2657, em: 0.0347, avg: 0.2200, multirc_loss: 0.6820
03/25 03:34:57 PM: Evaluate: task multirc, batch 58 (152): ans_f1: 0.4122, qst_f1: 0.2726, em: 0.0362, avg: 0.2242, multirc_loss: 0.6816
03/25 03:35:15 PM: Evaluate: task multirc, batch 60 (152): ans_f1: 0.4104, qst_f1: 0.2695, em: 0.0346, avg: 0.2225, multirc_loss: 0.6813
03/25 03:35:32 PM: Evaluate: task multirc, batch 62 (152): ans_f1: 0.4112, qst_f1: 0.2702, em: 0.0334, avg: 0.2223, multirc_loss: 0.6806
03/25 03:35:48 PM: Evaluate: task multirc, batch 64 (152): ans_f1: 0.4092, qst_f1: 0.2696, em: 0.0323, avg: 0.2208, multirc_loss: 0.6809
03/25 03:36:09 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.4215, qst_f1: 0.2808, em: 0.0358, avg: 0.2287, multirc_loss: 0.6804
03/25 03:36:26 PM: Evaluate: task multirc, batch 68 (152): ans_f1: 0.4243, qst_f1: 0.2824, em: 0.0370, avg: 0.2307, multirc_loss: 0.6807
03/25 03:36:45 PM: Evaluate: task multirc, batch 70 (152): ans_f1: 0.4256, qst_f1: 0.2869, em: 0.0339, avg: 0.2297, multirc_loss: 0.6811
03/25 03:36:58 PM: Evaluate: task multirc, batch 72 (152): ans_f1: 0.4308, qst_f1: 0.2895, em: 0.0330, avg: 0.2319, multirc_loss: 0.6815
03/25 03:37:15 PM: Evaluate: task multirc, batch 74 (152): ans_f1: 0.4313, qst_f1: 0.2888, em: 0.0323, avg: 0.2318, multirc_loss: 0.6814
03/25 03:37:32 PM: Evaluate: task multirc, batch 76 (152): ans_f1: 0.4319, qst_f1: 0.2914, em: 0.0335, avg: 0.2327, multirc_loss: 0.6813
03/25 03:37:46 PM: Evaluate: task multirc, batch 78 (152): ans_f1: 0.4340, qst_f1: 0.2913, em: 0.0307, avg: 0.2324, multirc_loss: 0.6815
03/25 03:38:01 PM: Evaluate: task multirc, batch 80 (152): ans_f1: 0.4295, qst_f1: 0.2872, em: 0.0299, avg: 0.2297, multirc_loss: 0.6815
03/25 03:38:16 PM: Evaluate: task multirc, batch 82 (152): ans_f1: 0.4245, qst_f1: 0.2807, em: 0.0311, avg: 0.2278, multirc_loss: 0.6815
03/25 03:38:31 PM: Evaluate: task multirc, batch 84 (152): ans_f1: 0.4223, qst_f1: 0.2783, em: 0.0284, avg: 0.2253, multirc_loss: 0.6816
03/25 03:38:51 PM: Evaluate: task multirc, batch 86 (152): ans_f1: 0.4242, qst_f1: 0.2795, em: 0.0279, avg: 0.2260, multirc_loss: 0.6813
03/25 03:39:10 PM: Evaluate: task multirc, batch 88 (152): ans_f1: 0.4241, qst_f1: 0.2819, em: 0.0309, avg: 0.2275, multirc_loss: 0.6813
03/25 03:39:27 PM: Evaluate: task multirc, batch 90 (152): ans_f1: 0.4268, qst_f1: 0.2827, em: 0.0302, avg: 0.2285, multirc_loss: 0.6815
03/25 03:39:42 PM: Evaluate: task multirc, batch 92 (152): ans_f1: 0.4276, qst_f1: 0.2840, em: 0.0297, avg: 0.2286, multirc_loss: 0.6817
03/25 03:40:01 PM: Evaluate: task multirc, batch 94 (152): ans_f1: 0.4302, qst_f1: 0.2866, em: 0.0291, avg: 0.2296, multirc_loss: 0.6817
03/25 03:40:12 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.4332, qst_f1: 0.2893, em: 0.0288, avg: 0.2310, multirc_loss: 0.6817
03/25 03:40:23 PM: Evaluate: task multirc, batch 96 (152): ans_f1: 0.4381, qst_f1: 0.2928, em: 0.0285, avg: 0.2333, multirc_loss: 0.6818
03/25 03:40:41 PM: Evaluate: task multirc, batch 98 (152): ans_f1: 0.4357, qst_f1: 0.2880, em: 0.0293, avg: 0.2325, multirc_loss: 0.6819
03/25 03:41:00 PM: Evaluate: task multirc, batch 100 (152): ans_f1: 0.4347, qst_f1: 0.2866, em: 0.0271, avg: 0.2309, multirc_loss: 0.6821
03/25 03:41:13 PM: Evaluate: task multirc, batch 102 (152): ans_f1: 0.4363, qst_f1: 0.2867, em: 0.0263, avg: 0.2313, multirc_loss: 0.6829
03/25 03:41:28 PM: Evaluate: task multirc, batch 104 (152): ans_f1: 0.4337, qst_f1: 0.2844, em: 0.0272, avg: 0.2305, multirc_loss: 0.6832
03/25 03:41:45 PM: Evaluate: task multirc, batch 106 (152): ans_f1: 0.4338, qst_f1: 0.2846, em: 0.0282, avg: 0.2310, multirc_loss: 0.6830
03/25 03:42:01 PM: Evaluate: task multirc, batch 108 (152): ans_f1: 0.4344, qst_f1: 0.2852, em: 0.0289, avg: 0.2317, multirc_loss: 0.6832
03/25 03:42:19 PM: Evaluate: task multirc, batch 110 (152): ans_f1: 0.4371, qst_f1: 0.2874, em: 0.0283, avg: 0.2327, multirc_loss: 0.6833
03/25 03:42:32 PM: Evaluate: task multirc, batch 112 (152): ans_f1: 0.4442, qst_f1: 0.2939, em: 0.0305, avg: 0.2374, multirc_loss: 0.6834
03/25 03:42:49 PM: Evaluate: task multirc, batch 114 (152): ans_f1: 0.4437, qst_f1: 0.2937, em: 0.0327, avg: 0.2382, multirc_loss: 0.6832
03/25 03:43:06 PM: Evaluate: task multirc, batch 116 (152): ans_f1: 0.4427, qst_f1: 0.2924, em: 0.0372, avg: 0.2400, multirc_loss: 0.6828
03/25 03:43:22 PM: Evaluate: task multirc, batch 118 (152): ans_f1: 0.4394, qst_f1: 0.2886, em: 0.0367, avg: 0.2380, multirc_loss: 0.6823
03/25 03:43:40 PM: Evaluate: task multirc, batch 120 (152): ans_f1: 0.4392, qst_f1: 0.2895, em: 0.0375, avg: 0.2383, multirc_loss: 0.6822
03/25 03:43:58 PM: Evaluate: task multirc, batch 122 (152): ans_f1: 0.4383, qst_f1: 0.2902, em: 0.0369, avg: 0.2376, multirc_loss: 0.6821
03/25 03:44:16 PM: Evaluate: task multirc, batch 124 (152): ans_f1: 0.4391, qst_f1: 0.2907, em: 0.0365, avg: 0.2378, multirc_loss: 0.6819
03/25 03:44:34 PM: Evaluate: task multirc, batch 126 (152): ans_f1: 0.4370, qst_f1: 0.2890, em: 0.0362, avg: 0.2366, multirc_loss: 0.6815
03/25 03:44:48 PM: Evaluate: task multirc, batch 128 (152): ans_f1: 0.4382, qst_f1: 0.2899, em: 0.0357, avg: 0.2370, multirc_loss: 0.6815
03/25 03:45:02 PM: Evaluate: task multirc, batch 130 (152): ans_f1: 0.4410, qst_f1: 0.2927, em: 0.0363, avg: 0.2387, multirc_loss: 0.6818
03/25 03:45:19 PM: Evaluate: task multirc, batch 132 (152): ans_f1: 0.4418, qst_f1: 0.2942, em: 0.0370, avg: 0.2394, multirc_loss: 0.6817
03/25 03:45:36 PM: Evaluate: task multirc, batch 134 (152): ans_f1: 0.4413, qst_f1: 0.2924, em: 0.0363, avg: 0.2388, multirc_loss: 0.6817
03/25 03:45:47 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.4407, qst_f1: 0.2921, em: 0.0360, avg: 0.2384, multirc_loss: 0.6816
03/25 03:45:57 PM: Evaluate: task multirc, batch 136 (152): ans_f1: 0.4420, qst_f1: 0.2926, em: 0.0358, avg: 0.2389, multirc_loss: 0.6816
03/25 03:46:08 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.4435, qst_f1: 0.2947, em: 0.0378, avg: 0.2407, multirc_loss: 0.6814
03/25 03:46:23 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.4459, qst_f1: 0.2965, em: 0.0386, avg: 0.2423, multirc_loss: 0.6813
03/25 03:46:36 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.4500, qst_f1: 0.3004, em: 0.0405, avg: 0.2453, multirc_loss: 0.6812
03/25 03:46:53 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.4502, qst_f1: 0.3003, em: 0.0400, avg: 0.2451, multirc_loss: 0.6808
03/25 03:47:10 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.4499, qst_f1: 0.2999, em: 0.0406, avg: 0.2452, multirc_loss: 0.6803
03/25 03:47:27 PM: Evaluate: task multirc, batch 147 (152): ans_f1: 0.4484, qst_f1: 0.2991, em: 0.0410, avg: 0.2447, multirc_loss: 0.6803
03/25 03:47:45 PM: Evaluate: task multirc, batch 149 (152): ans_f1: 0.4468, qst_f1: 0.2976, em: 0.0406, avg: 0.2437, multirc_loss: 0.6803
03/25 03:48:04 PM: Evaluate: task multirc, batch 151 (152): ans_f1: 0.4474, qst_f1: 0.2987, em: 0.0411, avg: 0.2443, multirc_loss: 0.6804
03/25 03:48:08 PM: Updating LR scheduler:
03/25 03:48:08 PM: 	Best result seen so far for macro_avg: 0.301
03/25 03:48:08 PM: 	# validation passes without improvement: 3
03/25 03:48:08 PM: multirc_loss: training: 0.693592 validation: 0.680463
03/25 03:48:08 PM: macro_avg: validation: 0.243895
03/25 03:48:08 PM: micro_avg: validation: 0.243895
03/25 03:48:08 PM: multirc_ans_f1: training: 0.560000 validation: 0.446866
03/25 03:48:08 PM: multirc_qst_f1: training: 0.215054 validation: 0.297951
03/25 03:48:08 PM: multirc_em: training: 0.645161 validation: 0.040923
03/25 03:48:08 PM: multirc_avg: training: 0.602581 validation: 0.243895
03/25 03:48:08 PM: Global learning rate: 5e-06
03/25 03:48:08 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 03:49:08 PM: Update 11: task multirc, steps since last val 1 (total steps = 11): ans_f1: 0.5000, qst_f1: 0.1875, em: 0.6250, avg: 0.5625, multirc_loss: 0.6698
03/25 03:49:08 PM: ***** Step 11 / Validation 11 *****
03/25 03:49:08 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 03:49:08 PM: Validating...
03/25 03:49:25 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.3077, qst_f1: 0.2060, em: 0.0000, avg: 0.1538, multirc_loss: 0.6754
03/25 03:49:41 PM: Evaluate: task multirc, batch 4 (152): ans_f1: 0.2927, qst_f1: 0.1794, em: 0.0690, avg: 0.1808, multirc_loss: 0.6911
03/25 03:49:58 PM: Evaluate: task multirc, batch 6 (152): ans_f1: 0.3115, qst_f1: 0.1723, em: 0.0227, avg: 0.1671, multirc_loss: 0.6873
03/25 03:50:16 PM: Evaluate: task multirc, batch 8 (152): ans_f1: 0.2390, qst_f1: 0.1182, em: 0.0161, avg: 0.1276, multirc_loss: 0.6968
03/25 03:50:33 PM: Evaluate: task multirc, batch 10 (152): ans_f1: 0.2083, qst_f1: 0.1156, em: 0.0278, avg: 0.1181, multirc_loss: 0.7051
03/25 03:50:48 PM: Evaluate: task multirc, batch 12 (152): ans_f1: 0.2026, qst_f1: 0.1123, em: 0.0233, avg: 0.1129, multirc_loss: 0.7040
03/25 03:51:01 PM: Evaluate: task multirc, batch 14 (152): ans_f1: 0.1697, qst_f1: 0.0966, em: 0.0200, avg: 0.0949, multirc_loss: 0.7128
03/25 03:51:15 PM: Evaluate: task multirc, batch 16 (152): ans_f1: 0.1549, qst_f1: 0.0878, em: 0.0273, avg: 0.0911, multirc_loss: 0.7069
03/25 03:51:25 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.1587, qst_f1: 0.0925, em: 0.0171, avg: 0.0879, multirc_loss: 0.7043
03/25 03:51:44 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.1543, qst_f1: 0.0891, em: 0.0155, avg: 0.0849, multirc_loss: 0.6973
03/25 03:51:54 PM: Evaluate: task multirc, batch 20 (152): ans_f1: 0.1586, qst_f1: 0.0900, em: 0.0149, avg: 0.0868, multirc_loss: 0.6952
03/25 03:52:08 PM: Evaluate: task multirc, batch 22 (152): ans_f1: 0.1470, qst_f1: 0.0844, em: 0.0140, avg: 0.0805, multirc_loss: 0.6933
03/25 03:52:18 PM: Evaluate: task multirc, batch 23 (152): ans_f1: 0.1421, qst_f1: 0.0815, em: 0.0135, avg: 0.0778, multirc_loss: 0.6920
03/25 03:52:39 PM: Evaluate: task multirc, batch 25 (152): ans_f1: 0.1353, qst_f1: 0.0764, em: 0.0127, avg: 0.0740, multirc_loss: 0.6879
03/25 03:52:56 PM: Evaluate: task multirc, batch 27 (152): ans_f1: 0.1267, qst_f1: 0.0722, em: 0.0120, avg: 0.0693, multirc_loss: 0.6867
03/25 03:53:15 PM: Evaluate: task multirc, batch 29 (152): ans_f1: 0.1284, qst_f1: 0.0711, em: 0.0112, avg: 0.0698, multirc_loss: 0.6873
03/25 03:53:32 PM: Evaluate: task multirc, batch 31 (152): ans_f1: 0.1218, qst_f1: 0.0660, em: 0.0104, avg: 0.0661, multirc_loss: 0.6869
03/25 03:53:50 PM: Evaluate: task multirc, batch 33 (152): ans_f1: 0.1203, qst_f1: 0.0647, em: 0.0097, avg: 0.0650, multirc_loss: 0.6840
03/25 03:54:05 PM: Evaluate: task multirc, batch 35 (152): ans_f1: 0.1166, qst_f1: 0.0617, em: 0.0093, avg: 0.0629, multirc_loss: 0.6798
03/25 03:54:21 PM: Evaluate: task multirc, batch 37 (152): ans_f1: 0.1119, qst_f1: 0.0582, em: 0.0087, avg: 0.0603, multirc_loss: 0.6778
03/25 03:54:37 PM: Evaluate: task multirc, batch 39 (152): ans_f1: 0.1074, qst_f1: 0.0560, em: 0.0084, avg: 0.0579, multirc_loss: 0.6771
03/25 03:54:53 PM: Evaluate: task multirc, batch 41 (152): ans_f1: 0.1058, qst_f1: 0.0560, em: 0.0081, avg: 0.0569, multirc_loss: 0.6769
03/25 03:55:09 PM: Evaluate: task multirc, batch 43 (152): ans_f1: 0.1014, qst_f1: 0.0532, em: 0.0077, avg: 0.0545, multirc_loss: 0.6767
03/25 03:55:24 PM: Evaluate: task multirc, batch 45 (152): ans_f1: 0.1017, qst_f1: 0.0538, em: 0.0074, avg: 0.0545, multirc_loss: 0.6797
03/25 03:55:40 PM: Evaluate: task multirc, batch 47 (152): ans_f1: 0.1000, qst_f1: 0.0506, em: 0.0069, avg: 0.0535, multirc_loss: 0.6796
03/25 03:55:58 PM: Evaluate: task multirc, batch 49 (152): ans_f1: 0.1206, qst_f1: 0.0603, em: 0.0133, avg: 0.0669, multirc_loss: 0.6792
03/25 03:56:13 PM: Evaluate: task multirc, batch 51 (152): ans_f1: 0.1204, qst_f1: 0.0604, em: 0.0158, avg: 0.0681, multirc_loss: 0.6777
03/25 03:56:31 PM: Evaluate: task multirc, batch 53 (152): ans_f1: 0.1199, qst_f1: 0.0597, em: 0.0182, avg: 0.0690, multirc_loss: 0.6765
03/25 03:56:48 PM: Evaluate: task multirc, batch 55 (152): ans_f1: 0.1166, qst_f1: 0.0578, em: 0.0176, avg: 0.0671, multirc_loss: 0.6743
03/25 03:57:04 PM: Evaluate: task multirc, batch 57 (152): ans_f1: 0.1201, qst_f1: 0.0599, em: 0.0198, avg: 0.0700, multirc_loss: 0.6732
03/25 03:57:25 PM: Evaluate: task multirc, batch 59 (152): ans_f1: 0.1202, qst_f1: 0.0600, em: 0.0217, avg: 0.0709, multirc_loss: 0.6739
03/25 03:57:36 PM: Evaluate: task multirc, batch 60 (152): ans_f1: 0.1188, qst_f1: 0.0589, em: 0.0213, avg: 0.0701, multirc_loss: 0.6728
03/25 03:57:47 PM: Evaluate: task multirc, batch 61 (152): ans_f1: 0.1178, qst_f1: 0.0581, em: 0.0210, avg: 0.0694, multirc_loss: 0.6714
03/25 03:57:59 PM: Evaluate: task multirc, batch 62 (152): ans_f1: 0.1178, qst_f1: 0.0580, em: 0.0206, avg: 0.0692, multirc_loss: 0.6717
03/25 03:58:15 PM: Evaluate: task multirc, batch 64 (152): ans_f1: 0.1178, qst_f1: 0.0593, em: 0.0199, avg: 0.0688, multirc_loss: 0.6725
03/25 03:58:36 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.1380, qst_f1: 0.0703, em: 0.0239, avg: 0.0809, multirc_loss: 0.6723
03/25 03:58:46 PM: Evaluate: task multirc, batch 67 (152): ans_f1: 0.1362, qst_f1: 0.0693, em: 0.0235, avg: 0.0799, multirc_loss: 0.6722
03/25 03:59:06 PM: Evaluate: task multirc, batch 69 (152): ans_f1: 0.1392, qst_f1: 0.0718, em: 0.0251, avg: 0.0821, multirc_loss: 0.6722
03/25 03:59:24 PM: Evaluate: task multirc, batch 71 (152): ans_f1: 0.1451, qst_f1: 0.0730, em: 0.0222, avg: 0.0837, multirc_loss: 0.6730
03/25 03:59:40 PM: Evaluate: task multirc, batch 73 (152): ans_f1: 0.1560, qst_f1: 0.0792, em: 0.0240, avg: 0.0900, multirc_loss: 0.6729
03/25 03:59:58 PM: Evaluate: task multirc, batch 75 (152): ans_f1: 0.1548, qst_f1: 0.0786, em: 0.0234, avg: 0.0891, multirc_loss: 0.6719
03/25 04:00:14 PM: Evaluate: task multirc, batch 77 (152): ans_f1: 0.1591, qst_f1: 0.0802, em: 0.0249, avg: 0.0920, multirc_loss: 0.6717
03/25 04:00:32 PM: Evaluate: task multirc, batch 79 (152): ans_f1: 0.1558, qst_f1: 0.0780, em: 0.0242, avg: 0.0900, multirc_loss: 0.6718
03/25 04:00:49 PM: Evaluate: task multirc, batch 81 (152): ans_f1: 0.1524, qst_f1: 0.0761, em: 0.0256, avg: 0.0890, multirc_loss: 0.6719
03/25 04:01:05 PM: Evaluate: task multirc, batch 83 (152): ans_f1: 0.1485, qst_f1: 0.0738, em: 0.0249, avg: 0.0867, multirc_loss: 0.6727
03/25 04:01:25 PM: Evaluate: task multirc, batch 85 (152): ans_f1: 0.1487, qst_f1: 0.0745, em: 0.0225, avg: 0.0856, multirc_loss: 0.6717
03/25 04:01:36 PM: Evaluate: task multirc, batch 86 (152): ans_f1: 0.1517, qst_f1: 0.0755, em: 0.0223, avg: 0.0870, multirc_loss: 0.6713
03/25 04:01:46 PM: Evaluate: task multirc, batch 87 (152): ans_f1: 0.1502, qst_f1: 0.0747, em: 0.0221, avg: 0.0861, multirc_loss: 0.6709
03/25 04:01:57 PM: Evaluate: task multirc, batch 88 (152): ans_f1: 0.1505, qst_f1: 0.0752, em: 0.0218, avg: 0.0862, multirc_loss: 0.6718
03/25 04:02:08 PM: Evaluate: task multirc, batch 89 (152): ans_f1: 0.1496, qst_f1: 0.0747, em: 0.0215, avg: 0.0855, multirc_loss: 0.6725
03/25 04:02:21 PM: Evaluate: task multirc, batch 91 (152): ans_f1: 0.1461, qst_f1: 0.0737, em: 0.0212, avg: 0.0837, multirc_loss: 0.6727
03/25 04:02:39 PM: Evaluate: task multirc, batch 93 (152): ans_f1: 0.1451, qst_f1: 0.0732, em: 0.0207, avg: 0.0829, multirc_loss: 0.6724
03/25 04:02:50 PM: Evaluate: task multirc, batch 94 (152): ans_f1: 0.1474, qst_f1: 0.0742, em: 0.0205, avg: 0.0840, multirc_loss: 0.6724
03/25 04:03:00 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.1510, qst_f1: 0.0774, em: 0.0203, avg: 0.0856, multirc_loss: 0.6723
03/25 04:03:11 PM: Evaluate: task multirc, batch 96 (152): ans_f1: 0.1526, qst_f1: 0.0782, em: 0.0201, avg: 0.0864, multirc_loss: 0.6725
03/25 04:03:30 PM: Evaluate: task multirc, batch 98 (152): ans_f1: 0.1527, qst_f1: 0.0770, em: 0.0211, avg: 0.0869, multirc_loss: 0.6733
03/25 04:03:40 PM: Evaluate: task multirc, batch 99 (152): ans_f1: 0.1523, qst_f1: 0.0768, em: 0.0209, avg: 0.0866, multirc_loss: 0.6737
03/25 04:03:57 PM: Evaluate: task multirc, batch 101 (152): ans_f1: 0.1585, qst_f1: 0.0811, em: 0.0204, avg: 0.0895, multirc_loss: 0.6742
03/25 04:04:09 PM: Evaluate: task multirc, batch 103 (152): ans_f1: 0.1547, qst_f1: 0.0786, em: 0.0198, avg: 0.0873, multirc_loss: 0.6765
03/25 04:04:27 PM: Evaluate: task multirc, batch 105 (152): ans_f1: 0.1525, qst_f1: 0.0773, em: 0.0195, avg: 0.0860, multirc_loss: 0.6759
03/25 04:04:45 PM: Evaluate: task multirc, batch 107 (152): ans_f1: 0.1519, qst_f1: 0.0774, em: 0.0191, avg: 0.0855, multirc_loss: 0.6758
03/25 04:05:03 PM: Evaluate: task multirc, batch 109 (152): ans_f1: 0.1521, qst_f1: 0.0774, em: 0.0200, avg: 0.0861, multirc_loss: 0.6764
03/25 04:05:18 PM: Evaluate: task multirc, batch 111 (152): ans_f1: 0.1543, qst_f1: 0.0792, em: 0.0196, avg: 0.0869, multirc_loss: 0.6768
03/25 04:05:35 PM: Evaluate: task multirc, batch 113 (152): ans_f1: 0.1525, qst_f1: 0.0785, em: 0.0192, avg: 0.0859, multirc_loss: 0.6769
03/25 04:05:46 PM: Evaluate: task multirc, batch 114 (152): ans_f1: 0.1517, qst_f1: 0.0778, em: 0.0204, avg: 0.0861, multirc_loss: 0.6765
03/25 04:06:05 PM: Evaluate: task multirc, batch 116 (152): ans_f1: 0.1506, qst_f1: 0.0767, em: 0.0199, avg: 0.0852, multirc_loss: 0.6760
03/25 04:06:22 PM: Evaluate: task multirc, batch 118 (152): ans_f1: 0.1488, qst_f1: 0.0757, em: 0.0197, avg: 0.0842, multirc_loss: 0.6753
03/25 04:06:33 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.1485, qst_f1: 0.0759, em: 0.0195, avg: 0.0840, multirc_loss: 0.6754
03/25 04:06:43 PM: Evaluate: task multirc, batch 120 (152): ans_f1: 0.1476, qst_f1: 0.0753, em: 0.0207, avg: 0.0841, multirc_loss: 0.6752
03/25 04:07:03 PM: Evaluate: task multirc, batch 122 (152): ans_f1: 0.1467, qst_f1: 0.0748, em: 0.0191, avg: 0.0829, multirc_loss: 0.6747
03/25 04:07:23 PM: Evaluate: task multirc, batch 124 (152): ans_f1: 0.1454, qst_f1: 0.0744, em: 0.0189, avg: 0.0821, multirc_loss: 0.6747
03/25 04:07:43 PM: Evaluate: task multirc, batch 126 (152): ans_f1: 0.1436, qst_f1: 0.0737, em: 0.0187, avg: 0.0811, multirc_loss: 0.6741
03/25 04:07:59 PM: Evaluate: task multirc, batch 128 (152): ans_f1: 0.1539, qst_f1: 0.0781, em: 0.0209, avg: 0.0874, multirc_loss: 0.6738
03/25 04:08:14 PM: Evaluate: task multirc, batch 130 (152): ans_f1: 0.1614, qst_f1: 0.0805, em: 0.0206, avg: 0.0910, multirc_loss: 0.6741
03/25 04:08:32 PM: Evaluate: task multirc, batch 132 (152): ans_f1: 0.1594, qst_f1: 0.0793, em: 0.0203, avg: 0.0898, multirc_loss: 0.6739
03/25 04:08:50 PM: Evaluate: task multirc, batch 134 (152): ans_f1: 0.1579, qst_f1: 0.0782, em: 0.0199, avg: 0.0889, multirc_loss: 0.6740
03/25 04:09:01 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.1577, qst_f1: 0.0782, em: 0.0197, avg: 0.0887, multirc_loss: 0.6739
03/25 04:09:11 PM: Evaluate: task multirc, batch 136 (152): ans_f1: 0.1574, qst_f1: 0.0783, em: 0.0196, avg: 0.0885, multirc_loss: 0.6740
03/25 04:09:23 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.1574, qst_f1: 0.0790, em: 0.0206, avg: 0.0890, multirc_loss: 0.6736
03/25 04:09:38 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.1573, qst_f1: 0.0788, em: 0.0205, avg: 0.0889, multirc_loss: 0.6732
03/25 04:09:51 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.1605, qst_f1: 0.0809, em: 0.0214, avg: 0.0909, multirc_loss: 0.6727
03/25 04:10:09 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.1607, qst_f1: 0.0813, em: 0.0211, avg: 0.0909, multirc_loss: 0.6720
03/25 04:10:27 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.1593, qst_f1: 0.0803, em: 0.0208, avg: 0.0900, multirc_loss: 0.6712
03/25 04:10:44 PM: Evaluate: task multirc, batch 147 (152): ans_f1: 0.1572, qst_f1: 0.0790, em: 0.0205, avg: 0.0889, multirc_loss: 0.6713
03/25 04:10:55 PM: Evaluate: task multirc, batch 148 (152): ans_f1: 0.1572, qst_f1: 0.0793, em: 0.0215, avg: 0.0893, multirc_loss: 0.6711
03/25 04:11:14 PM: Evaluate: task multirc, batch 150 (152): ans_f1: 0.1567, qst_f1: 0.0793, em: 0.0212, avg: 0.0890, multirc_loss: 0.6713
03/25 04:11:28 PM: Evaluate: task multirc, batch 152 (152): ans_f1: 0.1554, qst_f1: 0.0790, em: 0.0199, avg: 0.0877, multirc_loss: 0.6723
03/25 04:11:28 PM: Updating LR scheduler:
03/25 04:11:28 PM: 	Best result seen so far for macro_avg: 0.301
03/25 04:11:28 PM: 	# validation passes without improvement: 4
03/25 04:11:28 PM: multirc_loss: training: 0.669791 validation: 0.672255
03/25 04:11:28 PM: macro_avg: validation: 0.087667
03/25 04:11:28 PM: micro_avg: validation: 0.087667
03/25 04:11:28 PM: multirc_ans_f1: training: 0.500000 validation: 0.155397
03/25 04:11:28 PM: multirc_qst_f1: training: 0.187500 validation: 0.078979
03/25 04:11:28 PM: multirc_em: training: 0.625000 validation: 0.019937
03/25 04:11:28 PM: multirc_avg: training: 0.562500 validation: 0.087667
03/25 04:11:28 PM: Global learning rate: 5e-06
03/25 04:11:28 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 04:12:25 PM: Update 12: task multirc, steps since last val 1 (total steps = 12): ans_f1: 0.4000, qst_f1: 0.1250, em: 0.6250, avg: 0.5125, multirc_loss: 0.6952
03/25 04:12:25 PM: ***** Step 12 / Validation 12 *****
03/25 04:12:25 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 04:12:25 PM: Validating...
03/25 04:12:44 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6703
03/25 04:12:54 PM: Evaluate: task multirc, batch 3 (152): ans_f1: 0.0455, qst_f1: 0.0333, em: 0.0000, avg: 0.0227, multirc_loss: 0.6784
03/25 04:13:10 PM: Evaluate: task multirc, batch 5 (152): ans_f1: 0.0714, qst_f1: 0.0396, em: 0.0000, avg: 0.0357, multirc_loss: 0.7052
03/25 04:13:30 PM: Evaluate: task multirc, batch 7 (152): ans_f1: 0.0513, qst_f1: 0.0277, em: 0.0000, avg: 0.0256, multirc_loss: 0.7199
03/25 04:13:40 PM: Evaluate: task multirc, batch 8 (152): ans_f1: 0.0451, qst_f1: 0.0237, em: 0.0000, avg: 0.0226, multirc_loss: 0.7187
03/25 04:13:59 PM: Evaluate: task multirc, batch 10 (152): ans_f1: 0.0366, qst_f1: 0.0204, em: 0.0000, avg: 0.0183, multirc_loss: 0.7323
03/25 04:14:16 PM: Evaluate: task multirc, batch 12 (152): ans_f1: 0.0309, qst_f1: 0.0171, em: 0.0000, avg: 0.0155, multirc_loss: 0.7300
03/25 04:14:31 PM: Evaluate: task multirc, batch 14 (152): ans_f1: 0.0252, qst_f1: 0.0147, em: 0.0000, avg: 0.0126, multirc_loss: 0.7461
03/25 04:14:47 PM: Evaluate: task multirc, batch 16 (152): ans_f1: 0.0227, qst_f1: 0.0133, em: 0.0091, avg: 0.0159, multirc_loss: 0.7364
03/25 04:14:57 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.0217, qst_f1: 0.0125, em: 0.0000, avg: 0.0108, multirc_loss: 0.7318
03/25 04:15:17 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.0201, qst_f1: 0.0114, em: 0.0000, avg: 0.0101, multirc_loss: 0.7191
03/25 04:15:27 PM: Evaluate: task multirc, batch 20 (152): ans_f1: 0.0194, qst_f1: 0.0109, em: 0.0000, avg: 0.0097, multirc_loss: 0.7152
03/25 04:15:42 PM: Evaluate: task multirc, batch 22 (152): ans_f1: 0.0178, qst_f1: 0.0103, em: 0.0000, avg: 0.0089, multirc_loss: 0.7114
03/25 04:15:52 PM: Evaluate: task multirc, batch 23 (152): ans_f1: 0.0171, qst_f1: 0.0099, em: 0.0000, avg: 0.0085, multirc_loss: 0.7091
03/25 04:16:10 PM: Evaluate: task multirc, batch 25 (152): ans_f1: 0.0162, qst_f1: 0.0093, em: 0.0000, avg: 0.0081, multirc_loss: 0.7022
03/25 04:16:28 PM: Evaluate: task multirc, batch 27 (152): ans_f1: 0.0150, qst_f1: 0.0088, em: 0.0000, avg: 0.0075, multirc_loss: 0.7009
03/25 04:16:48 PM: Evaluate: task multirc, batch 29 (152): ans_f1: 0.0138, qst_f1: 0.0082, em: 0.0000, avg: 0.0069, multirc_loss: 0.7045
03/25 04:17:07 PM: Evaluate: task multirc, batch 31 (152): ans_f1: 0.0131, qst_f1: 0.0076, em: 0.0000, avg: 0.0065, multirc_loss: 0.7029
03/25 04:17:18 PM: Evaluate: task multirc, batch 32 (152): ans_f1: 0.0127, qst_f1: 0.0074, em: 0.0000, avg: 0.0064, multirc_loss: 0.7010
03/25 04:17:35 PM: Evaluate: task multirc, batch 34 (152): ans_f1: 0.0123, qst_f1: 0.0069, em: 0.0047, avg: 0.0085, multirc_loss: 0.6932
03/25 04:17:53 PM: Evaluate: task multirc, batch 36 (152): ans_f1: 0.0118, qst_f1: 0.0066, em: 0.0000, avg: 0.0059, multirc_loss: 0.6892
03/25 04:18:10 PM: Evaluate: task multirc, batch 38 (152): ans_f1: 0.0113, qst_f1: 0.0063, em: 0.0043, avg: 0.0078, multirc_loss: 0.6890
03/25 04:18:29 PM: Evaluate: task multirc, batch 40 (152): ans_f1: 0.0107, qst_f1: 0.0061, em: 0.0000, avg: 0.0054, multirc_loss: 0.6886
03/25 04:18:48 PM: Evaluate: task multirc, batch 42 (152): ans_f1: 0.0102, qst_f1: 0.0058, em: 0.0000, avg: 0.0051, multirc_loss: 0.6879
03/25 04:19:06 PM: Evaluate: task multirc, batch 44 (152): ans_f1: 0.0097, qst_f1: 0.0055, em: 0.0000, avg: 0.0049, multirc_loss: 0.6913
03/25 04:19:23 PM: Evaluate: task multirc, batch 46 (152): ans_f1: 0.0093, qst_f1: 0.0052, em: 0.0000, avg: 0.0046, multirc_loss: 0.6937
03/25 04:19:43 PM: Evaluate: task multirc, batch 48 (152): ans_f1: 0.0089, qst_f1: 0.0050, em: 0.0000, avg: 0.0044, multirc_loss: 0.6928
03/25 04:20:01 PM: Evaluate: task multirc, batch 50 (152): ans_f1: 0.0086, qst_f1: 0.0047, em: 0.0032, avg: 0.0059, multirc_loss: 0.6909
03/25 04:20:18 PM: Evaluate: task multirc, batch 52 (152): ans_f1: 0.0084, qst_f1: 0.0045, em: 0.0062, avg: 0.0073, multirc_loss: 0.6868
03/25 04:20:29 PM: Evaluate: task multirc, batch 53 (152): ans_f1: 0.0083, qst_f1: 0.0045, em: 0.0061, avg: 0.0072, multirc_loss: 0.6859
03/25 04:20:48 PM: Evaluate: task multirc, batch 55 (152): ans_f1: 0.0081, qst_f1: 0.0043, em: 0.0059, avg: 0.0070, multirc_loss: 0.6828
03/25 04:21:07 PM: Evaluate: task multirc, batch 57 (152): ans_f1: 0.0078, qst_f1: 0.0042, em: 0.0057, avg: 0.0067, multirc_loss: 0.6810
03/25 04:21:27 PM: Evaluate: task multirc, batch 59 (152): ans_f1: 0.0075, qst_f1: 0.0040, em: 0.0054, avg: 0.0065, multirc_loss: 0.6819
03/25 04:21:46 PM: Evaluate: task multirc, batch 61 (152): ans_f1: 0.0074, qst_f1: 0.0038, em: 0.0052, avg: 0.0063, multirc_loss: 0.6786
03/25 04:22:01 PM: Evaluate: task multirc, batch 63 (152): ans_f1: 0.0071, qst_f1: 0.0037, em: 0.0051, avg: 0.0061, multirc_loss: 0.6796
03/25 04:22:18 PM: Evaluate: task multirc, batch 65 (152): ans_f1: 0.0090, qst_f1: 0.0052, em: 0.0048, avg: 0.0069, multirc_loss: 0.6810
03/25 04:22:29 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.0111, qst_f1: 0.0060, em: 0.0048, avg: 0.0079, multirc_loss: 0.6806
03/25 04:22:48 PM: Evaluate: task multirc, batch 68 (152): ans_f1: 0.0108, qst_f1: 0.0059, em: 0.0069, avg: 0.0089, multirc_loss: 0.6812
03/25 04:22:59 PM: Evaluate: task multirc, batch 69 (152): ans_f1: 0.0128, qst_f1: 0.0073, em: 0.0046, avg: 0.0087, multirc_loss: 0.6802
03/25 04:23:09 PM: Evaluate: task multirc, batch 70 (152): ans_f1: 0.0126, qst_f1: 0.0072, em: 0.0045, avg: 0.0086, multirc_loss: 0.6805
03/25 04:23:24 PM: Evaluate: task multirc, batch 72 (152): ans_f1: 0.0122, qst_f1: 0.0070, em: 0.0044, avg: 0.0083, multirc_loss: 0.6805
03/25 04:23:42 PM: Evaluate: task multirc, batch 74 (152): ans_f1: 0.0120, qst_f1: 0.0069, em: 0.0065, avg: 0.0092, multirc_loss: 0.6789
03/25 04:24:01 PM: Evaluate: task multirc, batch 76 (152): ans_f1: 0.0117, qst_f1: 0.0067, em: 0.0042, avg: 0.0079, multirc_loss: 0.6781
03/25 04:24:16 PM: Evaluate: task multirc, batch 78 (152): ans_f1: 0.0114, qst_f1: 0.0065, em: 0.0041, avg: 0.0078, multirc_loss: 0.6775
03/25 04:24:33 PM: Evaluate: task multirc, batch 80 (152): ans_f1: 0.0112, qst_f1: 0.0064, em: 0.0040, avg: 0.0076, multirc_loss: 0.6774
03/25 04:24:49 PM: Evaluate: task multirc, batch 82 (152): ans_f1: 0.0109, qst_f1: 0.0062, em: 0.0058, avg: 0.0083, multirc_loss: 0.6783
03/25 04:25:06 PM: Evaluate: task multirc, batch 84 (152): ans_f1: 0.0106, qst_f1: 0.0060, em: 0.0038, avg: 0.0072, multirc_loss: 0.6793
03/25 04:25:17 PM: Evaluate: task multirc, batch 85 (152): ans_f1: 0.0105, qst_f1: 0.0060, em: 0.0037, avg: 0.0071, multirc_loss: 0.6783
03/25 04:25:28 PM: Evaluate: task multirc, batch 86 (152): ans_f1: 0.0104, qst_f1: 0.0059, em: 0.0037, avg: 0.0070, multirc_loss: 0.6774
03/25 04:25:48 PM: Evaluate: task multirc, batch 88 (152): ans_f1: 0.0101, qst_f1: 0.0058, em: 0.0036, avg: 0.0069, multirc_loss: 0.6789
03/25 04:25:58 PM: Evaluate: task multirc, batch 89 (152): ans_f1: 0.0099, qst_f1: 0.0057, em: 0.0036, avg: 0.0067, multirc_loss: 0.6802
03/25 04:26:12 PM: Evaluate: task multirc, batch 91 (152): ans_f1: 0.0096, qst_f1: 0.0057, em: 0.0035, avg: 0.0066, multirc_loss: 0.6808
03/25 04:26:29 PM: Evaluate: task multirc, batch 93 (152): ans_f1: 0.0095, qst_f1: 0.0055, em: 0.0035, avg: 0.0065, multirc_loss: 0.6796
03/25 04:26:40 PM: Evaluate: task multirc, batch 94 (152): ans_f1: 0.0094, qst_f1: 0.0055, em: 0.0034, avg: 0.0064, multirc_loss: 0.6795
03/25 04:26:50 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.0093, qst_f1: 0.0054, em: 0.0034, avg: 0.0063, multirc_loss: 0.6792
03/25 04:27:09 PM: Evaluate: task multirc, batch 97 (152): ans_f1: 0.0091, qst_f1: 0.0053, em: 0.0033, avg: 0.0062, multirc_loss: 0.6798
03/25 04:27:29 PM: Evaluate: task multirc, batch 99 (152): ans_f1: 0.0088, qst_f1: 0.0051, em: 0.0048, avg: 0.0068, multirc_loss: 0.6818
03/25 04:27:45 PM: Evaluate: task multirc, batch 101 (152): ans_f1: 0.0100, qst_f1: 0.0066, em: 0.0047, avg: 0.0074, multirc_loss: 0.6830
03/25 04:27:58 PM: Evaluate: task multirc, batch 103 (152): ans_f1: 0.0098, qst_f1: 0.0064, em: 0.0046, avg: 0.0072, multirc_loss: 0.6866
03/25 04:28:15 PM: Evaluate: task multirc, batch 105 (152): ans_f1: 0.0096, qst_f1: 0.0063, em: 0.0045, avg: 0.0070, multirc_loss: 0.6857
03/25 04:28:33 PM: Evaluate: task multirc, batch 107 (152): ans_f1: 0.0094, qst_f1: 0.0062, em: 0.0044, avg: 0.0069, multirc_loss: 0.6854
03/25 04:28:50 PM: Evaluate: task multirc, batch 109 (152): ans_f1: 0.0092, qst_f1: 0.0060, em: 0.0043, avg: 0.0068, multirc_loss: 0.6865
03/25 04:29:04 PM: Evaluate: task multirc, batch 111 (152): ans_f1: 0.0090, qst_f1: 0.0059, em: 0.0042, avg: 0.0066, multirc_loss: 0.6871
03/25 04:29:22 PM: Evaluate: task multirc, batch 113 (152): ans_f1: 0.0088, qst_f1: 0.0058, em: 0.0041, avg: 0.0065, multirc_loss: 0.6875
03/25 04:29:42 PM: Evaluate: task multirc, batch 115 (152): ans_f1: 0.0087, qst_f1: 0.0057, em: 0.0040, avg: 0.0064, multirc_loss: 0.6861
03/25 04:29:59 PM: Evaluate: task multirc, batch 117 (152): ans_f1: 0.0086, qst_f1: 0.0055, em: 0.0053, avg: 0.0069, multirc_loss: 0.6855
03/25 04:30:18 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.0084, qst_f1: 0.0055, em: 0.0052, avg: 0.0068, multirc_loss: 0.6856
03/25 04:30:37 PM: Evaluate: task multirc, batch 121 (152): ans_f1: 0.0083, qst_f1: 0.0054, em: 0.0051, avg: 0.0067, multirc_loss: 0.6855
03/25 04:30:56 PM: Evaluate: task multirc, batch 123 (152): ans_f1: 0.0082, qst_f1: 0.0053, em: 0.0051, avg: 0.0066, multirc_loss: 0.6845
03/25 04:31:06 PM: Evaluate: task multirc, batch 124 (152): ans_f1: 0.0081, qst_f1: 0.0053, em: 0.0050, avg: 0.0066, multirc_loss: 0.6847
03/25 04:31:27 PM: Evaluate: task multirc, batch 126 (152): ans_f1: 0.0080, qst_f1: 0.0052, em: 0.0050, avg: 0.0065, multirc_loss: 0.6841
03/25 04:31:43 PM: Evaluate: task multirc, batch 128 (152): ans_f1: 0.0113, qst_f1: 0.0064, em: 0.0049, avg: 0.0081, multirc_loss: 0.6833
03/25 04:31:59 PM: Evaluate: task multirc, batch 130 (152): ans_f1: 0.0111, qst_f1: 0.0063, em: 0.0048, avg: 0.0080, multirc_loss: 0.6835
03/25 04:32:18 PM: Evaluate: task multirc, batch 132 (152): ans_f1: 0.0109, qst_f1: 0.0062, em: 0.0048, avg: 0.0078, multirc_loss: 0.6832
03/25 04:32:37 PM: Evaluate: task multirc, batch 134 (152): ans_f1: 0.0107, qst_f1: 0.0061, em: 0.0047, avg: 0.0077, multirc_loss: 0.6834
03/25 04:32:48 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.0107, qst_f1: 0.0060, em: 0.0046, avg: 0.0077, multirc_loss: 0.6834
03/25 04:32:58 PM: Evaluate: task multirc, batch 136 (152): ans_f1: 0.0106, qst_f1: 0.0060, em: 0.0046, avg: 0.0076, multirc_loss: 0.6836
03/25 04:33:09 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.0115, qst_f1: 0.0071, em: 0.0057, avg: 0.0086, multirc_loss: 0.6831
03/25 04:33:24 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.0114, qst_f1: 0.0070, em: 0.0057, avg: 0.0086, multirc_loss: 0.6821
03/25 04:33:37 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.0113, qst_f1: 0.0070, em: 0.0056, avg: 0.0085, multirc_loss: 0.6812
03/25 04:33:55 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.0122, qst_f1: 0.0076, em: 0.0056, avg: 0.0089, multirc_loss: 0.6799
03/25 04:34:13 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.0121, qst_f1: 0.0075, em: 0.0055, avg: 0.0088, multirc_loss: 0.6789
03/25 04:34:28 PM: Evaluate: task multirc, batch 147 (152): ans_f1: 0.0119, qst_f1: 0.0074, em: 0.0054, avg: 0.0086, multirc_loss: 0.6791
03/25 04:34:46 PM: Evaluate: task multirc, batch 149 (152): ans_f1: 0.0117, qst_f1: 0.0073, em: 0.0053, avg: 0.0085, multirc_loss: 0.6791
03/25 04:35:04 PM: Evaluate: task multirc, batch 151 (152): ans_f1: 0.0115, qst_f1: 0.0072, em: 0.0053, avg: 0.0084, multirc_loss: 0.6800
03/25 04:35:09 PM: Updating LR scheduler:
03/25 04:35:09 PM: 	Best result seen so far for macro_avg: 0.301
03/25 04:35:09 PM: 	# validation passes without improvement: 0
03/25 04:35:09 PM: multirc_loss: training: 0.695170 validation: 0.680943
03/25 04:35:09 PM: macro_avg: validation: 0.008359
03/25 04:35:09 PM: micro_avg: validation: 0.008359
03/25 04:35:09 PM: multirc_ans_f1: training: 0.400000 validation: 0.011472
03/25 04:35:09 PM: multirc_qst_f1: training: 0.125000 validation: 0.007205
03/25 04:35:09 PM: multirc_em: training: 0.625000 validation: 0.005247
03/25 04:35:09 PM: multirc_avg: training: 0.512500 validation: 0.008359
03/25 04:35:09 PM: Global learning rate: 2.5e-06
03/25 04:35:09 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 04:35:45 PM: Update 13: task multirc, steps since last val 1 (total steps = 13): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.4643, avg: 0.2321, multirc_loss: 0.7205
03/25 04:35:45 PM: ***** Step 13 / Validation 13 *****
03/25 04:35:45 PM: multirc: trained on 1 steps (1 batches) since val, 0.001 epochs
03/25 04:35:45 PM: Validating...
03/25 04:36:03 PM: Evaluate: task multirc, batch 2 (152): ans_f1: 0.0000, qst_f1: 0.0000, em: 0.0000, avg: 0.0000, multirc_loss: 0.6704
03/25 04:36:14 PM: Evaluate: task multirc, batch 3 (152): ans_f1: 0.0455, qst_f1: 0.0333, em: 0.0000, avg: 0.0227, multirc_loss: 0.6793
03/25 04:36:29 PM: Evaluate: task multirc, batch 5 (152): ans_f1: 0.0714, qst_f1: 0.0396, em: 0.0000, avg: 0.0357, multirc_loss: 0.7095
03/25 04:36:39 PM: Evaluate: task multirc, batch 6 (152): ans_f1: 0.0612, qst_f1: 0.0333, em: 0.0000, avg: 0.0306, multirc_loss: 0.7073
03/25 04:36:49 PM: Evaluate: task multirc, batch 7 (152): ans_f1: 0.0513, qst_f1: 0.0277, em: 0.0000, avg: 0.0256, multirc_loss: 0.7259
03/25 04:37:00 PM: Evaluate: task multirc, batch 8 (152): ans_f1: 0.0451, qst_f1: 0.0237, em: 0.0000, avg: 0.0226, multirc_loss: 0.7248
03/25 04:37:18 PM: Evaluate: task multirc, batch 10 (152): ans_f1: 0.0366, qst_f1: 0.0204, em: 0.0000, avg: 0.0183, multirc_loss: 0.7397
03/25 04:37:35 PM: Evaluate: task multirc, batch 12 (152): ans_f1: 0.0309, qst_f1: 0.0171, em: 0.0000, avg: 0.0155, multirc_loss: 0.7372
03/25 04:37:49 PM: Evaluate: task multirc, batch 14 (152): ans_f1: 0.0252, qst_f1: 0.0147, em: 0.0000, avg: 0.0126, multirc_loss: 0.7536
03/25 04:38:04 PM: Evaluate: task multirc, batch 16 (152): ans_f1: 0.0227, qst_f1: 0.0133, em: 0.0091, avg: 0.0159, multirc_loss: 0.7432
03/25 04:38:15 PM: Evaluate: task multirc, batch 17 (152): ans_f1: 0.0217, qst_f1: 0.0125, em: 0.0000, avg: 0.0108, multirc_loss: 0.7382
03/25 04:38:33 PM: Evaluate: task multirc, batch 19 (152): ans_f1: 0.0201, qst_f1: 0.0114, em: 0.0000, avg: 0.0101, multirc_loss: 0.7245
03/25 04:38:44 PM: Evaluate: task multirc, batch 20 (152): ans_f1: 0.0194, qst_f1: 0.0109, em: 0.0000, avg: 0.0097, multirc_loss: 0.7203
03/25 04:38:58 PM: Evaluate: task multirc, batch 22 (152): ans_f1: 0.0178, qst_f1: 0.0103, em: 0.0000, avg: 0.0089, multirc_loss: 0.7159
03/25 04:39:17 PM: Evaluate: task multirc, batch 24 (152): ans_f1: 0.0166, qst_f1: 0.0096, em: 0.0000, avg: 0.0083, multirc_loss: 0.7094
03/25 04:39:34 PM: Evaluate: task multirc, batch 26 (152): ans_f1: 0.0157, qst_f1: 0.0091, em: 0.0062, avg: 0.0109, multirc_loss: 0.7041
03/25 04:39:52 PM: Evaluate: task multirc, batch 28 (152): ans_f1: 0.0145, qst_f1: 0.0085, em: 0.0000, avg: 0.0072, multirc_loss: 0.7055
03/25 04:40:03 PM: Evaluate: task multirc, batch 29 (152): ans_f1: 0.0138, qst_f1: 0.0082, em: 0.0000, avg: 0.0069, multirc_loss: 0.7094
03/25 04:40:21 PM: Evaluate: task multirc, batch 31 (152): ans_f1: 0.0131, qst_f1: 0.0076, em: 0.0000, avg: 0.0065, multirc_loss: 0.7078
03/25 04:40:32 PM: Evaluate: task multirc, batch 32 (152): ans_f1: 0.0127, qst_f1: 0.0074, em: 0.0000, avg: 0.0064, multirc_loss: 0.7057
03/25 04:40:48 PM: Evaluate: task multirc, batch 34 (152): ans_f1: 0.0123, qst_f1: 0.0069, em: 0.0047, avg: 0.0085, multirc_loss: 0.6974
03/25 04:41:05 PM: Evaluate: task multirc, batch 36 (152): ans_f1: 0.0118, qst_f1: 0.0066, em: 0.0000, avg: 0.0059, multirc_loss: 0.6932
03/25 04:41:22 PM: Evaluate: task multirc, batch 38 (152): ans_f1: 0.0113, qst_f1: 0.0063, em: 0.0043, avg: 0.0078, multirc_loss: 0.6931
03/25 04:41:40 PM: Evaluate: task multirc, batch 40 (152): ans_f1: 0.0107, qst_f1: 0.0061, em: 0.0000, avg: 0.0054, multirc_loss: 0.6926
03/25 04:41:57 PM: Evaluate: task multirc, batch 42 (152): ans_f1: 0.0102, qst_f1: 0.0058, em: 0.0000, avg: 0.0051, multirc_loss: 0.6920
03/25 04:42:14 PM: Evaluate: task multirc, batch 44 (152): ans_f1: 0.0097, qst_f1: 0.0055, em: 0.0000, avg: 0.0049, multirc_loss: 0.6958
03/25 04:42:32 PM: Evaluate: task multirc, batch 46 (152): ans_f1: 0.0093, qst_f1: 0.0052, em: 0.0000, avg: 0.0046, multirc_loss: 0.6985
03/25 04:42:50 PM: Evaluate: task multirc, batch 48 (152): ans_f1: 0.0089, qst_f1: 0.0050, em: 0.0000, avg: 0.0044, multirc_loss: 0.6975
03/25 04:43:08 PM: Evaluate: task multirc, batch 50 (152): ans_f1: 0.0086, qst_f1: 0.0047, em: 0.0032, avg: 0.0059, multirc_loss: 0.6954
03/25 04:43:27 PM: Evaluate: task multirc, batch 52 (152): ans_f1: 0.0084, qst_f1: 0.0045, em: 0.0062, avg: 0.0073, multirc_loss: 0.6908
03/25 04:43:39 PM: Evaluate: task multirc, batch 53 (152): ans_f1: 0.0083, qst_f1: 0.0045, em: 0.0061, avg: 0.0072, multirc_loss: 0.6898
03/25 04:43:58 PM: Evaluate: task multirc, batch 55 (152): ans_f1: 0.0081, qst_f1: 0.0043, em: 0.0059, avg: 0.0070, multirc_loss: 0.6865
03/25 04:44:16 PM: Evaluate: task multirc, batch 57 (152): ans_f1: 0.0078, qst_f1: 0.0042, em: 0.0057, avg: 0.0067, multirc_loss: 0.6845
03/25 04:44:37 PM: Evaluate: task multirc, batch 59 (152): ans_f1: 0.0075, qst_f1: 0.0040, em: 0.0054, avg: 0.0065, multirc_loss: 0.6855
03/25 04:44:55 PM: Evaluate: task multirc, batch 61 (152): ans_f1: 0.0074, qst_f1: 0.0038, em: 0.0052, avg: 0.0063, multirc_loss: 0.6820
03/25 04:45:10 PM: Evaluate: task multirc, batch 63 (152): ans_f1: 0.0071, qst_f1: 0.0037, em: 0.0051, avg: 0.0061, multirc_loss: 0.6829
03/25 04:45:30 PM: Evaluate: task multirc, batch 65 (152): ans_f1: 0.0068, qst_f1: 0.0036, em: 0.0048, avg: 0.0058, multirc_loss: 0.6846
03/25 04:45:41 PM: Evaluate: task multirc, batch 66 (152): ans_f1: 0.0067, qst_f1: 0.0035, em: 0.0048, avg: 0.0057, multirc_loss: 0.6841
03/25 04:45:53 PM: Evaluate: task multirc, batch 67 (152): ans_f1: 0.0066, qst_f1: 0.0035, em: 0.0047, avg: 0.0057, multirc_loss: 0.6838
03/25 04:46:16 PM: Evaluate: task multirc, batch 69 (152): ans_f1: 0.0064, qst_f1: 0.0033, em: 0.0046, avg: 0.0055, multirc_loss: 0.6836
03/25 04:46:27 PM: Evaluate: task multirc, batch 70 (152): ans_f1: 0.0063, qst_f1: 0.0033, em: 0.0045, avg: 0.0054, multirc_loss: 0.6839
03/25 04:46:43 PM: Evaluate: task multirc, batch 72 (152): ans_f1: 0.0061, qst_f1: 0.0032, em: 0.0044, avg: 0.0053, multirc_loss: 0.6838
03/25 04:46:53 PM: Evaluate: task multirc, batch 73 (152): ans_f1: 0.0061, qst_f1: 0.0032, em: 0.0044, avg: 0.0052, multirc_loss: 0.6835
03/25 04:47:11 PM: Evaluate: task multirc, batch 75 (152): ans_f1: 0.0059, qst_f1: 0.0031, em: 0.0042, avg: 0.0051, multirc_loss: 0.6817
03/25 04:47:22 PM: Evaluate: task multirc, batch 76 (152): ans_f1: 0.0059, qst_f1: 0.0031, em: 0.0042, avg: 0.0050, multirc_loss: 0.6812
03/25 04:47:38 PM: Evaluate: task multirc, batch 78 (152): ans_f1: 0.0057, qst_f1: 0.0030, em: 0.0041, avg: 0.0049, multirc_loss: 0.6804
03/25 04:47:56 PM: Evaluate: task multirc, batch 80 (152): ans_f1: 0.0056, qst_f1: 0.0029, em: 0.0040, avg: 0.0048, multirc_loss: 0.6804
03/25 04:48:14 PM: Evaluate: task multirc, batch 82 (152): ans_f1: 0.0054, qst_f1: 0.0028, em: 0.0058, avg: 0.0056, multirc_loss: 0.6813
03/25 04:48:33 PM: Evaluate: task multirc, batch 84 (152): ans_f1: 0.0053, qst_f1: 0.0028, em: 0.0038, avg: 0.0045, multirc_loss: 0.6824
03/25 04:48:45 PM: Evaluate: task multirc, batch 85 (152): ans_f1: 0.0053, qst_f1: 0.0027, em: 0.0037, avg: 0.0045, multirc_loss: 0.6813
03/25 04:48:58 PM: Evaluate: task multirc, batch 86 (152): ans_f1: 0.0052, qst_f1: 0.0027, em: 0.0037, avg: 0.0045, multirc_loss: 0.6804
03/25 04:49:10 PM: Evaluate: task multirc, batch 87 (152): ans_f1: 0.0052, qst_f1: 0.0027, em: 0.0037, avg: 0.0044, multirc_loss: 0.6800
03/25 04:49:23 PM: Evaluate: task multirc, batch 88 (152): ans_f1: 0.0051, qst_f1: 0.0027, em: 0.0036, avg: 0.0043, multirc_loss: 0.6821
03/25 04:49:35 PM: Evaluate: task multirc, batch 89 (152): ans_f1: 0.0050, qst_f1: 0.0026, em: 0.0036, avg: 0.0043, multirc_loss: 0.6837
03/25 04:49:49 PM: Evaluate: task multirc, batch 91 (152): ans_f1: 0.0048, qst_f1: 0.0026, em: 0.0035, avg: 0.0042, multirc_loss: 0.6841
03/25 04:50:01 PM: Evaluate: task multirc, batch 92 (152): ans_f1: 0.0048, qst_f1: 0.0026, em: 0.0035, avg: 0.0041, multirc_loss: 0.6837
03/25 04:50:23 PM: Evaluate: task multirc, batch 94 (152): ans_f1: 0.0047, qst_f1: 0.0025, em: 0.0034, avg: 0.0041, multirc_loss: 0.6826
03/25 04:50:36 PM: Evaluate: task multirc, batch 95 (152): ans_f1: 0.0047, qst_f1: 0.0025, em: 0.0034, avg: 0.0040, multirc_loss: 0.6823
03/25 04:50:49 PM: Evaluate: task multirc, batch 96 (152): ans_f1: 0.0046, qst_f1: 0.0025, em: 0.0034, avg: 0.0040, multirc_loss: 0.6826
03/25 04:51:10 PM: Evaluate: task multirc, batch 98 (152): ans_f1: 0.0045, qst_f1: 0.0024, em: 0.0049, avg: 0.0047, multirc_loss: 0.6845
03/25 04:51:22 PM: Evaluate: task multirc, batch 99 (152): ans_f1: 0.0044, qst_f1: 0.0024, em: 0.0048, avg: 0.0046, multirc_loss: 0.6852
03/25 04:51:39 PM: Evaluate: task multirc, batch 101 (152): ans_f1: 0.0057, qst_f1: 0.0039, em: 0.0047, avg: 0.0052, multirc_loss: 0.6865
03/25 04:51:54 PM: Evaluate: task multirc, batch 103 (152): ans_f1: 0.0056, qst_f1: 0.0038, em: 0.0046, avg: 0.0051, multirc_loss: 0.6902
03/25 04:52:13 PM: Evaluate: task multirc, batch 105 (152): ans_f1: 0.0055, qst_f1: 0.0037, em: 0.0045, avg: 0.0050, multirc_loss: 0.6894
03/25 04:52:33 PM: Evaluate: task multirc, batch 107 (152): ans_f1: 0.0054, qst_f1: 0.0036, em: 0.0044, avg: 0.0049, multirc_loss: 0.6890
03/25 04:52:51 PM: Evaluate: task multirc, batch 109 (152): ans_f1: 0.0053, qst_f1: 0.0035, em: 0.0043, avg: 0.0048, multirc_loss: 0.6902
03/25 04:53:08 PM: Evaluate: task multirc, batch 111 (152): ans_f1: 0.0052, qst_f1: 0.0035, em: 0.0042, avg: 0.0047, multirc_loss: 0.6908
03/25 04:53:27 PM: Evaluate: task multirc, batch 113 (152): ans_f1: 0.0051, qst_f1: 0.0034, em: 0.0041, avg: 0.0046, multirc_loss: 0.6912
03/25 04:53:38 PM: Evaluate: task multirc, batch 114 (152): ans_f1: 0.0050, qst_f1: 0.0034, em: 0.0054, avg: 0.0052, multirc_loss: 0.6904
03/25 04:53:51 PM: Evaluate: task multirc, batch 115 (152): ans_f1: 0.0050, qst_f1: 0.0033, em: 0.0040, avg: 0.0045, multirc_loss: 0.6898
03/25 04:54:01 PM: Evaluate: task multirc, batch 116 (152): ans_f1: 0.0050, qst_f1: 0.0033, em: 0.0053, avg: 0.0051, multirc_loss: 0.6898
03/25 04:54:14 PM: Evaluate: task multirc, batch 117 (152): ans_f1: 0.0049, qst_f1: 0.0032, em: 0.0053, avg: 0.0051, multirc_loss: 0.6891
03/25 04:54:36 PM: Evaluate: task multirc, batch 119 (152): ans_f1: 0.0048, qst_f1: 0.0032, em: 0.0052, avg: 0.0050, multirc_loss: 0.6893
03/25 04:54:48 PM: Evaluate: task multirc, batch 120 (152): ans_f1: 0.0048, qst_f1: 0.0032, em: 0.0065, avg: 0.0056, multirc_loss: 0.6889
03/25 04:55:08 PM: Evaluate: task multirc, batch 122 (152): ans_f1: 0.0047, qst_f1: 0.0031, em: 0.0051, avg: 0.0049, multirc_loss: 0.6881
03/25 04:55:30 PM: Evaluate: task multirc, batch 124 (152): ans_f1: 0.0047, qst_f1: 0.0031, em: 0.0050, avg: 0.0048, multirc_loss: 0.6884
03/25 04:55:42 PM: Evaluate: task multirc, batch 125 (152): ans_f1: 0.0046, qst_f1: 0.0031, em: 0.0050, avg: 0.0048, multirc_loss: 0.6885
03/25 04:55:58 PM: Evaluate: task multirc, batch 126 (152): ans_f1: 0.0046, qst_f1: 0.0031, em: 0.0050, avg: 0.0048, multirc_loss: 0.6878
03/25 04:56:11 PM: Evaluate: task multirc, batch 127 (152): ans_f1: 0.0046, qst_f1: 0.0031, em: 0.0050, avg: 0.0048, multirc_loss: 0.6872
03/25 04:56:27 PM: Evaluate: task multirc, batch 129 (152): ans_f1: 0.0078, qst_f1: 0.0042, em: 0.0049, avg: 0.0064, multirc_loss: 0.6867
03/25 04:56:38 PM: Evaluate: task multirc, batch 130 (152): ans_f1: 0.0078, qst_f1: 0.0042, em: 0.0048, avg: 0.0063, multirc_loss: 0.6871
03/25 04:56:53 PM: Evaluate: task multirc, batch 131 (152): ans_f1: 0.0077, qst_f1: 0.0042, em: 0.0048, avg: 0.0063, multirc_loss: 0.6869
03/25 04:57:06 PM: Evaluate: task multirc, batch 132 (152): ans_f1: 0.0077, qst_f1: 0.0041, em: 0.0048, avg: 0.0062, multirc_loss: 0.6868
03/25 04:57:22 PM: Evaluate: task multirc, batch 133 (152): ans_f1: 0.0076, qst_f1: 0.0041, em: 0.0047, avg: 0.0062, multirc_loss: 0.6866
03/25 04:57:34 PM: Evaluate: task multirc, batch 134 (152): ans_f1: 0.0075, qst_f1: 0.0041, em: 0.0047, avg: 0.0061, multirc_loss: 0.6870
03/25 04:57:48 PM: Evaluate: task multirc, batch 135 (152): ans_f1: 0.0075, qst_f1: 0.0040, em: 0.0046, avg: 0.0061, multirc_loss: 0.6871
03/25 04:58:00 PM: Evaluate: task multirc, batch 136 (152): ans_f1: 0.0074, qst_f1: 0.0040, em: 0.0046, avg: 0.0060, multirc_loss: 0.6873
03/25 04:58:13 PM: Evaluate: task multirc, batch 137 (152): ans_f1: 0.0084, qst_f1: 0.0051, em: 0.0057, avg: 0.0071, multirc_loss: 0.6868
03/25 04:58:29 PM: Evaluate: task multirc, batch 139 (152): ans_f1: 0.0083, qst_f1: 0.0051, em: 0.0057, avg: 0.0070, multirc_loss: 0.6857
03/25 04:58:45 PM: Evaluate: task multirc, batch 141 (152): ans_f1: 0.0082, qst_f1: 0.0050, em: 0.0056, avg: 0.0069, multirc_loss: 0.6846
03/25 04:59:00 PM: Evaluate: task multirc, batch 142 (152): ans_f1: 0.0082, qst_f1: 0.0050, em: 0.0056, avg: 0.0069, multirc_loss: 0.6838
03/25 04:59:12 PM: Evaluate: task multirc, batch 143 (152): ans_f1: 0.0082, qst_f1: 0.0050, em: 0.0056, avg: 0.0069, multirc_loss: 0.6833
03/25 04:59:32 PM: Evaluate: task multirc, batch 145 (152): ans_f1: 0.0081, qst_f1: 0.0049, em: 0.0055, avg: 0.0068, multirc_loss: 0.6823
03/25 04:59:45 PM: Evaluate: task multirc, batch 146 (152): ans_f1: 0.0080, qst_f1: 0.0049, em: 0.0054, avg: 0.0067, multirc_loss: 0.6828
03/25 05:00:10 PM: Evaluate: task multirc, batch 148 (152): ans_f1: 0.0079, qst_f1: 0.0048, em: 0.0064, avg: 0.0072, multirc_loss: 0.6821
03/25 05:00:22 PM: Evaluate: task multirc, batch 149 (152): ans_f1: 0.0078, qst_f1: 0.0048, em: 0.0053, avg: 0.0066, multirc_loss: 0.6824
03/25 05:00:35 PM: Evaluate: task multirc, batch 150 (152): ans_f1: 0.0078, qst_f1: 0.0047, em: 0.0064, avg: 0.0071, multirc_loss: 0.6825
03/25 05:00:50 PM: Evaluate: task multirc, batch 152 (152): ans_f1: 0.0077, qst_f1: 0.0047, em: 0.0052, avg: 0.0065, multirc_loss: 0.6845
03/25 05:00:51 PM: Updating LR scheduler:
03/25 05:00:51 PM: 	Best result seen so far for macro_avg: 0.301
03/25 05:00:51 PM: 	# validation passes without improvement: 1
03/25 05:00:51 PM: Ran out of early stopping patience. Stopping training.
03/25 05:00:51 PM: multirc_loss: training: 0.720493 validation: 0.684476
03/25 05:00:51 PM: macro_avg: validation: 0.006457
03/25 05:00:51 PM: micro_avg: validation: 0.006457
03/25 05:00:51 PM: multirc_ans_f1: training: 0.000000 validation: 0.007667
03/25 05:00:51 PM: multirc_qst_f1: training: 0.000000 validation: 0.004687
03/25 05:00:51 PM: multirc_em: training: 0.464286 validation: 0.005247
03/25 05:00:51 PM: multirc_avg: training: 0.232143 validation: 0.006457
03/25 05:00:51 PM: Global learning rate: 2.5e-06
03/25 05:00:51 PM: Saving checkpoints to: /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 05:00:52 PM: Stopped training after 13 validation checks
03/25 05:00:52 PM: Trained multirc for 13 steps or 0.015 epochs
03/25 05:00:52 PM: ***** VALIDATION RESULTS *****
03/25 05:00:52 PM: multirc_avg (for best val pass 7): multirc_loss: 0.70646, macro_avg: 0.30132, micro_avg: 0.30132, multirc_ans_f1: 0.58690, multirc_qst_f1: 0.56048, multirc_em: 0.01574, multirc_avg: 0.30132
03/25 05:00:52 PM: micro_avg (for best val pass 7): multirc_loss: 0.70646, macro_avg: 0.30132, micro_avg: 0.30132, multirc_ans_f1: 0.58690, multirc_qst_f1: 0.56048, multirc_em: 0.01574, multirc_avg: 0.30132
03/25 05:00:52 PM: macro_avg (for best val pass 7): multirc_loss: 0.70646, macro_avg: 0.30132, micro_avg: 0.30132, multirc_ans_f1: 0.58690, multirc_qst_f1: 0.56048, multirc_em: 0.01574, multirc_avg: 0.30132
03/25 05:00:52 PM: Evaluating...
03/25 05:00:53 PM: Loaded model state from /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0/multirc/model_state_target_train_val_7.best.th
03/25 05:00:53 PM: Evaluating on: multirc, split: val
03/25 05:01:30 PM: 	Task multirc: batch 3
03/25 05:02:09 PM: 	Task multirc: batch 7
03/25 05:02:44 PM: 	Task multirc: batch 11
03/25 05:03:23 PM: 	Task multirc: batch 16
03/25 05:04:00 PM: 	Task multirc: batch 20
03/25 05:04:34 PM: 	Task multirc: batch 24
03/25 05:05:14 PM: 	Task multirc: batch 28
03/25 05:05:46 PM: 	Task multirc: batch 31
03/25 05:06:20 PM: 	Task multirc: batch 35
03/25 05:06:58 PM: 	Task multirc: batch 39
03/25 05:07:36 PM: 	Task multirc: batch 43
03/25 05:08:16 PM: 	Task multirc: batch 47
03/25 05:08:57 PM: 	Task multirc: batch 51
03/25 05:09:31 PM: 	Task multirc: batch 54
03/25 05:10:11 PM: 	Task multirc: batch 58
03/25 05:10:49 PM: 	Task multirc: batch 62
03/25 05:11:22 PM: 	Task multirc: batch 65
03/25 05:11:54 PM: 	Task multirc: batch 68
03/25 05:12:38 PM: 	Task multirc: batch 72
03/25 05:13:12 PM: 	Task multirc: batch 75
03/25 05:13:49 PM: 	Task multirc: batch 79
03/25 05:14:27 PM: 	Task multirc: batch 83
03/25 05:15:05 PM: 	Task multirc: batch 86
03/25 05:15:39 PM: 	Task multirc: batch 89
03/25 05:16:18 PM: 	Task multirc: batch 93
03/25 05:16:56 PM: 	Task multirc: batch 96
03/25 05:17:32 PM: 	Task multirc: batch 99
03/25 05:18:05 PM: 	Task multirc: batch 103
03/25 05:18:38 PM: 	Task multirc: batch 106
03/25 05:19:15 PM: 	Task multirc: batch 109
03/25 05:19:47 PM: 	Task multirc: batch 112
03/25 05:20:25 PM: 	Task multirc: batch 115
03/25 05:21:04 PM: 	Task multirc: batch 118
03/25 05:21:44 PM: 	Task multirc: batch 121
03/25 05:22:19 PM: 	Task multirc: batch 124
03/25 05:22:53 PM: 	Task multirc: batch 127
03/25 05:23:34 PM: 	Task multirc: batch 131
03/25 05:24:08 PM: 	Task multirc: batch 134
03/25 05:24:50 PM: 	Task multirc: batch 137
03/25 05:25:24 PM: 	Task multirc: batch 141
03/25 05:25:57 PM: 	Task multirc: batch 144
03/25 05:26:32 PM: 	Task multirc: batch 147
03/25 05:27:08 PM: 	Task multirc: batch 150
03/25 05:27:14 PM: Task 'multirc': sorting predictions by 'idx'
03/25 05:27:14 PM: Finished evaluating on: multirc
03/25 05:27:14 PM: Task 'multirc': Wrote predictions to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 05:27:14 PM: Wrote all preds for split 'val' to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 05:27:14 PM: Evaluating on: multirc, split: test
03/25 05:27:53 PM: 	Task multirc: batch 2
03/25 05:28:25 PM: 	Task multirc: batch 5
03/25 05:28:56 PM: 	Task multirc: batch 8
03/25 05:29:32 PM: 	Task multirc: batch 11
03/25 05:30:03 PM: 	Task multirc: batch 14
03/25 05:30:43 PM: 	Task multirc: batch 17
03/25 05:31:15 PM: 	Task multirc: batch 20
03/25 05:31:49 PM: 	Task multirc: batch 23
03/25 05:32:26 PM: 	Task multirc: batch 26
03/25 05:32:59 PM: 	Task multirc: batch 29
03/25 05:33:37 PM: 	Task multirc: batch 33
03/25 05:34:15 PM: 	Task multirc: batch 36
03/25 05:34:51 PM: 	Task multirc: batch 39
03/25 05:35:29 PM: 	Task multirc: batch 42
03/25 05:36:08 PM: 	Task multirc: batch 46
03/25 05:36:47 PM: 	Task multirc: batch 50
03/25 05:37:19 PM: 	Task multirc: batch 53
03/25 05:37:56 PM: 	Task multirc: batch 57
03/25 05:38:35 PM: 	Task multirc: batch 61
03/25 05:39:08 PM: 	Task multirc: batch 64
03/25 05:39:43 PM: 	Task multirc: batch 68
03/25 05:40:15 PM: 	Task multirc: batch 71
03/25 05:40:50 PM: 	Task multirc: batch 74
03/25 05:41:20 PM: 	Task multirc: batch 77
03/25 05:41:58 PM: 	Task multirc: batch 80
03/25 05:42:31 PM: 	Task multirc: batch 83
03/25 05:43:07 PM: 	Task multirc: batch 86
03/25 05:43:41 PM: 	Task multirc: batch 89
03/25 05:44:14 PM: 	Task multirc: batch 92
03/25 05:44:52 PM: 	Task multirc: batch 96
03/25 05:45:34 PM: 	Task multirc: batch 100
03/25 05:46:12 PM: 	Task multirc: batch 103
03/25 05:46:44 PM: 	Task multirc: batch 106
03/25 05:47:17 PM: 	Task multirc: batch 110
03/25 05:47:51 PM: 	Task multirc: batch 113
03/25 05:48:23 PM: 	Task multirc: batch 116
03/25 05:49:00 PM: 	Task multirc: batch 119
03/25 05:49:40 PM: 	Task multirc: batch 123
03/25 05:50:14 PM: 	Task multirc: batch 126
03/25 05:50:48 PM: 	Task multirc: batch 130
03/25 05:51:21 PM: 	Task multirc: batch 133
03/25 05:51:55 PM: 	Task multirc: batch 137
03/25 05:52:36 PM: 	Task multirc: batch 141
03/25 05:53:06 PM: 	Task multirc: batch 145
03/25 05:53:42 PM: 	Task multirc: batch 150
03/25 05:54:14 PM: 	Task multirc: batch 153
03/25 05:54:44 PM: 	Task multirc: batch 156
03/25 05:55:18 PM: 	Task multirc: batch 159
03/25 05:55:50 PM: 	Task multirc: batch 162
03/25 05:56:27 PM: 	Task multirc: batch 165
03/25 05:57:05 PM: 	Task multirc: batch 168
03/25 05:57:36 PM: 	Task multirc: batch 171
03/25 05:58:10 PM: 	Task multirc: batch 175
03/25 05:58:47 PM: 	Task multirc: batch 179
03/25 05:59:25 PM: 	Task multirc: batch 182
03/25 06:00:00 PM: 	Task multirc: batch 185
03/25 06:00:37 PM: 	Task multirc: batch 188
03/25 06:01:17 PM: 	Task multirc: batch 191
03/25 06:01:49 PM: 	Task multirc: batch 194
03/25 06:02:37 PM: 	Task multirc: batch 197
03/25 06:03:12 PM: 	Task multirc: batch 200
03/25 06:03:52 PM: 	Task multirc: batch 203
03/25 06:04:26 PM: 	Task multirc: batch 206
03/25 06:04:56 PM: 	Task multirc: batch 209
03/25 06:05:28 PM: 	Task multirc: batch 212
03/25 06:06:09 PM: 	Task multirc: batch 215
03/25 06:06:51 PM: 	Task multirc: batch 219
03/25 06:07:22 PM: 	Task multirc: batch 221
03/25 06:07:53 PM: 	Task multirc: batch 224
03/25 06:08:29 PM: 	Task multirc: batch 227
03/25 06:09:00 PM: 	Task multirc: batch 230
03/25 06:09:30 PM: 	Task multirc: batch 233
03/25 06:10:07 PM: 	Task multirc: batch 236
03/25 06:10:41 PM: 	Task multirc: batch 239
03/25 06:11:15 PM: 	Task multirc: batch 242
03/25 06:11:49 PM: 	Task multirc: batch 245
03/25 06:12:22 PM: 	Task multirc: batch 249
03/25 06:12:58 PM: 	Task multirc: batch 252
03/25 06:13:34 PM: 	Task multirc: batch 255
03/25 06:14:10 PM: 	Task multirc: batch 259
03/25 06:14:42 PM: 	Task multirc: batch 262
03/25 06:15:21 PM: 	Task multirc: batch 265
03/25 06:15:57 PM: 	Task multirc: batch 269
03/25 06:16:29 PM: 	Task multirc: batch 272
03/25 06:17:04 PM: 	Task multirc: batch 275
03/25 06:17:44 PM: 	Task multirc: batch 278
03/25 06:18:17 PM: 	Task multirc: batch 282
03/25 06:18:53 PM: 	Task multirc: batch 285
03/25 06:19:29 PM: 	Task multirc: batch 288
03/25 06:20:07 PM: 	Task multirc: batch 292
03/25 06:20:42 PM: 	Task multirc: batch 295
03/25 06:21:19 PM: 	Task multirc: batch 298
03/25 06:21:57 PM: 	Task multirc: batch 301
03/25 06:22:08 PM: Task 'multirc': sorting predictions by 'idx'
03/25 06:22:08 PM: Finished evaluating on: multirc
03/25 06:22:09 PM: Task 'multirc': Wrote predictions to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 06:22:09 PM: Wrote all preds for split 'test' to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/run0
03/25 06:22:09 PM: Writing results for split 'val' to /Users/hpaila/Projects/NLP/multi_rc/baseline/output/bert_uncased_multirc_PT/results.tsv
03/25 06:22:09 PM: micro_avg: 0.301, macro_avg: 0.301, multirc_ans_f1: 0.587, multirc_qst_f1: 0.560, multirc_em: 0.016, multirc_avg: 0.301
03/25 06:22:09 PM: Done!
