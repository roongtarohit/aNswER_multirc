04/02 06:08:03 PM: Git branch: HEAD
04/02 06:08:03 PM: Git SHA: f4a155efabb900c8b3da314cca47384795f97ab6
04/02 06:08:21 PM: Parsed args: 
{
  "classifier": "log_reg",
  "do_pretrain": 0,
  "do_target_task_training": 0,
  "dropout": 0.1,
  "dropout_embs": 0.1,
  "exp_dir": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/",
  "exp_name": "bert_uncased_multirc_PT_reduced",
  "input_module": "bert-base-uncased",
  "load_eval_checkpoint": "bert_uncased_multirc_PT_reduced/bert_uncased_multirc/multirc/model_state_target_train_val_7.best.th",
  "load_target_train_checkpoint": "bert_uncased_multirc_PT_reduced/bert_uncased_multirc/multirc/model_state_target_train_val_7.best.th",
  "local_log_path": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit/log.log",
  "lr": 5e-06,
  "lr_patience": 4,
  "max_epochs": 10,
  "max_seq_len": 256,
  "max_vals": 10,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "remote_log_name": "bert_uncased_multirc_PT_reduced__eval_logit",
  "run_dir": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit",
  "run_name": "eval_logit",
  "s2s": {
    "attention": "none"
  },
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "multirc",
  "target_train_max_vals": 10,
  "target_train_val_interval": 1,
  "transfer_paradigm": "finetune",
  "transformers_output_mode": "top",
  "val_interval": 1,
  "write_preds": "val,test",
  "write_strict_glue_format": 1
}
04/02 06:08:21 PM: Saved config to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit/params.conf
04/02 06:08:21 PM: Using random seed 1234
04/02 06:08:21 PM: Loading tasks...
04/02 06:08:21 PM: Writing pre-preprocessed tasks to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/
04/02 06:08:21 PM: 	Loaded existing task multirc
04/02 06:08:22 PM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
04/02 06:08:22 PM: 	Loaded existing task sst
04/02 06:08:22 PM: 	Task 'sst': |train|=67349 |val|=872 |test|=1821
04/02 06:08:22 PM: 	Finished loading tasks: multirc sst.
04/02 06:08:22 PM: Loading token dictionary from /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/vocab.
04/02 06:08:22 PM: 	Loaded vocab from /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/vocab
04/02 06:08:22 PM: 	Vocab namespace chars: size 78
04/02 06:08:22 PM: 	Vocab namespace bert_uncased: size 30524
04/02 06:08:22 PM: 	Vocab namespace tokens: size 17932
04/02 06:08:22 PM: 	Finished building vocab.
04/02 06:08:22 PM: 	Task 'multirc', split 'train': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__train_data
04/02 06:08:22 PM: 	Task 'multirc', split 'val': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__val_data
04/02 06:08:22 PM: 	Task 'multirc', split 'test': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__test_data
04/02 06:08:22 PM: 	Task 'sst', split 'train': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__train_data
04/02 06:08:22 PM: 	Task 'sst', split 'val': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__val_data
04/02 06:08:22 PM: 	Task 'sst', split 'test': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__test_data
04/02 06:08:22 PM: 	Finished indexing tasks
04/02 06:08:22 PM: 	Creating trimmed target-only version of multirc train.
04/02 06:08:22 PM: 	Creating trimmed pretraining-only version of sst train.
04/02 06:08:22 PM: 	  Training on sst
04/02 06:08:22 PM: 	  Evaluating on multirc
04/02 06:08:22 PM: 	Finished loading tasks in 0.937s
04/02 06:08:22 PM: 	 Tasks: ['multirc', 'sst']
04/02 06:08:22 PM: Building model...
04/02 06:08:22 PM: Using BERT model (bert-base-uncased).
04/02 06:09:47 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
04/02 06:10:03 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

04/02 06:10:04 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
04/02 06:10:04 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/home/soujanya/Projects/jiant/jiant/__main__.py", line 558, in main
    model = build_model(args, vocab, word_embs, tasks, cuda_device)
  File "/home/soujanya/Projects/jiant/jiant/models.py", line 243, in build_model
    embedder = BertEmbedderModule(args)
  File "/home/soujanya/Projects/jiant/jiant/huggingface_transformers_interface/modules.py", line 262, in __init__
    args.input_module, cache_dir=self.cache_dir, output_hidden_states=True
  File "/home/soujanya/anaconda3/envs/jiant/lib/python3.6/site-packages/transformers/modeling_utils.py", line 411, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/soujanya/anaconda3/envs/jiant/lib/python3.6/site-packages/transformers/modeling_bert.py", line 619, in __init__
    self.embeddings = BertEmbeddings(config)
  File "/home/soujanya/anaconda3/envs/jiant/lib/python3.6/site-packages/transformers/modeling_bert.py", line 162, in __init__
    self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)
  File "/home/soujanya/anaconda3/envs/jiant/lib/python3.6/site-packages/torch/nn/modules/sparse.py", line 99, in __init__
    self.weight = Parameter(torch.Tensor(num_embeddings, embedding_dim))
RuntimeError: [enforce fail at CPUAllocator.cpp:56] posix_memalign(&data, gAlignment, nbytes) == 0. 12 vs 0

04/02 06:10:35 PM: Git branch: HEAD
04/02 06:10:35 PM: Git SHA: f4a155efabb900c8b3da314cca47384795f97ab6
04/02 06:10:35 PM: Parsed args: 
{
  "classifier": "log_reg",
  "do_pretrain": 0,
  "do_target_task_training": 0,
  "dropout": 0.1,
  "dropout_embs": 0.1,
  "exp_dir": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/",
  "exp_name": "bert_uncased_multirc_PT_reduced",
  "input_module": "bert-base-uncased",
  "load_eval_checkpoint": "bert_uncased_multirc_PT_reduced/bert_uncased_multirc/multirc/model_state_target_train_val_7.best.th",
  "load_target_train_checkpoint": "bert_uncased_multirc_PT_reduced/bert_uncased_multirc/multirc/model_state_target_train_val_7.best.th",
  "local_log_path": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit/log.log",
  "lr": 5e-06,
  "lr_patience": 4,
  "max_epochs": 10,
  "max_seq_len": 256,
  "max_vals": 10,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "remote_log_name": "bert_uncased_multirc_PT_reduced__eval_logit",
  "run_dir": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit",
  "run_name": "eval_logit",
  "s2s": {
    "attention": "none"
  },
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "multirc",
  "target_train_max_vals": 10,
  "target_train_val_interval": 1,
  "transfer_paradigm": "finetune",
  "transformers_output_mode": "top",
  "val_interval": 1,
  "write_preds": "val,test",
  "write_strict_glue_format": 1
}
04/02 06:10:35 PM: Saved config to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit/params.conf
04/02 06:10:35 PM: Using random seed 1234
04/02 06:10:35 PM: Loading tasks...
04/02 06:10:36 PM: Writing pre-preprocessed tasks to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/
04/02 06:10:36 PM: 	Loaded existing task multirc
04/02 06:10:36 PM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
04/02 06:10:36 PM: 	Loaded existing task sst
04/02 06:10:36 PM: 	Task 'sst': |train|=67349 |val|=872 |test|=1821
04/02 06:10:36 PM: 	Finished loading tasks: multirc sst.
04/02 06:10:36 PM: Loading token dictionary from /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/vocab.
04/02 06:10:36 PM: 	Loaded vocab from /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/vocab
04/02 06:10:36 PM: 	Vocab namespace chars: size 78
04/02 06:10:36 PM: 	Vocab namespace bert_uncased: size 30524
04/02 06:10:36 PM: 	Vocab namespace tokens: size 17932
04/02 06:10:36 PM: 	Finished building vocab.
04/02 06:10:36 PM: 	Task 'multirc', split 'train': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__train_data
04/02 06:10:36 PM: 	Task 'multirc', split 'val': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__val_data
04/02 06:10:36 PM: 	Task 'multirc', split 'test': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__test_data
04/02 06:10:36 PM: 	Task 'sst', split 'train': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__train_data
04/02 06:10:36 PM: 	Task 'sst', split 'val': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__val_data
04/02 06:10:36 PM: 	Task 'sst', split 'test': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__test_data
04/02 06:10:36 PM: 	Finished indexing tasks
04/02 06:10:36 PM: 	Creating trimmed target-only version of multirc train.
04/02 06:10:36 PM: 	Creating trimmed pretraining-only version of sst train.
04/02 06:10:36 PM: 	  Training on sst
04/02 06:10:36 PM: 	  Evaluating on multirc
04/02 06:10:36 PM: 	Finished loading tasks in 0.275s
04/02 06:10:36 PM: 	 Tasks: ['multirc', 'sst']
04/02 06:10:36 PM: Building model...
04/02 06:10:36 PM: Using BERT model (bert-base-uncased).
04/02 06:10:36 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
04/02 06:10:36 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

04/02 06:10:37 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
04/02 06:10:39 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
04/02 06:10:40 PM: Initializing parameters
04/02 06:10:40 PM: Done initializing parameters; the following parameters are using their default initialization from their code
04/02 06:10:40 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
04/02 06:10:40 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
04/02 06:10:40 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
04/02 06:10:40 PM:    _text_field_embedder.model.pooler.dense.bias
04/02 06:10:40 PM:    _text_field_embedder.model.pooler.dense.weight
04/02 06:10:40 PM: 	Task 'multirc' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
04/02 06:10:40 PM: 	Task 'sst' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "sst"
}
04/02 06:10:40 PM: Model specification:
04/02 06:10:40 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.1)
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (sst_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
04/02 06:10:40 PM: Model parameters:
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 23440896 with torch.Size([30522, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:10:40 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:10:40 PM: 	multirc_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
04/02 06:10:40 PM: 	multirc_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
04/02 06:10:40 PM: 	sst_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
04/02 06:10:40 PM: 	sst_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
04/02 06:10:40 PM: Total number of parameters: 109485316 (1.09485e+08)
04/02 06:10:40 PM: Number of trainable parameters: 109485316 (1.09485e+08)
04/02 06:10:40 PM: Finished building model in 3.816s
04/02 06:10:40 PM: Evauluating a target task model on tasks {'multirc'} without training it in this run. It's up to you to ensure that you are loading parameters that were sufficiently trained for this task.
04/02 06:10:40 PM: Will run the following steps for this experiment:
Loading model from path: bert_uncased_multirc_PT_reduced/bert_uncased_multirc/multirc/model_state_target_train_val_7.best.th 
Evaluating model on tasks: multirc 

04/02 06:10:40 PM: Evaluating...
04/02 06:10:55 PM: Loaded model state from bert_uncased_multirc_PT_reduced/bert_uncased_multirc/multirc/model_state_target_train_val_7.best.th
04/02 06:10:55 PM: Evaluating on: multirc, split: val
04/02 06:11:27 PM: 	Task multirc: batch 3
04/02 06:15:04 PM: Git branch: HEAD
04/02 06:15:04 PM: Git SHA: f4a155efabb900c8b3da314cca47384795f97ab6
04/02 06:15:04 PM: Parsed args: 
{
  "classifier": "log_reg",
  "do_pretrain": 0,
  "do_target_task_training": 0,
  "dropout": 0.1,
  "dropout_embs": 0.1,
  "exp_dir": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/",
  "exp_name": "bert_uncased_multirc_PT_reduced",
  "input_module": "bert-base-uncased",
  "load_eval_checkpoint": "bert_uncased_multirc_PT_reduced/bert_uncased_multirc/multirc/model_state_target_train_val_7.best.th",
  "load_target_train_checkpoint": "bert_uncased_multirc_PT_reduced/bert_uncased_multirc/multirc/model_state_target_train_val_7.best.th",
  "local_log_path": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit/log.log",
  "lr": 5e-06,
  "lr_patience": 4,
  "max_epochs": 10,
  "max_seq_len": 256,
  "max_vals": 10,
  "optimizer": "bert_adam",
  "pair_attn": 0,
  "remote_log_name": "bert_uncased_multirc_PT_reduced__eval_logit",
  "run_dir": "/home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit",
  "run_name": "eval_logit",
  "s2s": {
    "attention": "none"
  },
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "multirc",
  "target_train_max_vals": 10,
  "target_train_val_interval": 1,
  "transfer_paradigm": "finetune",
  "transformers_output_mode": "top",
  "val_interval": 1,
  "write_preds": "val,test",
  "write_strict_glue_format": 1
}
04/02 06:15:04 PM: Saved config to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit/params.conf
04/02 06:15:04 PM: Using random seed 1234
04/02 06:15:04 PM: Loading tasks...
04/02 06:15:04 PM: Writing pre-preprocessed tasks to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/
04/02 06:15:04 PM: 	Loaded existing task multirc
04/02 06:15:04 PM: 	Task 'multirc': |train|=27243 |val|=4848 |test|=9693
04/02 06:15:05 PM: 	Loaded existing task sst
04/02 06:15:05 PM: 	Task 'sst': |train|=67349 |val|=872 |test|=1821
04/02 06:15:05 PM: 	Finished loading tasks: multirc sst.
04/02 06:15:05 PM: Loading token dictionary from /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/vocab.
04/02 06:15:05 PM: 	Loaded vocab from /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/vocab
04/02 06:15:05 PM: 	Vocab namespace chars: size 78
04/02 06:15:05 PM: 	Vocab namespace bert_uncased: size 30524
04/02 06:15:05 PM: 	Vocab namespace tokens: size 17932
04/02 06:15:05 PM: 	Finished building vocab.
04/02 06:15:05 PM: 	Task 'multirc', split 'train': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__train_data
04/02 06:15:05 PM: 	Task 'multirc', split 'val': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__val_data
04/02 06:15:05 PM: 	Task 'multirc', split 'test': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/multirc__test_data
04/02 06:15:05 PM: 	Task 'sst', split 'train': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__train_data
04/02 06:15:05 PM: 	Task 'sst', split 'val': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__val_data
04/02 06:15:05 PM: 	Task 'sst', split 'test': Found preprocessed copy in /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/preproc/sst__test_data
04/02 06:15:05 PM: 	Finished indexing tasks
04/02 06:15:05 PM: 	Creating trimmed target-only version of multirc train.
04/02 06:15:05 PM: 	Creating trimmed pretraining-only version of sst train.
04/02 06:15:05 PM: 	  Training on sst
04/02 06:15:05 PM: 	  Evaluating on multirc
04/02 06:15:05 PM: 	Finished loading tasks in 0.302s
04/02 06:15:05 PM: 	 Tasks: ['multirc', 'sst']
04/02 06:15:05 PM: Building model...
04/02 06:15:05 PM: Using BERT model (bert-base-uncased).
04/02 06:15:05 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
04/02 06:15:05 PM: Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

04/02 06:15:06 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
04/02 06:15:08 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
04/02 06:15:08 PM: Initializing parameters
04/02 06:15:08 PM: Done initializing parameters; the following parameters are using their default initialization from their code
04/02 06:15:08 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
04/02 06:15:08 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
04/02 06:15:08 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
04/02 06:15:08 PM:    _text_field_embedder.model.pooler.dense.bias
04/02 06:15:08 PM:    _text_field_embedder.model.pooler.dense.weight
04/02 06:15:08 PM: 	Task 'multirc' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "multirc"
}
04/02 06:15:08 PM: 	Task 'sst' params: {
  "cls_type": "log_reg",
  "d_hid": 512,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "softmax",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "sst"
}
04/02 06:15:08 PM: Model specification:
04/02 06:15:08 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.1)
  )
  (multirc_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (sst_mdl): SingleClassifier(
    (pooler): Pooler()
    (classifier): Classifier(
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
04/02 06:15:08 PM: Model parameters:
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 23440896 with torch.Size([30522, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])
04/02 06:15:08 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])
04/02 06:15:08 PM: 	multirc_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
04/02 06:15:08 PM: 	multirc_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
04/02 06:15:08 PM: 	sst_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])
04/02 06:15:08 PM: 	sst_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])
04/02 06:15:08 PM: Total number of parameters: 109485316 (1.09485e+08)
04/02 06:15:08 PM: Number of trainable parameters: 109485316 (1.09485e+08)
04/02 06:15:08 PM: Finished building model in 3.682s
04/02 06:15:08 PM: Evauluating a target task model on tasks {'multirc'} without training it in this run. It's up to you to ensure that you are loading parameters that were sufficiently trained for this task.
04/02 06:15:08 PM: Will run the following steps for this experiment:
Loading model from path: bert_uncased_multirc_PT_reduced/bert_uncased_multirc/multirc/model_state_target_train_val_7.best.th 
Evaluating model on tasks: multirc 

04/02 06:15:08 PM: Evaluating...
04/02 06:15:09 PM: Loaded model state from bert_uncased_multirc_PT_reduced/bert_uncased_multirc/multirc/model_state_target_train_val_7.best.th
04/02 06:15:09 PM: Evaluating on: multirc, split: val
04/02 06:15:39 PM: 	Task multirc: batch 3
04/02 06:16:10 PM: 	Task multirc: batch 7
04/02 06:16:44 PM: 	Task multirc: batch 12
04/02 06:17:14 PM: 	Task multirc: batch 16
04/02 06:17:49 PM: 	Task multirc: batch 21
04/02 06:18:23 PM: 	Task multirc: batch 26
04/02 06:19:00 PM: 	Task multirc: batch 31
04/02 06:19:32 PM: 	Task multirc: batch 36
04/02 06:20:05 PM: 	Task multirc: batch 41
04/02 06:20:37 PM: 	Task multirc: batch 46
04/02 06:21:12 PM: 	Task multirc: batch 51
04/02 06:21:48 PM: 	Task multirc: batch 56
04/02 06:22:24 PM: 	Task multirc: batch 61
04/02 06:22:59 PM: 	Task multirc: batch 66
04/02 06:23:35 PM: 	Task multirc: batch 71
04/02 06:24:09 PM: 	Task multirc: batch 76
04/02 06:24:40 PM: 	Task multirc: batch 81
04/02 06:25:17 PM: 	Task multirc: batch 86
04/02 06:25:53 PM: 	Task multirc: batch 91
04/02 06:26:29 PM: 	Task multirc: batch 96
04/02 06:27:03 PM: 	Task multirc: batch 101
04/02 06:27:35 PM: 	Task multirc: batch 106
04/02 06:28:09 PM: 	Task multirc: batch 111
04/02 06:28:42 PM: 	Task multirc: batch 116
04/02 06:29:17 PM: 	Task multirc: batch 121
04/02 06:29:53 PM: 	Task multirc: batch 126
04/02 06:30:28 PM: 	Task multirc: batch 131
04/02 06:31:05 PM: 	Task multirc: batch 136
04/02 06:31:38 PM: 	Task multirc: batch 141
04/02 06:32:11 PM: 	Task multirc: batch 146
04/02 06:32:42 PM: 	Task multirc: batch 151
04/02 06:32:43 PM: Task 'multirc': sorting predictions by 'idx'
04/02 06:32:43 PM: Finished evaluating on: multirc
04/02 06:32:43 PM: Task 'multirc': Wrote predictions to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit
04/02 06:32:43 PM: Wrote all preds for split 'val' to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit
04/02 06:32:43 PM: Evaluating on: multirc, split: test
04/02 06:33:19 PM: 	Task multirc: batch 4
04/02 06:33:54 PM: 	Task multirc: batch 9
04/02 06:34:27 PM: 	Task multirc: batch 14
04/02 06:35:03 PM: 	Task multirc: batch 19
04/02 06:35:37 PM: 	Task multirc: batch 24
04/02 06:36:10 PM: 	Task multirc: batch 29
04/02 06:36:46 PM: 	Task multirc: batch 34
04/02 06:37:22 PM: 	Task multirc: batch 39
04/02 06:37:56 PM: 	Task multirc: batch 44
04/02 06:38:29 PM: 	Task multirc: batch 49
04/02 06:39:03 PM: 	Task multirc: batch 54
04/02 06:39:37 PM: 	Task multirc: batch 59
04/02 06:40:09 PM: 	Task multirc: batch 64
04/02 06:40:42 PM: 	Task multirc: batch 69
04/02 06:41:17 PM: 	Task multirc: batch 74
04/02 06:41:50 PM: 	Task multirc: batch 79
04/02 06:42:20 PM: 	Task multirc: batch 83
04/02 06:42:56 PM: 	Task multirc: batch 88
04/02 06:43:31 PM: 	Task multirc: batch 93
04/02 06:44:06 PM: 	Task multirc: batch 98
04/02 06:44:37 PM: 	Task multirc: batch 102
04/02 06:45:13 PM: 	Task multirc: batch 107
04/02 06:45:48 PM: 	Task multirc: batch 112
04/02 06:46:25 PM: 	Task multirc: batch 117
04/02 06:47:01 PM: 	Task multirc: batch 122
04/02 06:47:35 PM: 	Task multirc: batch 127
04/02 06:48:10 PM: 	Task multirc: batch 132
04/02 06:48:41 PM: 	Task multirc: batch 136
04/02 06:49:15 PM: 	Task multirc: batch 141
04/02 06:49:50 PM: 	Task multirc: batch 146
04/02 06:50:23 PM: 	Task multirc: batch 151
04/02 06:50:59 PM: 	Task multirc: batch 156
04/02 06:51:32 PM: 	Task multirc: batch 161
04/02 06:52:07 PM: 	Task multirc: batch 165
04/02 06:52:41 PM: 	Task multirc: batch 169
04/02 06:53:18 PM: 	Task multirc: batch 174
04/02 06:53:48 PM: 	Task multirc: batch 178
04/02 06:54:20 PM: 	Task multirc: batch 182
04/02 06:54:53 PM: 	Task multirc: batch 187
04/02 06:55:29 PM: 	Task multirc: batch 192
04/02 06:56:05 PM: 	Task multirc: batch 197
04/02 06:56:40 PM: 	Task multirc: batch 202
04/02 06:57:10 PM: 	Task multirc: batch 206
04/02 06:57:43 PM: 	Task multirc: batch 211
04/02 06:58:14 PM: 	Task multirc: batch 215
04/02 06:58:51 PM: 	Task multirc: batch 220
04/02 06:59:28 PM: 	Task multirc: batch 225
04/02 07:00:01 PM: 	Task multirc: batch 230
04/02 07:00:34 PM: 	Task multirc: batch 235
04/02 07:01:08 PM: 	Task multirc: batch 240
04/02 07:01:41 PM: 	Task multirc: batch 245
04/02 07:02:14 PM: 	Task multirc: batch 249
04/02 07:02:51 PM: 	Task multirc: batch 254
04/02 07:03:24 PM: 	Task multirc: batch 259
04/02 07:03:55 PM: 	Task multirc: batch 263
04/02 07:04:25 PM: 	Task multirc: batch 267
04/02 07:05:01 PM: 	Task multirc: batch 272
04/02 07:05:38 PM: 	Task multirc: batch 277
04/02 07:06:14 PM: 	Task multirc: batch 282
04/02 07:06:50 PM: 	Task multirc: batch 287
04/02 07:07:26 PM: 	Task multirc: batch 292
04/02 07:08:03 PM: 	Task multirc: batch 297
04/02 07:08:40 PM: 	Task multirc: batch 302
04/02 07:08:40 PM: Task 'multirc': sorting predictions by 'idx'
04/02 07:08:40 PM: Finished evaluating on: multirc
04/02 07:08:41 PM: Task 'multirc': Wrote predictions to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit
04/02 07:08:41 PM: Wrote all preds for split 'test' to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/eval_logit
04/02 07:08:41 PM: Writing results for split 'val' to /home/soujanya/Projects/jiant/bert_uncased_multirc_PT_reduced/results.tsv
04/02 07:08:41 PM: micro_avg: 0.300, macro_avg: 0.300, multirc_ans_f1: 0.591, multirc_qst_f1: 0.563, multirc_em: 0.009, multirc_avg: 0.300
04/02 07:08:41 PM: Done!
