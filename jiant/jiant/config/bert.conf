// Config for BERT experiments.

// Get default configs from a file:

include "defaults.conf"

exp_name = "bert-large-cased"

// Data and preprocessing settings
max_seq_len = 256

// Model settings
pretrain_tasks = "multirc"
target_tasks = "multirc"
input_module = "bert-large-cased"
transformers_output_mode = "top"
s2s = {
    attention = none
}
sent_enc = "none"
sep_embs_for_skip = 1
classifier = log_reg

// fine-tune entire BERT model
transfer_paradigm = finetune

// Training settings
dropout = 0.1
optimizer = bert_adam
batch_size = 4
max_epochs = 10
lr = .00001
min_lr = .0000001
lr_patience = 4
patience = 20
max_vals = 10000

// Phase configuration
do_pretrain = 1
do_target_task_training = 1
do_full_eval = 1
write_preds = "val,test"
write_strict_glue_format = 1

// Task specific configuration
commitbank = {
    val_interval = 60
    max_epochs = 40
}
